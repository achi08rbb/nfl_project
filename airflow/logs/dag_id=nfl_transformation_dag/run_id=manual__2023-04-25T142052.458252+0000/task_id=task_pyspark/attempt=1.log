[2023-04-25T14:21:11.562+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-25T14:20:52.458252+00:00 [queued]>
[2023-04-25T14:21:11.575+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-25T14:20:52.458252+00:00 [queued]>
[2023-04-25T14:21:11.576+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-25T14:21:11.577+0000] {taskinstance.py:1289} INFO - Starting attempt 1 of 3
[2023-04-25T14:21:11.577+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-25T14:21:11.594+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-25 14:20:52.458252+00:00
[2023-04-25T14:21:11.602+0000] {standard_task_runner.py:55} INFO - Started process 1938 to run task
[2023-04-25T14:21:11.606+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'manual__2023-04-25T14:20:52.458252+00:00', '--job-id', '1295', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpq6xpy2as']
[2023-04-25T14:21:11.609+0000] {standard_task_runner.py:83} INFO - Job 1295: Subtask task_pyspark
[2023-04-25T14:21:11.677+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-25T14:20:52.458252+00:00 [running]> on host 36bdc6733002
[2023-04-25T14:21:11.749+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-25T14:20:52.458252+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-25T14:20:52.458252+00:00
[2023-04-25T14:21:11.752+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-25T14:21:11.755+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-25T14:21:11.768+0000] {subprocess.py:86} INFO - Output:
[2023-04-25T14:21:18.512+0000] {subprocess.py:93} INFO - Job [07c10c5f9d8949f5a1ebb9af1fba9de1] submitted.
[2023-04-25T14:21:18.513+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-25T14:21:30.372+0000] {subprocess.py:93} INFO - 23/04/25 14:21:29 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-25T14:21:30.373+0000] {subprocess.py:93} INFO - 23/04/25 14:21:29 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-25T14:21:30.374+0000] {subprocess.py:93} INFO - 23/04/25 14:21:29 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-25T14:21:30.375+0000] {subprocess.py:93} INFO - 23/04/25 14:21:29 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-25T14:21:30.375+0000] {subprocess.py:93} INFO - 23/04/25 14:21:30 INFO org.sparkproject.jetty.util.log: Logging initialized @5581ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-25T14:21:30.376+0000] {subprocess.py:93} INFO - 23/04/25 14:21:30 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-25T14:21:30.377+0000] {subprocess.py:93} INFO - 23/04/25 14:21:30 INFO org.sparkproject.jetty.server.Server: Started @5736ms
[2023-04-25T14:21:30.378+0000] {subprocess.py:93} INFO - 23/04/25 14:21:30 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@6140d3c5{HTTP/1.1, (http/1.1)}{0.0.0.0:38067}
[2023-04-25T14:21:35.033+0000] {subprocess.py:93} INFO - 23/04/25 14:21:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:21:39.710+0000] {subprocess.py:93} INFO - 23/04/25 14:21:39 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-25T14:21:39.712+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-25T14:21:42.511+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-25T14:21:47.164+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-25T14:21:49.969+0000] {subprocess.py:93} INFO - 23/04/25 14:21:48 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-25T14:21:49.969+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.970+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.971+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.971+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.972+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.972+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.973+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.974+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:49.975+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:21:49.975+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:21:49.976+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:21:49.977+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:21:49.978+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:21:49.978+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:21:49.979+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:21:49.979+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:21:49.980+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:21:49.981+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:21:49.982+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:21:49.983+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:21:49.983+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:21:49.984+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:21:49.984+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:21:49.985+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:21:49.985+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:21:49.986+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:21:49.986+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:21:49.987+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:21:49.987+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:21:49.989+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:21:49.990+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:21:49.990+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:21:49.991+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:21:49.992+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:21:49.993+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:21:49.993+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:21:49.994+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:21:49.995+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:21:49.996+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:21:49.997+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:21:49.997+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:21:49.998+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:21:49.998+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:21:49.999+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:21:49.999+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:21:50.000+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:21:50.000+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:21:50.001+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:21:50.001+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:21:50.003+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:21:50.004+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:21:50.004+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:21:50.005+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:21:50.006+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:21:50.006+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:21:50.007+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:21:50.007+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:21:50.008+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:21:50.009+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:21:50.011+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:21:50.012+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:21:50.013+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:21:50.014+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:21:50.014+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:21:50.015+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-25T14:21:50.017+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-25T14:21:50.018+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-25T14:21:50.019+0000] {subprocess.py:93} INFO - 23/04/25 14:21:49 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-25T14:21:52.769+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/attempt_202304251421491839561966093590801_0010_m_000002_11/' directory.
[2023-04-25T14:21:52.770+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/attempt_202304251421494885201882412930463_0010_m_000003_12/' directory.
[2023-04-25T14:21:52.771+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/attempt_202304251421492139210397165235227_0010_m_000000_9/' directory.
[2023-04-25T14:21:52.772+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/attempt_202304251421495299325274132810104_0010_m_000001_10/' directory.
[2023-04-25T14:21:52.772+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/' directory.
[2023-04-25T14:21:52.774+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:21:52.775+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/' directory.
[2023-04-25T14:21:52.776+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:21:52.777+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/' directory.
[2023-04-25T14:21:52.778+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:21:52.778+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/' directory.
[2023-04-25T14:21:52.779+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:52.780+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:52.780+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:21:52.781+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:21:52.782+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:21:52.783+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:21:52.784+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:21:52.784+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:21:52.785+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:21:52.786+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:21:52.786+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:21:52.787+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:21:52.788+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:21:52.789+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:21:52.790+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:21:52.791+0000] {subprocess.py:93} INFO - 23/04/25 14:21:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:21:52.791+0000] {subprocess.py:93} INFO - 23/04/25 14:21:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/_temporary/0/_temporary/attempt_202304251421498726116297134142377_0010_m_000004_13/' directory.
[2023-04-25T14:21:52.792+0000] {subprocess.py:93} INFO - 23/04/25 14:21:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/' directory.
[2023-04-25T14:21:52.793+0000] {subprocess.py:93} INFO - 23/04/25 14:21:51 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/part-00003-7a2a05c5-6e38-4e47-8041-a12bc4e148a0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/part-00004-7a2a05c5-6e38-4e47-8041-a12bc4e148a0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/part-00001-7a2a05c5-6e38-4e47-8041-a12bc4e148a0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/part-00002-7a2a05c5-6e38-4e47-8041-a12bc4e148a0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-28b3fcd4-7440-404d-bbcc-234246ee5e3f/part-00000-7a2a05c5-6e38-4e47-8041-a12bc4e148a0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=ffa6016d-b83b-484c-8020-41e23542c52c, location=europe-west6}
[2023-04-25T14:21:57.132+0000] {subprocess.py:93} INFO - 23/04/25 14:21:54 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=ffa6016d-b83b-484c-8020-41e23542c52c, location=europe-west6}
[2023-04-25T14:21:57.134+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:57.134+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:21:57.135+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:21:57.136+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:21:57.136+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:21:57.137+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:21:57.137+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:21:57.138+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:21:57.138+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:21:57.139+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:21:57.139+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:21:57.140+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:21:57.142+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:21:57.142+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:21:57.143+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:21:57.143+0000] {subprocess.py:93} INFO - 23/04/25 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:21:57.144+0000] {subprocess.py:93} INFO - 23/04/25 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-8f140eea-6b40-4ee5-bef9-9f141728efec/_temporary/0/_temporary/attempt_202304251421557395125922760823702_0011_m_000000_14/' directory.
[2023-04-25T14:21:57.144+0000] {subprocess.py:93} INFO - 23/04/25 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-8f140eea-6b40-4ee5-bef9-9f141728efec/_temporary/0/_temporary/' directory.
[2023-04-25T14:21:57.145+0000] {subprocess.py:93} INFO - 23/04/25 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-8f140eea-6b40-4ee5-bef9-9f141728efec/' directory.
[2023-04-25T14:21:57.145+0000] {subprocess.py:93} INFO - 23/04/25 14:21:56 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-8f140eea-6b40-4ee5-bef9-9f141728efec/part-00000-ac0134f9-048b-4588-bab4-8bf2ca67f729-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=50071d63-fc41-4048-9a85-d6d4a4666df4, location=europe-west6}
[2023-04-25T14:21:59.656+0000] {subprocess.py:93} INFO - 23/04/25 14:21:59 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=50071d63-fc41-4048-9a85-d6d4a4666df4, location=europe-west6}
[2023-04-25T14:22:02.160+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:02.161+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:02.162+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:02.162+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:02.163+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:02.163+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:02.164+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:02.164+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:02.165+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:02.166+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:02.166+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:02.167+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:02.167+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:02.168+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:02.168+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:02.169+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:02.169+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-d7b810ec-ab9e-4382-bc02-4a93fbf3e088/_temporary/0/_temporary/attempt_202304251421595149305059232419039_0012_m_000000_15/' directory.
[2023-04-25T14:22:02.170+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-d7b810ec-ab9e-4382-bc02-4a93fbf3e088/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:02.170+0000] {subprocess.py:93} INFO - 23/04/25 14:22:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-d7b810ec-ab9e-4382-bc02-4a93fbf3e088/' directory.
[2023-04-25T14:22:02.171+0000] {subprocess.py:93} INFO - 23/04/25 14:22:01 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-d7b810ec-ab9e-4382-bc02-4a93fbf3e088/part-00000-b8e2ecc7-a1bd-41c1-ac57-0f1908aebee0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=2246b08b-ce77-4029-bd54-1316bfc5a1ea, location=europe-west6}
[2023-04-25T14:22:04.687+0000] {subprocess.py:93} INFO - 23/04/25 14:22:03 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=2246b08b-ce77-4029-bd54-1316bfc5a1ea, location=europe-west6}
[2023-04-25T14:22:04.688+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.689+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.689+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:04.690+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:04.690+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:04.691+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:04.693+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:04.694+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:04.695+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:04.695+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:04.696+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:04.696+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:04.697+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:04.698+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:04.699+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:04.700+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:04.701+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.701+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.702+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:04.703+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:04.704+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:04.705+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:04.706+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:04.707+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:04.708+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:04.709+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:04.710+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:04.710+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:04.711+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:04.711+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:04.713+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:04.714+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:04.715+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.716+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.717+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:04.717+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:04.718+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:04.718+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:04.719+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:04.720+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:04.720+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:04.721+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:04.721+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:04.722+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:04.722+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:04.723+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:04.723+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:04.724+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:04.724+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.725+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:04.725+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:04.726+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:04.727+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:04.727+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:04.728+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:04.728+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:04.729+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:04.729+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:04.730+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:04.730+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:04.731+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:04.731+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:04.732+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:04.732+0000] {subprocess.py:93} INFO - 23/04/25 14:22:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:09.036+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/attempt_202304251422043548900289135858548_0016_m_000002_20/' directory.
[2023-04-25T14:22:09.038+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/attempt_202304251422046738088046794266517_0016_m_000001_19/' directory.
[2023-04-25T14:22:09.038+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/attempt_202304251422047492813498695799837_0016_m_000000_18/' directory.
[2023-04-25T14:22:09.039+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/attempt_202304251422042858878529748604903_0016_m_000003_21/' directory.
[2023-04-25T14:22:09.039+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:09.040+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:09.040+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:09.041+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:09.042+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:09.042+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:09.043+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:09.044+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:09.045+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:09.045+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:09.046+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:09.047+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:09.047+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:09.048+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:09.048+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:09.049+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:09.050+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:09.051+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:22:09.052+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:09.053+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:22:09.053+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:09.054+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-25T14:22:09.054+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:09.055+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/_temporary/0/_temporary/attempt_202304251422043124627579691082414_0016_m_000004_22/' directory.
[2023-04-25T14:22:09.055+0000] {subprocess.py:93} INFO - 23/04/25 14:22:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/' directory.
[2023-04-25T14:22:09.056+0000] {subprocess.py:93} INFO - 23/04/25 14:22:06 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/part-00000-9a928ed3-3020-47e0-99cc-bc18e43d8bed-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/part-00004-9a928ed3-3020-47e0-99cc-bc18e43d8bed-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/part-00001-9a928ed3-3020-47e0-99cc-bc18e43d8bed-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/part-00003-9a928ed3-3020-47e0-99cc-bc18e43d8bed-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-c37a659c-27e1-424f-9ea2-0893dbe2e047/part-00002-9a928ed3-3020-47e0-99cc-bc18e43d8bed-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=32b70b18-1bcb-4612-ad20-0e8388cbea78, location=europe-west6}
[2023-04-25T14:22:11.266+0000] {subprocess.py:93} INFO - 23/04/25 14:22:09 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=32b70b18-1bcb-4612-ad20-0e8388cbea78, location=europe-west6}
[2023-04-25T14:22:14.100+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:14.101+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:14.101+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:14.102+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:14.102+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:14.103+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:14.104+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:14.105+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:14.106+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:14.106+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:14.107+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:14.107+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:14.108+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:14.108+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:14.109+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:14.109+0000] {subprocess.py:93} INFO - 23/04/25 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:14.110+0000] {subprocess.py:93} INFO - 23/04/25 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-6b712923-79cc-49fb-893f-bf2ac8e1fd94/_temporary/0/_temporary/attempt_202304251422101188540742779516587_0018_m_000000_24/' directory.
[2023-04-25T14:22:14.111+0000] {subprocess.py:93} INFO - 23/04/25 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-6b712923-79cc-49fb-893f-bf2ac8e1fd94/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:14.112+0000] {subprocess.py:93} INFO - 23/04/25 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-6b712923-79cc-49fb-893f-bf2ac8e1fd94/' directory.
[2023-04-25T14:22:14.112+0000] {subprocess.py:93} INFO - 23/04/25 14:22:13 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-6b712923-79cc-49fb-893f-bf2ac8e1fd94/part-00000-8e6bd07a-eb4c-4503-8648-8e6ce121e5a3-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=88b3fce7-c9b4-4ebc-9817-0886ab864152, location=europe-west6}
[2023-04-25T14:22:16.894+0000] {subprocess.py:93} INFO - 23/04/25 14:22:15 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=88b3fce7-c9b4-4ebc-9817-0886ab864152, location=europe-west6}
[2023-04-25T14:22:21.632+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.633+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.634+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:21.634+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:21.635+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:21.636+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:21.636+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:21.637+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:21.637+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:21.638+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:21.638+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:21.639+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:21.639+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:21.640+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:21.640+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:21.641+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:21.641+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.643+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.643+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:21.644+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:21.644+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:21.645+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:21.645+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:21.646+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:21.647+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:21.647+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:21.648+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:21.649+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:21.650+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:21.650+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:21.651+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:21.651+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:21.652+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.652+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.653+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:21.653+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:21.654+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:21.655+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:21.655+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:21.657+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:21.658+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:21.658+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:21.659+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:21.659+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:21.659+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:21.660+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:21.660+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:21.661+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:21.661+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.662+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.663+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:21.664+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:21.664+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:21.665+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:21.665+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:21.666+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:21.666+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:21.667+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:21.667+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:21.668+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:21.668+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:21.669+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:21.670+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:21.671+0000] {subprocess.py:93} INFO - 23/04/25 14:22:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:21.671+0000] {subprocess.py:93} INFO - 23/04/25 14:22:20 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/_temporary/0/_temporary/attempt_202304251422196148366005706514619_0022_m_000002_30/' directory.
[2023-04-25T14:22:21.672+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/_temporary/0/_temporary/attempt_20230425142219110835539906443747_0022_m_000000_28/' directory.
[2023-04-25T14:22:21.672+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:21.673+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.673+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:21.674+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:21.674+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:21.675+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:21.675+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:21.676+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:21.676+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:21.677+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:21.678+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:21.679+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:21.680+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:21.680+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:21.680+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:21.681+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:21.681+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:21.682+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/_temporary/0/_temporary/attempt_20230425142219859942097670035550_0022_m_000001_29/' directory.
[2023-04-25T14:22:21.683+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/_temporary/0/_temporary/attempt_20230425142219445077078653003174_0022_m_000003_31/' directory.
[2023-04-25T14:22:25.249+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/_temporary/0/_temporary/attempt_20230425142219745521920813007566_0022_m_000004_32/' directory.
[2023-04-25T14:22:25.250+0000] {subprocess.py:93} INFO - 23/04/25 14:22:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/' directory.
[2023-04-25T14:22:25.251+0000] {subprocess.py:93} INFO - 23/04/25 14:22:22 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/part-00000-81362f21-640a-4295-834a-73eaae511f89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/part-00004-81362f21-640a-4295-834a-73eaae511f89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/part-00002-81362f21-640a-4295-834a-73eaae511f89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/part-00003-81362f21-640a-4295-834a-73eaae511f89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-422d088d-eb55-4371-b10c-c3ac185576c0/part-00001-81362f21-640a-4295-834a-73eaae511f89-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=d5aad3db-b585-4382-b167-54cd03fbe88e, location=europe-west6}
[2023-04-25T14:22:27.772+0000] {subprocess.py:93} INFO - 23/04/25 14:22:26 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=d5aad3db-b585-4382-b167-54cd03fbe88e, location=europe-west6}
[2023-04-25T14:22:27.773+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-25T14:22:27.774+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.774+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.775+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:27.776+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:27.777+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:27.778+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:27.778+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:27.779+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:27.779+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:27.780+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:27.780+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.780+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:27.781+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:27.782+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:27.782+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:27.783+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:27.784+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:27.784+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.785+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:27.785+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:27.786+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:27.786+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:27.787+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:27.788+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:27.788+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:27.789+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:27.789+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:27.790+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:27.791+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:27.792+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:27.793+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:27.794+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:27.794+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.795+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.796+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:27.797+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:27.798+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:27.798+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:27.799+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:27.799+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:27.800+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:27.800+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:27.801+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:27.801+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:27.802+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:27.803+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:27.804+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:27.805+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:27.806+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.806+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:27.807+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:27.807+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:27.807+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:27.808+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:27.808+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:27.809+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:27.809+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:27.810+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:27.810+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:27.811+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:27.812+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:27.812+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:27.813+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:27.813+0000] {subprocess.py:93} INFO - 23/04/25 14:22:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:32.417+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/_temporary/0/_temporary/attempt_202304251422286694292542936422287_0035_m_000000_56/' directory.
[2023-04-25T14:22:32.418+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:32.418+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/_temporary/0/_temporary/attempt_20230425142228993883582722173831_0035_m_000001_57/' directory.
[2023-04-25T14:22:32.419+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/_temporary/0/_temporary/attempt_202304251422283673524685383596262_0035_m_000002_58/' directory.
[2023-04-25T14:22:32.420+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:32.420+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:32.421+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:32.421+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:32.422+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:32.423+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:32.424+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:32.425+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:32.425+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:32.426+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:32.426+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:32.427+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:32.428+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:32.428+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:32.430+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:32.431+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:32.431+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/_temporary/0/_temporary/attempt_202304251422283150347004350542648_0035_m_000003_59/' directory.
[2023-04-25T14:22:32.432+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/_temporary/0/_temporary/attempt_202304251422287804812751753000269_0035_m_000004_60/' directory.
[2023-04-25T14:22:32.433+0000] {subprocess.py:93} INFO - 23/04/25 14:22:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/' directory.
[2023-04-25T14:22:32.433+0000] {subprocess.py:93} INFO - 23/04/25 14:22:30 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/part-00000-4c026b4e-67bd-4d2b-aac0-12be357904ba-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/part-00002-4c026b4e-67bd-4d2b-aac0-12be357904ba-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/part-00004-4c026b4e-67bd-4d2b-aac0-12be357904ba-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/part-00001-4c026b4e-67bd-4d2b-aac0-12be357904ba-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-1224e925-4e3d-43cd-8cdf-b718e5e9bd87/part-00003-4c026b4e-67bd-4d2b-aac0-12be357904ba-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=458f5aa5-9e8e-4b30-b13e-64094a61284e, location=europe-west6}
[2023-04-25T14:22:35.218+0000] {subprocess.py:93} INFO - 23/04/25 14:22:34 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=458f5aa5-9e8e-4b30-b13e-64094a61284e, location=europe-west6}
[2023-04-25T14:22:35.218+0000] {subprocess.py:93} INFO - 23/04/25 14:22:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:35.219+0000] {subprocess.py:93} INFO - 23/04/25 14:22:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.763+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.764+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.765+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.765+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.766+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.766+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.767+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.767+0000] {subprocess.py:93} INFO - 23/04/25 14:22:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.767+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.768+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.768+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.769+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.769+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:37.770+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:40.287+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:40.288+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:40.289+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:40.290+0000] {subprocess.py:93} INFO - 23/04/25 14:22:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-25T14:22:40.291+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:40.293+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-25T14:22:40.294+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-25T14:22:40.295+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-25T14:22:40.295+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-25T14:22:40.296+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-25T14:22:40.297+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-25T14:22:40.298+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-25T14:22:40.299+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-25T14:22:40.299+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-25T14:22:40.300+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-25T14:22:40.301+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-25T14:22:40.302+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-25T14:22:40.302+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-25T14:22:40.303+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-25T14:22:40.305+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-25T14:22:40.306+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-b58374b8-7f49-488a-a4c6-c3ff6eb6c07a/_temporary/0/_temporary/attempt_202304251422382219636939755654554_0080_m_000000_120/' directory.
[2023-04-25T14:22:40.306+0000] {subprocess.py:93} INFO - 23/04/25 14:22:38 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-b58374b8-7f49-488a-a4c6-c3ff6eb6c07a/_temporary/0/_temporary/' directory.
[2023-04-25T14:22:40.307+0000] {subprocess.py:93} INFO - 23/04/25 14:22:39 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-b58374b8-7f49-488a-a4c6-c3ff6eb6c07a/' directory.
[2023-04-25T14:22:40.308+0000] {subprocess.py:93} INFO - 23/04/25 14:22:39 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682432490502-b58374b8-7f49-488a-a4c6-c3ff6eb6c07a/part-00000-6cf168b1-2bec-4dc0-b724-1954ab1a9f60-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=31b25d9b-8dbc-4cf6-9a86-dbd3d7a10ec8, location=europe-west6}
[2023-04-25T14:22:45.006+0000] {subprocess.py:93} INFO - 23/04/25 14:22:40 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=31b25d9b-8dbc-4cf6-9a86-dbd3d7a10ec8, location=europe-west6}
[2023-04-25T14:22:45.007+0000] {subprocess.py:93} INFO - 23/04/25 14:22:41 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@6140d3c5{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-25T14:22:45.412+0000] {subprocess.py:93} INFO - Job [07c10c5f9d8949f5a1ebb9af1fba9de1] finished successfully.
[2023-04-25T14:22:45.435+0000] {subprocess.py:93} INFO - done: true
[2023-04-25T14:22:45.436+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/07c10c5f9d8949f5a1ebb9af1fba9de1/
[2023-04-25T14:22:45.437+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/07c10c5f9d8949f5a1ebb9af1fba9de1/driveroutput
[2023-04-25T14:22:45.437+0000] {subprocess.py:93} INFO - jobUuid: b5e33525-efeb-3b69-963e-96a928a3043d
[2023-04-25T14:22:45.438+0000] {subprocess.py:93} INFO - placement:
[2023-04-25T14:22:45.439+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-25T14:22:45.439+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-25T14:22:45.440+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-25T14:22:45.440+0000] {subprocess.py:93} INFO -   args:
[2023-04-25T14:22:45.441+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-25T14:22:45.442+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-25T14:22:45.443+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-25T14:22:45.444+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-25T14:22:45.444+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-25T14:22:45.445+0000] {subprocess.py:93} INFO - reference:
[2023-04-25T14:22:45.445+0000] {subprocess.py:93} INFO -   jobId: 07c10c5f9d8949f5a1ebb9af1fba9de1
[2023-04-25T14:22:45.446+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-25T14:22:45.446+0000] {subprocess.py:93} INFO - status:
[2023-04-25T14:22:45.447+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-25T14:22:45.448+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-25T14:22:41.809036Z'
[2023-04-25T14:22:45.449+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-25T14:22:45.450+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-25T14:22:45.451+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-25T14:21:20.298776Z'
[2023-04-25T14:22:45.451+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-25T14:22:45.452+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-25T14:21:20.361251Z'
[2023-04-25T14:22:45.453+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-25T14:22:45.454+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-25T14:22:45.454+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-25T14:21:20.718080Z'
[2023-04-25T14:22:45.607+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-25T14:22:45.608+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-25T14:22:45.621+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230425T142052, start_date=20230425T142111, end_date=20230425T142245
[2023-04-25T14:22:45.659+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-25T14:22:45.677+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
