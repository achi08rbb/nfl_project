[2023-04-24T14:48:21.316+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [queued]>
[2023-04-24T14:48:21.343+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [queued]>
[2023-04-24T14:48:21.344+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T14:48:21.346+0000] {taskinstance.py:1289} INFO - Starting attempt 2 of 4
[2023-04-24T14:48:21.347+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T14:48:21.373+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-24 14:35:19.187015+00:00
[2023-04-24T14:48:21.392+0000] {standard_task_runner.py:55} INFO - Started process 5629 to run task
[2023-04-24T14:48:21.411+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'manual__2023-04-24T14:35:19.187015+00:00', '--job-id', '1245', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpcvpyv1p4']
[2023-04-24T14:48:21.440+0000] {standard_task_runner.py:83} INFO - Job 1245: Subtask task_pyspark
[2023-04-24T14:48:21.714+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [running]> on host 36bdc6733002
[2023-04-24T14:48:21.877+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-24T14:35:19.187015+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-24T14:35:19.187015+00:00
[2023-04-24T14:48:21.896+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-24T14:48:21.903+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-24T14:48:21.927+0000] {subprocess.py:86} INFO - Output:
[2023-04-24T14:48:22.696+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [running]>, dependency 'Task Instance Not Running' FAILED: Task is in the running state
[2023-04-24T14:48:22.706+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state.
[2023-04-24T14:48:22.710+0000] {local_task_job.py:151} INFO - Task is not able to be run
[2023-04-24T14:48:27.711+0000] {subprocess.py:93} INFO - Job [2af3e7c3cbc14635bee3d7efc57def3b] submitted.
[2023-04-24T14:48:27.714+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-24T14:48:38.022+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-24T14:48:38.023+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-24T14:48:38.024+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-24T14:48:38.024+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-24T14:48:38.025+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.sparkproject.jetty.util.log: Logging initialized @4486ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-24T14:48:38.026+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-24T14:48:38.027+0000] {subprocess.py:93} INFO - 23/04/24 14:48:35 INFO org.sparkproject.jetty.server.Server: Started @4650ms
[2023-04-24T14:48:38.027+0000] {subprocess.py:93} INFO - 23/04/24 14:48:36 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@227a509e{HTTP/1.1, (http/1.1)}{0.0.0.0:33293}
[2023-04-24T14:48:40.868+0000] {subprocess.py:93} INFO - 23/04/24 14:48:37 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:48:47.367+0000] {subprocess.py:93} INFO - 23/04/24 14:48:44 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-24T14:48:47.368+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-24T14:48:47.368+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-24T14:48:52.050+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-24T14:48:56.480+0000] {subprocess.py:93} INFO - 23/04/24 14:48:52 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-24T14:48:56.481+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.482+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.484+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.485+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.486+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.487+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.489+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.490+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.491+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:48:56.492+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:48:56.493+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:48:56.493+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:48:56.494+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:48:56.495+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:48:56.497+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:48:56.498+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:48:56.499+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:48:56.500+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:48:56.501+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:48:56.501+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:48:56.502+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:48:56.503+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:48:56.504+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:48:56.505+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:48:56.505+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:48:56.506+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:48:56.506+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:48:56.507+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:48:56.507+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:48:56.508+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:48:56.509+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:48:56.509+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:48:56.510+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:48:56.510+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:48:56.512+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:48:56.512+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:48:56.513+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:48:56.513+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:48:56.514+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:48:56.515+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:48:56.517+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:48:56.518+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:48:56.519+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:48:56.520+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:48:56.520+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:48:56.521+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:48:56.521+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:48:56.522+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:48:56.522+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:48:56.523+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:48:56.523+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:48:56.524+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:48:56.525+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:48:56.526+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:48:56.527+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:48:56.527+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:48:56.529+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:48:56.530+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:48:56.531+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:48:56.533+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:48:56.535+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:48:56.537+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:48:56.538+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:48:56.539+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:48:56.540+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:48:56.540+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:48:56.541+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:48:56.542+0000] {subprocess.py:93} INFO - 23/04/24 14:48:53 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:48:56.543+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/attempt_202304241448521229771162071870692_0010_m_000000_9/' directory.
[2023-04-24T14:48:56.543+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/attempt_20230424144852935337852357185266_0010_m_000003_12/' directory.
[2023-04-24T14:48:56.544+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/attempt_202304241448522330697745304335372_0010_m_000001_10/' directory.
[2023-04-24T14:48:56.545+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/attempt_202304241448528151914110010985034_0010_m_000002_11/' directory.
[2023-04-24T14:48:56.546+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/' directory.
[2023-04-24T14:48:56.547+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:48:56.548+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/' directory.
[2023-04-24T14:48:56.549+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:48:56.550+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/' directory.
[2023-04-24T14:48:56.551+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:48:56.552+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/' directory.
[2023-04-24T14:48:56.553+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.553+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:56.554+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:48:56.554+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:48:56.555+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:48:56.556+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:48:56.556+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:48:56.557+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:48:56.557+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:48:56.558+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:48:56.559+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:48:56.560+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:48:56.561+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:48:56.562+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:48:56.562+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:48:56.562+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:48:56.563+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/_temporary/0/_temporary/attempt_202304241448524001892971470511108_0010_m_000004_13/' directory.
[2023-04-24T14:48:56.563+0000] {subprocess.py:93} INFO - 23/04/24 14:48:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/' directory.
[2023-04-24T14:48:58.994+0000] {subprocess.py:93} INFO - 23/04/24 14:48:55 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/part-00000-d5d5733f-9c81-4e15-954a-0ee37b85674c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/part-00002-d5d5733f-9c81-4e15-954a-0ee37b85674c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/part-00004-d5d5733f-9c81-4e15-954a-0ee37b85674c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/part-00001-d5d5733f-9c81-4e15-954a-0ee37b85674c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b0d1c3c8-6995-4df9-8ad7-c96a4aa9bf8d/part-00003-d5d5733f-9c81-4e15-954a-0ee37b85674c-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=90c8b2a0-8e71-4832-bd7e-52fddc9a850f, location=europe-west6}
[2023-04-24T14:48:58.995+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=90c8b2a0-8e71-4832-bd7e-52fddc9a850f, location=europe-west6}
[2023-04-24T14:48:58.996+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:58.998+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:48:58.999+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:48:59.000+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:48:59.001+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:48:59.002+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:48:59.003+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:48:59.004+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:48:59.005+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:48:59.006+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:48:59.007+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:48:59.008+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:48:59.009+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:48:59.010+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:48:59.012+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:48:59.014+0000] {subprocess.py:93} INFO - 23/04/24 14:48:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:48:59.015+0000] {subprocess.py:93} INFO - 23/04/24 14:48:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-c9ab6db1-5c76-4afa-a033-38e49db85222/_temporary/0/_temporary/attempt_202304241448578670940869403857529_0011_m_000000_14/' directory.
[2023-04-24T14:48:59.017+0000] {subprocess.py:93} INFO - 23/04/24 14:48:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-c9ab6db1-5c76-4afa-a033-38e49db85222/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:01.562+0000] {subprocess.py:93} INFO - 23/04/24 14:48:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-c9ab6db1-5c76-4afa-a033-38e49db85222/' directory.
[2023-04-24T14:49:01.564+0000] {subprocess.py:93} INFO - 23/04/24 14:48:58 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-c9ab6db1-5c76-4afa-a033-38e49db85222/part-00000-eb130a30-f27b-4663-9bf7-3fdf5a394157-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=9edf23be-5d81-4319-82a0-bbd486d93d07, location=europe-west6}
[2023-04-24T14:49:04.108+0000] {subprocess.py:93} INFO - 23/04/24 14:49:01 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=9edf23be-5d81-4319-82a0-bbd486d93d07, location=europe-west6}
[2023-04-24T14:49:04.109+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:04.110+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:04.110+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:04.111+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:04.111+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:04.112+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:04.112+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:04.113+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:04.113+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:04.114+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:04.115+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:04.115+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:04.117+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:04.117+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:04.117+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:04.118+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:08.565+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-8ee6d4cf-8cd3-49fb-aa46-7e6df7115268/_temporary/0/_temporary/attempt_202304241449015146486504516404176_0012_m_000000_15/' directory.
[2023-04-24T14:49:08.566+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-8ee6d4cf-8cd3-49fb-aa46-7e6df7115268/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:08.567+0000] {subprocess.py:93} INFO - 23/04/24 14:49:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-8ee6d4cf-8cd3-49fb-aa46-7e6df7115268/' directory.
[2023-04-24T14:49:08.568+0000] {subprocess.py:93} INFO - 23/04/24 14:49:03 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-8ee6d4cf-8cd3-49fb-aa46-7e6df7115268/part-00000-32a7cf38-6eb5-426d-88e9-6f43475e7073-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=78084903-8281-4fd8-a5d6-41c110cc55c2, location=europe-west6}
[2023-04-24T14:49:08.568+0000] {subprocess.py:93} INFO - 23/04/24 14:49:05 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=78084903-8281-4fd8-a5d6-41c110cc55c2, location=europe-west6}
[2023-04-24T14:49:08.569+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.570+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.571+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:08.572+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:08.572+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:08.573+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:08.573+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:08.574+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:08.574+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:08.575+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:08.575+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:08.576+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:08.576+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:08.577+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:08.578+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:08.578+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:08.579+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.580+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.581+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:08.582+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:08.583+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:08.585+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:08.585+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:08.586+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:08.586+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:08.587+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:08.587+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:08.587+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:08.588+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:08.588+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:08.589+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:08.589+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:08.590+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.590+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.590+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:08.591+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:08.592+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:08.592+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:08.593+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:08.594+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:08.595+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:08.595+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:08.596+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:08.597+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:08.597+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:08.597+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:08.598+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:08.598+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:08.599+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.599+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:08.599+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:08.600+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:08.601+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:08.601+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:08.602+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:08.602+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:08.603+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:08.604+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:08.604+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:08.605+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:08.605+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:08.606+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:08.607+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:08.608+0000] {subprocess.py:93} INFO - 23/04/24 14:49:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:08.609+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/attempt_20230424144906425926650900899746_0016_m_000003_21/' directory.
[2023-04-24T14:49:08.609+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/attempt_202304241449068552808247823671323_0016_m_000002_20/' directory.
[2023-04-24T14:49:11.400+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/attempt_202304241449067594093419584570162_0016_m_000000_18/' directory.
[2023-04-24T14:49:11.401+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:11.402+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/attempt_20230424144906695611798644953756_0016_m_000001_19/' directory.
[2023-04-24T14:49:11.403+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:49:11.404+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:11.404+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:11.405+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:11.406+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:11.407+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:11.407+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:11.408+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:11.409+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:11.410+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:11.411+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:11.411+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:11.412+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:11.413+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:11.414+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:11.415+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:11.416+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:11.416+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:11.417+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:49:11.418+0000] {subprocess.py:93} INFO - 23/04/24 14:49:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:11.420+0000] {subprocess.py:93} INFO - 23/04/24 14:49:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/_temporary/0/_temporary/attempt_202304241449063588650252999327932_0016_m_000004_22/' directory.
[2023-04-24T14:49:11.421+0000] {subprocess.py:93} INFO - 23/04/24 14:49:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/' directory.
[2023-04-24T14:49:11.422+0000] {subprocess.py:93} INFO - 23/04/24 14:49:08 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/part-00002-0d1a8719-61a3-4787-a63d-b949f46a025f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/part-00000-0d1a8719-61a3-4787-a63d-b949f46a025f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/part-00004-0d1a8719-61a3-4787-a63d-b949f46a025f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/part-00001-0d1a8719-61a3-4787-a63d-b949f46a025f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-f2c04f67-7bde-40bc-9f99-9fb4f1bf28dc/part-00003-0d1a8719-61a3-4787-a63d-b949f46a025f-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=49b1337e-d96e-47ed-8be6-c20194cb4310, location=europe-west6}
[2023-04-24T14:49:14.268+0000] {subprocess.py:93} INFO - 23/04/24 14:49:11 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=49b1337e-d96e-47ed-8be6-c20194cb4310, location=europe-west6}
[2023-04-24T14:49:17.068+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:17.069+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:17.070+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:17.070+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:17.071+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:17.072+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:17.072+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:17.073+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:17.074+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:17.075+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:17.075+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:17.076+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:17.077+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:17.077+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:17.078+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:17.079+0000] {subprocess.py:93} INFO - 23/04/24 14:49:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:17.079+0000] {subprocess.py:93} INFO - 23/04/24 14:49:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-3bb8500d-9687-4093-8398-dcb07302e946/_temporary/0/_temporary/attempt_202304241449115427562547242753639_0018_m_000000_24/' directory.
[2023-04-24T14:49:17.080+0000] {subprocess.py:93} INFO - 23/04/24 14:49:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-3bb8500d-9687-4093-8398-dcb07302e946/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:17.080+0000] {subprocess.py:93} INFO - 23/04/24 14:49:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-3bb8500d-9687-4093-8398-dcb07302e946/' directory.
[2023-04-24T14:49:17.081+0000] {subprocess.py:93} INFO - 23/04/24 14:49:15 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-3bb8500d-9687-4093-8398-dcb07302e946/part-00000-c06075e7-17b0-42b5-a3d1-ebd7c4c717fc-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=356152c1-098f-41f2-9dde-e46da186a54b, location=europe-west6}
[2023-04-24T14:49:21.432+0000] {subprocess.py:93} INFO - 23/04/24 14:49:17 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=356152c1-098f-41f2-9dde-e46da186a54b, location=europe-west6}
[2023-04-24T14:49:24.235+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.236+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.237+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:24.238+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:24.239+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:24.240+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:24.241+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:24.242+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:24.242+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:24.243+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:24.244+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:24.245+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:24.245+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:24.246+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:24.247+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:24.248+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:24.249+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.250+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.251+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:24.252+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:24.253+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:24.254+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:24.255+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:24.256+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:24.256+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:24.257+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:24.258+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:24.259+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:24.259+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:24.260+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:24.262+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:24.263+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:24.264+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.265+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.267+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.268+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.269+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:24.270+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:24.272+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:24.273+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:24.273+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:24.274+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:24.275+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:24.276+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:24.277+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:24.278+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:24.279+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:24.279+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:24.280+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:24.281+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:24.282+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:24.283+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:24.284+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:24.285+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:24.286+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:24.287+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:24.288+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:24.290+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:24.291+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:24.292+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:24.293+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:24.294+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:24.296+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:24.297+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:24.297+0000] {subprocess.py:93} INFO - 23/04/24 14:49:21 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/attempt_202304241449211032614302483568406_0022_m_000000_28/' directory.
[2023-04-24T14:49:24.298+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/attempt_202304241449218544629761793699803_0022_m_000002_30/' directory.
[2023-04-24T14:49:24.299+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:24.300+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/attempt_202304241449217507974240714135774_0022_m_000003_31/' directory.
[2023-04-24T14:49:24.300+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/attempt_202304241449214208831155622617254_0022_m_000001_29/' directory.
[2023-04-24T14:49:24.301+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.302+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:24.302+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:24.303+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:24.304+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:24.305+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:24.306+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:24.306+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:24.307+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:24.308+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:24.309+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:24.310+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:24.311+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:24.312+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:24.313+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:24.314+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:24.315+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:49:24.316+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:24.316+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/_temporary/0/_temporary/attempt_202304241449213863724981170408205_0022_m_000004_32/' directory.
[2023-04-24T14:49:24.317+0000] {subprocess.py:93} INFO - 23/04/24 14:49:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/' directory.
[2023-04-24T14:49:24.318+0000] {subprocess.py:93} INFO - 23/04/24 14:49:23 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/part-00003-c897b9c7-2e4e-4198-92ca-86f437ecd25b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/part-00000-c897b9c7-2e4e-4198-92ca-86f437ecd25b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/part-00002-c897b9c7-2e4e-4198-92ca-86f437ecd25b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/part-00004-c897b9c7-2e4e-4198-92ca-86f437ecd25b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-b744b37f-37b2-4a19-b748-266fb1906f34/part-00001-c897b9c7-2e4e-4198-92ca-86f437ecd25b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=662eefe6-f6f1-4a12-a7d7-d9e0d9c9ff17, location=europe-west6}
[2023-04-24T14:49:29.276+0000] {subprocess.py:93} INFO - 23/04/24 14:49:26 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=662eefe6-f6f1-4a12-a7d7-d9e0d9c9ff17, location=europe-west6}
[2023-04-24T14:49:29.277+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-24T14:49:33.897+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.898+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.898+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:33.899+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:33.900+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:33.900+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:33.901+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:33.902+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:33.902+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:33.903+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:33.904+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:33.905+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:33.906+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:33.907+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:33.908+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:33.909+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:33.910+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.911+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.912+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:33.913+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:33.913+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:33.914+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:33.915+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:33.915+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:33.916+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:33.916+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:33.917+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:33.918+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:33.918+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:33.919+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:33.919+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:33.920+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:33.921+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.921+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.922+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:33.922+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:33.923+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:33.923+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:33.924+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:33.924+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:33.925+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:33.926+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:33.926+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:33.927+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:33.928+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:33.928+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:33.929+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:33.929+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:33.930+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.931+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.931+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:33.932+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:33.933+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:33.933+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:33.934+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:33.934+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:33.935+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:33.936+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:33.936+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:33.937+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:33.937+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:33.938+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:33.939+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:33.940+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:33.940+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/_temporary/0/_temporary/attempt_202304241449286105001850103749487_0035_m_000003_59/' directory.
[2023-04-24T14:49:33.941+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:33.941+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/_temporary/0/_temporary/attempt_202304241449288702699487037357709_0035_m_000001_57/' directory.
[2023-04-24T14:49:33.942+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/_temporary/0/_temporary/attempt_202304241449288128580071249600394_0035_m_000000_56/' directory.
[2023-04-24T14:49:33.943+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.943+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:33.944+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:33.945+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:33.945+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:33.946+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:33.946+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:33.947+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:33.948+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:33.948+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:33.949+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:33.949+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:33.950+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:33.950+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:33.951+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:33.951+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:33.952+0000] {subprocess.py:93} INFO - 23/04/24 14:49:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/_temporary/0/_temporary/attempt_202304241449284631889337067130439_0035_m_000002_58/' directory.
[2023-04-24T14:49:33.952+0000] {subprocess.py:93} INFO - 23/04/24 14:49:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/_temporary/0/_temporary/attempt_202304241449282590301205127106556_0035_m_000004_60/' directory.
[2023-04-24T14:49:33.952+0000] {subprocess.py:93} INFO - 23/04/24 14:49:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/' directory.
[2023-04-24T14:49:33.953+0000] {subprocess.py:93} INFO - 23/04/24 14:49:29 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/part-00003-b4d67b78-db9c-43fc-9029-91335522dda4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/part-00000-b4d67b78-db9c-43fc-9029-91335522dda4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/part-00001-b4d67b78-db9c-43fc-9029-91335522dda4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/part-00002-b4d67b78-db9c-43fc-9029-91335522dda4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-10a91dc2-d70b-44ec-8941-39bfb26dc160/part-00004-b4d67b78-db9c-43fc-9029-91335522dda4-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=0f0dfee5-d819-48e1-a545-f0eb361a6f4d, location=europe-west6}
[2023-04-24T14:49:33.953+0000] {subprocess.py:93} INFO - 23/04/24 14:49:31 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=0f0dfee5-d819-48e1-a545-f0eb361a6f4d, location=europe-west6}
[2023-04-24T14:49:36.702+0000] {subprocess.py:93} INFO - 23/04/24 14:49:33 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.703+0000] {subprocess.py:93} INFO - 23/04/24 14:49:33 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.703+0000] {subprocess.py:93} INFO - 23/04/24 14:49:33 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.704+0000] {subprocess.py:93} INFO - 23/04/24 14:49:33 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.705+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.705+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.706+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.707+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.708+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.709+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.709+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.710+0000] {subprocess.py:93} INFO - 23/04/24 14:49:34 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.710+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.711+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.711+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:36.712+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.519+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.520+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.522+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.523+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.525+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.527+0000] {subprocess.py:93} INFO - 23/04/24 14:49:35 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:49:39.528+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:39.529+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:49:39.530+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:49:39.530+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:49:39.531+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:49:39.532+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:49:39.532+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:49:39.533+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:49:39.533+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:49:39.534+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:49:39.534+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:49:39.535+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:49:39.536+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:49:39.536+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:49:39.537+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:49:39.537+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:49:39.538+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-090ad08e-9b26-42c4-80f7-e12188571e89/_temporary/0/_temporary/attempt_202304241449353941387368790012757_0080_m_000000_120/' directory.
[2023-04-24T14:49:39.538+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-090ad08e-9b26-42c4-80f7-e12188571e89/_temporary/0/_temporary/' directory.
[2023-04-24T14:49:39.539+0000] {subprocess.py:93} INFO - 23/04/24 14:49:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-090ad08e-9b26-42c4-80f7-e12188571e89/' directory.
[2023-04-24T14:49:39.539+0000] {subprocess.py:93} INFO - 23/04/24 14:49:37 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347716195-090ad08e-9b26-42c4-80f7-e12188571e89/part-00000-ed2f32fe-57f7-4d36-8d3f-a469397d6b42-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=2b347bff-c97a-4899-b77c-6ea495944c4c, location=europe-west6}
[2023-04-24T14:49:44.231+0000] {subprocess.py:93} INFO - 23/04/24 14:49:38 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=2b347bff-c97a-4899-b77c-6ea495944c4c, location=europe-west6}
[2023-04-24T14:49:44.232+0000] {subprocess.py:93} INFO - 23/04/24 14:49:38 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@227a509e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-24T14:49:44.632+0000] {subprocess.py:93} INFO - Job [2af3e7c3cbc14635bee3d7efc57def3b] finished successfully.
[2023-04-24T14:49:44.651+0000] {subprocess.py:93} INFO - done: true
[2023-04-24T14:49:44.653+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/2af3e7c3cbc14635bee3d7efc57def3b/
[2023-04-24T14:49:44.653+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/2af3e7c3cbc14635bee3d7efc57def3b/driveroutput
[2023-04-24T14:49:44.654+0000] {subprocess.py:93} INFO - jobUuid: e6b2e72f-081b-3483-bdfc-9f08da353b8c
[2023-04-24T14:49:44.655+0000] {subprocess.py:93} INFO - placement:
[2023-04-24T14:49:44.655+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-24T14:49:44.656+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-24T14:49:44.656+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-24T14:49:44.657+0000] {subprocess.py:93} INFO -   args:
[2023-04-24T14:49:44.658+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-24T14:49:44.658+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-24T14:49:44.659+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-24T14:49:44.660+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-24T14:49:44.665+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-24T14:49:44.666+0000] {subprocess.py:93} INFO - reference:
[2023-04-24T14:49:44.667+0000] {subprocess.py:93} INFO -   jobId: 2af3e7c3cbc14635bee3d7efc57def3b
[2023-04-24T14:49:44.668+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-24T14:49:44.669+0000] {subprocess.py:93} INFO - status:
[2023-04-24T14:49:44.670+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-24T14:49:44.671+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:49:39.279875Z'
[2023-04-24T14:49:44.671+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-24T14:49:44.673+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-24T14:49:44.673+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:48:27.917319Z'
[2023-04-24T14:49:44.674+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-24T14:49:44.675+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:48:27.958361Z'
[2023-04-24T14:49:44.676+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-24T14:49:44.677+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-24T14:49:44.677+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:48:28.251627Z'
[2023-04-24T14:49:44.830+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-24T14:49:44.833+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-24T14:49:44.850+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230424T143519, start_date=20230424T144821, end_date=20230424T144944
[2023-04-24T14:49:44.888+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-24T14:49:44.907+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
