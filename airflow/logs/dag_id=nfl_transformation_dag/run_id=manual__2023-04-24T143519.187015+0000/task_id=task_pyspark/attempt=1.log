[2023-04-24T14:41:25.876+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [queued]>
[2023-04-24T14:41:25.897+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [queued]>
[2023-04-24T14:41:25.898+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T14:41:25.898+0000] {taskinstance.py:1289} INFO - Starting attempt 1 of 3
[2023-04-24T14:41:25.899+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T14:41:25.913+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-24 14:35:19.187015+00:00
[2023-04-24T14:41:25.923+0000] {standard_task_runner.py:55} INFO - Started process 5336 to run task
[2023-04-24T14:41:25.926+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'manual__2023-04-24T14:35:19.187015+00:00', '--job-id', '1244', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpth30qebp']
[2023-04-24T14:41:25.930+0000] {standard_task_runner.py:83} INFO - Job 1244: Subtask task_pyspark
[2023-04-24T14:41:26.002+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:35:19.187015+00:00 [running]> on host 36bdc6733002
[2023-04-24T14:41:26.066+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-24T14:35:19.187015+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-24T14:35:19.187015+00:00
[2023-04-24T14:41:26.072+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-24T14:41:26.073+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-24T14:41:26.089+0000] {subprocess.py:86} INFO - Output:
[2023-04-24T14:41:30.987+0000] {subprocess.py:93} INFO - Job [fbbb52e6acaa4f6eb1144a4a3b0d1cf0] submitted.
[2023-04-24T14:41:30.988+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-24T14:41:43.034+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-24T14:41:43.035+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-24T14:41:43.036+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-24T14:41:43.037+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-24T14:41:43.038+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.sparkproject.jetty.util.log: Logging initialized @4769ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-24T14:41:43.039+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-24T14:41:43.040+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.sparkproject.jetty.server.Server: Started @4905ms
[2023-04-24T14:41:43.041+0000] {subprocess.py:93} INFO - 23/04/24 14:41:39 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@34795e0e{HTTP/1.1, (http/1.1)}{0.0.0.0:39227}
[2023-04-24T14:41:47.726+0000] {subprocess.py:93} INFO - 23/04/24 14:41:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:41:50.528+0000] {subprocess.py:93} INFO - 23/04/24 14:41:48 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-24T14:41:50.529+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-24T14:41:53.342+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-24T14:41:57.703+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-24T14:41:57.705+0000] {subprocess.py:93} INFO - 23/04/24 14:41:55 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-24T14:42:00.231+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.232+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.233+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.233+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.234+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.234+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.235+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.236+0000] {subprocess.py:93} INFO - 23/04/24 14:41:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.237+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:00.237+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:00.238+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:00.238+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:00.239+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:00.239+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:00.240+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:00.240+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:00.241+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:00.241+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:00.242+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:00.242+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:00.243+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:00.243+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:00.244+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:00.244+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:00.245+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:00.245+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:00.246+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:00.246+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:00.247+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:00.247+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:00.248+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:00.248+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:00.249+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:00.249+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:00.250+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:00.250+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:00.250+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:00.251+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:00.251+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:00.252+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:00.252+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:00.253+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:00.253+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:00.254+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:00.254+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:00.255+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:00.255+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:00.256+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:00.256+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:00.256+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:00.257+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:00.257+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:00.258+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:00.258+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:00.259+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:00.260+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:00.260+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:00.260+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:00.261+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:00.261+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:00.262+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:00.262+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:00.263+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:00.263+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:00.264+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:42:00.264+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:42:00.264+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:42:00.265+0000] {subprocess.py:93} INFO - 23/04/24 14:41:57 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T14:42:00.265+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/attempt_20230424144156535483424581553921_0010_m_000000_9/' directory.
[2023-04-24T14:42:00.266+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/attempt_202304241441566178097942549183140_0010_m_000001_10/' directory.
[2023-04-24T14:42:00.266+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/attempt_202304241441562152581935903138521_0010_m_000003_12/' directory.
[2023-04-24T14:42:00.267+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/attempt_20230424144156446483561358971933_0010_m_000002_11/' directory.
[2023-04-24T14:42:00.267+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:00.268+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:00.269+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:00.269+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.270+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:00.270+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:00.271+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:00.271+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:00.272+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:00.272+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:00.273+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:00.273+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:00.274+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:00.274+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:00.275+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:00.276+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:00.276+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:00.277+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:00.277+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:00.278+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:00.278+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:00.278+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:00.279+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:00.279+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/_temporary/0/_temporary/attempt_202304241441561324041448574720556_0010_m_000004_13/' directory.
[2023-04-24T14:42:00.280+0000] {subprocess.py:93} INFO - 23/04/24 14:41:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/' directory.
[2023-04-24T14:42:02.741+0000] {subprocess.py:93} INFO - 23/04/24 14:41:59 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/part-00000-64186098-d697-4154-8b0c-15d08ef6a583-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/part-00001-64186098-d697-4154-8b0c-15d08ef6a583-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/part-00004-64186098-d697-4154-8b0c-15d08ef6a583-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/part-00002-64186098-d697-4154-8b0c-15d08ef6a583-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-4edf7f2e-1892-4c06-ae07-f03ecc45257f/part-00003-64186098-d697-4154-8b0c-15d08ef6a583-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=e96cccec-334b-4b02-a592-542548afb2dd, location=europe-west6}
[2023-04-24T14:42:05.524+0000] {subprocess.py:93} INFO - 23/04/24 14:42:02 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=e96cccec-334b-4b02-a592-542548afb2dd, location=europe-west6}
[2023-04-24T14:42:05.525+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:05.525+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:05.526+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:05.526+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:05.527+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:05.528+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:05.529+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:05.530+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:05.531+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:05.531+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:05.532+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:05.533+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:05.534+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:05.535+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:05.536+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:05.537+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:05.538+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-cf47459d-5a23-452a-96db-a2e4ab85f69b/_temporary/0/_temporary/attempt_202304241442027923703820670434150_0011_m_000000_14/' directory.
[2023-04-24T14:42:05.538+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-cf47459d-5a23-452a-96db-a2e4ab85f69b/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:05.539+0000] {subprocess.py:93} INFO - 23/04/24 14:42:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-cf47459d-5a23-452a-96db-a2e4ab85f69b/' directory.
[2023-04-24T14:42:05.540+0000] {subprocess.py:93} INFO - 23/04/24 14:42:04 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-cf47459d-5a23-452a-96db-a2e4ab85f69b/part-00000-d9b99796-be68-40e3-b957-a7b61bc9f3e1-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=5fdad5b6-7feb-477b-87ed-742d2f6e903d, location=europe-west6}
[2023-04-24T14:42:10.137+0000] {subprocess.py:93} INFO - 23/04/24 14:42:06 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=5fdad5b6-7feb-477b-87ed-742d2f6e903d, location=europe-west6}
[2023-04-24T14:42:10.138+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:10.138+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:10.139+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:10.139+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:10.140+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:10.140+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:10.141+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:10.141+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:10.142+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:10.142+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:10.143+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:10.143+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:10.144+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:10.144+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:10.144+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:10.145+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:10.145+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-dac01fea-4aa1-42f7-b9a6-fd63b83c0cbb/_temporary/0/_temporary/attempt_202304241442065077124167565988432_0012_m_000000_15/' directory.
[2023-04-24T14:42:10.146+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-dac01fea-4aa1-42f7-b9a6-fd63b83c0cbb/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:10.146+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-dac01fea-4aa1-42f7-b9a6-fd63b83c0cbb/' directory.
[2023-04-24T14:42:10.147+0000] {subprocess.py:93} INFO - 23/04/24 14:42:07 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-dac01fea-4aa1-42f7-b9a6-fd63b83c0cbb/part-00000-8783675b-09f9-45de-97d5-f590afffb05d-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=394690ba-ed05-42b2-878c-a87c6f6e3659, location=europe-west6}
[2023-04-24T14:42:12.935+0000] {subprocess.py:93} INFO - 23/04/24 14:42:09 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=394690ba-ed05-42b2-878c-a87c6f6e3659, location=europe-west6}
[2023-04-24T14:42:12.936+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.937+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.937+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:12.938+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:12.939+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:12.940+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:12.941+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:12.942+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:12.942+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:12.943+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:12.943+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:12.944+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:12.945+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:12.945+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:12.946+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:12.947+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:12.948+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.949+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.950+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:12.951+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:12.951+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:12.952+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:12.953+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:12.954+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:12.955+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:12.956+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:12.957+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:12.958+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:12.959+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:12.960+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:12.960+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:12.961+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:12.962+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.963+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.964+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:12.965+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:12.966+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:12.967+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:12.967+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:12.968+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:12.969+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:12.971+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:12.972+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:12.973+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:12.974+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:12.974+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:12.975+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:12.976+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:12.977+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.977+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:12.978+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:12.979+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:12.980+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:12.981+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:12.982+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:12.982+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:12.983+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:12.984+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:12.984+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:12.985+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:12.986+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:12.987+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:12.988+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:12.989+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:15.749+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/attempt_202304241442105074183492876449402_0016_m_000000_18/' directory.
[2023-04-24T14:42:15.750+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/attempt_202304241442106208230367857281313_0016_m_000001_19/' directory.
[2023-04-24T14:42:15.751+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/attempt_202304241442103691527472883505826_0016_m_000002_20/' directory.
[2023-04-24T14:42:15.751+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/attempt_202304241442107567392411063686053_0016_m_000003_21/' directory.
[2023-04-24T14:42:15.752+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:15.753+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:15.754+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:15.755+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:15.756+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:15.756+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:15.757+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:15.757+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:15.758+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:15.758+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:15.759+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:15.760+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:15.761+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:15.761+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:15.762+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:15.762+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:15.763+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:15.763+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:15.764+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:15.764+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:15.765+0000] {subprocess.py:93} INFO - 23/04/24 14:42:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:15.765+0000] {subprocess.py:93} INFO - 23/04/24 14:42:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/_temporary/0/_temporary/attempt_202304241442104281251201920027399_0016_m_000004_22/' directory.
[2023-04-24T14:42:15.766+0000] {subprocess.py:93} INFO - 23/04/24 14:42:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/' directory.
[2023-04-24T14:42:15.766+0000] {subprocess.py:93} INFO - 23/04/24 14:42:12 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/part-00001-95dc63d3-7344-4d75-9c0e-5368e3df5455-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/part-00000-95dc63d3-7344-4d75-9c0e-5368e3df5455-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/part-00002-95dc63d3-7344-4d75-9c0e-5368e3df5455-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/part-00004-95dc63d3-7344-4d75-9c0e-5368e3df5455-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-aba497b7-86ce-471c-b134-be0085acba32/part-00003-95dc63d3-7344-4d75-9c0e-5368e3df5455-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=e034e49c-9170-4ab9-81a4-0297df3c5ae3, location=europe-west6}
[2023-04-24T14:42:20.596+0000] {subprocess.py:93} INFO - 23/04/24 14:42:14 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=e034e49c-9170-4ab9-81a4-0297df3c5ae3, location=europe-west6}
[2023-04-24T14:42:20.597+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:20.598+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:20.598+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:20.599+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:20.600+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:20.600+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:20.601+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:20.602+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:20.603+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:20.604+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:20.605+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:20.607+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:20.608+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:20.610+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:20.612+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:20.613+0000] {subprocess.py:93} INFO - 23/04/24 14:42:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:20.615+0000] {subprocess.py:93} INFO - 23/04/24 14:42:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-1ce94185-d7ed-4274-8aa4-ef630707a364/_temporary/0/_temporary/attempt_202304241442158921863528367992829_0018_m_000000_24/' directory.
[2023-04-24T14:42:20.616+0000] {subprocess.py:93} INFO - 23/04/24 14:42:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-1ce94185-d7ed-4274-8aa4-ef630707a364/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:23.163+0000] {subprocess.py:93} INFO - 23/04/24 14:42:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-1ce94185-d7ed-4274-8aa4-ef630707a364/' directory.
[2023-04-24T14:42:23.164+0000] {subprocess.py:93} INFO - 23/04/24 14:42:19 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-1ce94185-d7ed-4274-8aa4-ef630707a364/part-00000-62f1f0f1-4222-46d5-9e0e-a55bf2964b62-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=c5ca5c7d-eb1a-4d90-861c-0096769d2fcc, location=europe-west6}
[2023-04-24T14:42:23.164+0000] {subprocess.py:93} INFO - 23/04/24 14:42:20 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=c5ca5c7d-eb1a-4d90-861c-0096769d2fcc, location=europe-west6}
[2023-04-24T14:42:29.625+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.626+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.626+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:29.627+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:29.628+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:29.629+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:29.630+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:29.631+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:29.631+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:29.632+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:29.633+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:29.634+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:29.636+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:29.637+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:29.639+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:29.640+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:29.640+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.641+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.641+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:29.642+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:29.643+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:29.644+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:29.644+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:29.645+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:29.646+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:29.647+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:29.648+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:29.649+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:29.650+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:29.650+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:29.651+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:29.651+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:29.652+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.652+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.653+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:29.653+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:29.654+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:29.654+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:29.655+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:29.655+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:29.656+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:29.657+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:29.657+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:29.658+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:29.658+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:29.659+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:29.660+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:29.660+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:29.661+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.661+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.662+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:29.663+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:29.665+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:29.666+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:29.667+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:29.668+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:29.669+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:29.669+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:29.670+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:29.671+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:29.671+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:29.672+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:29.673+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:29.674+0000] {subprocess.py:93} INFO - 23/04/24 14:42:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:29.674+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/attempt_202304241442242856216206207499_0022_m_000002_30/' directory.
[2023-04-24T14:42:29.675+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/attempt_202304241442241162044640102778915_0022_m_000003_31/' directory.
[2023-04-24T14:42:29.676+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/attempt_202304241442247272602341740086682_0022_m_000000_28/' directory.
[2023-04-24T14:42:29.676+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/attempt_202304241442244213892454825249252_0022_m_000001_29/' directory.
[2023-04-24T14:42:29.677+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:29.678+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:29.679+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:29.680+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:29.681+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:29.681+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.682+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:29.683+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:29.683+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:29.684+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:29.684+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:29.685+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:29.686+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:29.686+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:29.687+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:29.687+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:29.688+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:29.688+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:29.689+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:29.689+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:29.690+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:29.691+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:29.691+0000] {subprocess.py:93} INFO - 23/04/24 14:42:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:31.007+0000] {subprocess.py:93} INFO - 23/04/24 14:42:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/_temporary/0/_temporary/attempt_202304241442244515323249389135934_0022_m_000004_32/' directory.
[2023-04-24T14:42:31.008+0000] {subprocess.py:93} INFO - 23/04/24 14:42:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/' directory.
[2023-04-24T14:42:31.009+0000] {subprocess.py:93} INFO - 23/04/24 14:42:26 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/part-00002-0b515a44-1234-409f-8a3d-f513a04d81ae-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/part-00003-0b515a44-1234-409f-8a3d-f513a04d81ae-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/part-00001-0b515a44-1234-409f-8a3d-f513a04d81ae-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/part-00000-0b515a44-1234-409f-8a3d-f513a04d81ae-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-fc11cf30-4563-4171-90f6-b1f74f23840a/part-00004-0b515a44-1234-409f-8a3d-f513a04d81ae-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=1a519204-a47d-4312-b6e7-51611b5cb413, location=europe-west6}
[2023-04-24T14:42:33.804+0000] {subprocess.py:93} INFO - 23/04/24 14:42:29 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=1a519204-a47d-4312-b6e7-51611b5cb413, location=europe-west6}
[2023-04-24T14:42:33.805+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-24T14:42:33.806+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.807+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.807+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:33.808+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:33.809+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:33.810+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:33.811+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:33.812+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:33.813+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:33.813+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:33.814+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:33.814+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:33.815+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:33.815+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:33.816+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:33.816+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:33.817+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.817+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.818+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.819+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.820+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:33.821+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:33.822+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:33.823+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:33.824+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:33.825+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:33.826+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:33.827+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:33.827+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:33.828+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:33.829+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:33.830+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:33.831+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:33.831+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:33.832+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:33.833+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.835+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:33.836+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:33.838+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:33.839+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:33.842+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:33.844+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:33.846+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:33.848+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:33.849+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:33.853+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:33.855+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:33.856+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:33.858+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:33.859+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:33.860+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:33.861+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:33.862+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:33.863+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:33.864+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:33.865+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:33.866+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:33.867+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:33.868+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:33.870+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:33.871+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:33.873+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:33.874+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:33.875+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:36.598+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/attempt_202304241442317280012609696165829_0035_m_000000_56/' directory.
[2023-04-24T14:42:36.599+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/attempt_202304241442312304892122745499701_0035_m_000001_57/' directory.
[2023-04-24T14:42:36.601+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/attempt_202304241442313795718200754127224_0035_m_000002_58/' directory.
[2023-04-24T14:42:36.601+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/attempt_202304241442315583794361511966207_0035_m_000003_59/' directory.
[2023-04-24T14:42:36.602+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:36.603+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:36.604+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:36.604+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:36.605+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:36.606+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:36.607+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:36.608+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:36.609+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:36.609+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:36.610+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:36.610+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:36.611+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:36.611+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:36.612+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:36.613+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:36.614+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:36.615+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:36.616+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:36.616+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T14:42:36.617+0000] {subprocess.py:93} INFO - 23/04/24 14:42:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:36.618+0000] {subprocess.py:93} INFO - 23/04/24 14:42:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/_temporary/0/_temporary/attempt_202304241442314032314798424737978_0035_m_000004_60/' directory.
[2023-04-24T14:42:36.618+0000] {subprocess.py:93} INFO - 23/04/24 14:42:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/' directory.
[2023-04-24T14:42:36.619+0000] {subprocess.py:93} INFO - 23/04/24 14:42:33 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/part-00000-2817699f-9539-47ff-ae0a-fea69d1381e0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/part-00004-2817699f-9539-47ff-ae0a-fea69d1381e0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/part-00001-2817699f-9539-47ff-ae0a-fea69d1381e0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/part-00003-2817699f-9539-47ff-ae0a-fea69d1381e0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-30b218ee-7dfc-4c0e-8914-f0aebbc3951a/part-00002-2817699f-9539-47ff-ae0a-fea69d1381e0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=5ad90f56-fdbc-4ed1-9614-263619ff01d3, location=europe-west6}
[2023-04-24T14:42:41.271+0000] {subprocess.py:93} INFO - 23/04/24 14:42:35 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=5ad90f56-fdbc-4ed1-9614-263619ff01d3, location=europe-west6}
[2023-04-24T14:42:41.272+0000] {subprocess.py:93} INFO - 23/04/24 14:42:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.273+0000] {subprocess.py:93} INFO - 23/04/24 14:42:36 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.273+0000] {subprocess.py:93} INFO - 23/04/24 14:42:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.274+0000] {subprocess.py:93} INFO - 23/04/24 14:42:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.275+0000] {subprocess.py:93} INFO - 23/04/24 14:42:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.276+0000] {subprocess.py:93} INFO - 23/04/24 14:42:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.277+0000] {subprocess.py:93} INFO - 23/04/24 14:42:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.277+0000] {subprocess.py:93} INFO - 23/04/24 14:42:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.278+0000] {subprocess.py:93} INFO - 23/04/24 14:42:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.278+0000] {subprocess.py:93} INFO - 23/04/24 14:42:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.279+0000] {subprocess.py:93} INFO - 23/04/24 14:42:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.279+0000] {subprocess.py:93} INFO - 23/04/24 14:42:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.280+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.281+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.282+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.283+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.284+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.284+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.285+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:41.285+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T14:42:44.087+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:44.089+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T14:42:44.090+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T14:42:44.091+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T14:42:44.091+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T14:42:44.092+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T14:42:44.092+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T14:42:44.093+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T14:42:44.094+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T14:42:44.095+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T14:42:44.096+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T14:42:44.098+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T14:42:44.099+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T14:42:44.100+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T14:42:44.102+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T14:42:44.103+0000] {subprocess.py:93} INFO - 23/04/24 14:42:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T14:42:44.104+0000] {subprocess.py:93} INFO - 23/04/24 14:42:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-9eae40c5-5025-4580-8297-e0ca97e7f970/_temporary/0/_temporary/attempt_202304241442394747685719499443651_0080_m_000000_120/' directory.
[2023-04-24T14:42:44.105+0000] {subprocess.py:93} INFO - 23/04/24 14:42:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-9eae40c5-5025-4580-8297-e0ca97e7f970/_temporary/0/_temporary/' directory.
[2023-04-24T14:42:44.106+0000] {subprocess.py:93} INFO - 23/04/24 14:42:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-9eae40c5-5025-4580-8297-e0ca97e7f970/' directory.
[2023-04-24T14:42:44.106+0000] {subprocess.py:93} INFO - 23/04/24 14:42:40 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682347300194-9eae40c5-5025-4580-8297-e0ca97e7f970/part-00000-905f6d1e-54f4-42d3-88fd-1a059de0fa60-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=faddd13c-9e9e-4595-8ae3-0136e8d2e802, location=europe-west6}
[2023-04-24T14:42:44.107+0000] {subprocess.py:93} INFO - 23/04/24 14:42:41 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=faddd13c-9e9e-4595-8ae3-0136e8d2e802, location=europe-west6}
[2023-04-24T14:42:44.107+0000] {subprocess.py:93} INFO - 23/04/24 14:42:41 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@34795e0e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-24T14:42:47.365+0000] {subprocess.py:93} INFO - Job [fbbb52e6acaa4f6eb1144a4a3b0d1cf0] finished successfully.
[2023-04-24T14:42:47.384+0000] {subprocess.py:93} INFO - done: true
[2023-04-24T14:42:47.386+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/fbbb52e6acaa4f6eb1144a4a3b0d1cf0/
[2023-04-24T14:42:47.387+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/fbbb52e6acaa4f6eb1144a4a3b0d1cf0/driveroutput
[2023-04-24T14:42:47.388+0000] {subprocess.py:93} INFO - jobUuid: 55999e01-7e08-3aa7-8183-081d775ca8b2
[2023-04-24T14:42:47.389+0000] {subprocess.py:93} INFO - placement:
[2023-04-24T14:42:47.389+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-24T14:42:47.390+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-24T14:42:47.391+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-24T14:42:47.391+0000] {subprocess.py:93} INFO -   args:
[2023-04-24T14:42:47.392+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-24T14:42:47.393+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-24T14:42:47.394+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-24T14:42:47.395+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-24T14:42:47.397+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-24T14:42:47.398+0000] {subprocess.py:93} INFO - reference:
[2023-04-24T14:42:47.399+0000] {subprocess.py:93} INFO -   jobId: fbbb52e6acaa4f6eb1144a4a3b0d1cf0
[2023-04-24T14:42:47.400+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-24T14:42:47.401+0000] {subprocess.py:93} INFO - status:
[2023-04-24T14:42:47.402+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-24T14:42:47.403+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:42:42.330636Z'
[2023-04-24T14:42:47.404+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-24T14:42:47.405+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-24T14:42:47.406+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:41:31.210397Z'
[2023-04-24T14:42:47.407+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-24T14:42:47.408+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:41:31.255784Z'
[2023-04-24T14:42:47.409+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-24T14:42:47.410+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-24T14:42:47.411+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T14:41:31.596984Z'
[2023-04-24T14:42:47.585+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-24T14:42:47.587+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-24T14:42:47.606+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230424T143519, start_date=20230424T144125, end_date=20230424T144247
[2023-04-24T14:42:47.669+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-24T14:42:47.691+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
