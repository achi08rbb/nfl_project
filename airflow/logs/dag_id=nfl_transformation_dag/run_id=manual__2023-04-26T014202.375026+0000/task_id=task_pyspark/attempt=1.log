[2023-04-26T01:42:48.039+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-26T01:42:02.375026+00:00 [queued]>
[2023-04-26T01:42:48.060+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-26T01:42:02.375026+00:00 [queued]>
[2023-04-26T01:42:48.061+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-26T01:42:48.062+0000] {taskinstance.py:1289} INFO - Starting attempt 1 of 3
[2023-04-26T01:42:48.062+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-26T01:42:48.084+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-26 01:42:02.375026+00:00
[2023-04-26T01:42:48.097+0000] {standard_task_runner.py:55} INFO - Started process 1151 to run task
[2023-04-26T01:42:48.107+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'manual__2023-04-26T01:42:02.375026+00:00', '--job-id', '1301', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpy54ajku0']
[2023-04-26T01:42:48.113+0000] {standard_task_runner.py:83} INFO - Job 1301: Subtask task_pyspark
[2023-04-26T01:42:48.206+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-26T01:42:02.375026+00:00 [running]> on host 36bdc6733002
[2023-04-26T01:42:48.291+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-26T01:42:02.375026+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-26T01:42:02.375026+00:00
[2023-04-26T01:42:48.294+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-26T01:42:48.297+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-26T01:42:48.312+0000] {subprocess.py:86} INFO - Output:
[2023-04-26T01:42:54.422+0000] {subprocess.py:93} INFO - Job [25bc8f2afae249fd8580ea342a41faf6] submitted.
[2023-04-26T01:42:54.423+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-26T01:43:06.625+0000] {subprocess.py:93} INFO - 23/04/26 01:43:06 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-26T01:43:06.636+0000] {subprocess.py:93} INFO - 23/04/26 01:43:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-26T01:43:06.637+0000] {subprocess.py:93} INFO - 23/04/26 01:43:06 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-26T01:43:06.638+0000] {subprocess.py:93} INFO - 23/04/26 01:43:06 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-26T01:43:06.639+0000] {subprocess.py:93} INFO - 23/04/26 01:43:06 INFO org.sparkproject.jetty.util.log: Logging initialized @5337ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-26T01:43:06.641+0000] {subprocess.py:93} INFO - 23/04/26 01:43:06 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-26T01:43:06.643+0000] {subprocess.py:93} INFO - 23/04/26 01:43:07 INFO org.sparkproject.jetty.server.Server: Started @5486ms
[2023-04-26T01:43:06.644+0000] {subprocess.py:93} INFO - 23/04/26 01:43:07 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@77b58774{HTTP/1.1, (http/1.1)}{0.0.0.0:38171}
[2023-04-26T01:43:11.294+0000] {subprocess.py:93} INFO - 23/04/26 01:43:09 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:43:16.212+0000] {subprocess.py:93} INFO - 23/04/26 01:43:16 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-26T01:43:16.213+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-26T01:43:16.214+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-26T01:43:22.698+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-26T01:43:25.560+0000] {subprocess.py:93} INFO - 23/04/26 01:43:24 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-26T01:43:25.562+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.564+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.566+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.569+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.571+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.573+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.576+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.578+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.580+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:25.582+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:25.585+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:25.587+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:25.590+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:25.592+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:25.594+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:25.596+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:25.599+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:25.601+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:25.603+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:25.605+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:25.607+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:25.609+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:25.611+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:25.613+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:25.615+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:25.618+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:25.621+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:25.623+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:25.626+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:25.628+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:25.631+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:25.633+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:25.634+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:25.636+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:25.637+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:25.638+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:25.639+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:25.640+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:25.641+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:25.642+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:25.643+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:25.644+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:25.644+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:25.645+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:25.646+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:25.647+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:25.648+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:25.649+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:25.651+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:25.652+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:25.654+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:25.657+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:25.659+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:25.660+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:25.661+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:25.663+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:25.664+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:25.665+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:25.667+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:25.668+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:25.669+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:25.670+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:25.672+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:25.673+0000] {subprocess.py:93} INFO - 23/04/26 01:43:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:25.674+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T01:43:25.676+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T01:43:25.678+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T01:43:25.679+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T01:43:25.687+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/attempt_202304260143258336288734729070086_0010_m_000003_12/' directory.
[2023-04-26T01:43:25.691+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/attempt_202304260143259137062452437284934_0010_m_000000_9/' directory.
[2023-04-26T01:43:25.692+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/attempt_202304260143252651259484629936139_0010_m_000001_10/' directory.
[2023-04-26T01:43:25.693+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/attempt_202304260143257125747435634868734_0010_m_000002_11/' directory.
[2023-04-26T01:43:25.696+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:25.697+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:43:25.699+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:25.700+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:43:25.701+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:25.703+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.705+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:25.707+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:25.709+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:25.711+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:25.712+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:25.713+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:25.715+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:25.716+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:25.718+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:25.720+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:25.721+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:25.723+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:25.725+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:25.727+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:25.728+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:25.730+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:43:25.731+0000] {subprocess.py:93} INFO - 23/04/26 01:43:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:25.733+0000] {subprocess.py:93} INFO - 23/04/26 01:43:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/_temporary/0/_temporary/attempt_202304260143255456129039768198302_0010_m_000004_13/' directory.
[2023-04-26T01:43:28.356+0000] {subprocess.py:93} INFO - 23/04/26 01:43:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/' directory.
[2023-04-26T01:43:28.358+0000] {subprocess.py:93} INFO - 23/04/26 01:43:27 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/part-00000-83658287-f4d2-4527-9f66-8963afd808f7-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/part-00002-83658287-f4d2-4527-9f66-8963afd808f7-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/part-00004-83658287-f4d2-4527-9f66-8963afd808f7-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/part-00001-83658287-f4d2-4527-9f66-8963afd808f7-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-e8fb12ac-7eac-4612-a006-6cbaa8376296/part-00003-83658287-f4d2-4527-9f66-8963afd808f7-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=2a5bda3f-45a5-4843-8bff-de6b8f916aaf, location=europe-west6}
[2023-04-26T01:43:31.162+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=2a5bda3f-45a5-4843-8bff-de6b8f916aaf, location=europe-west6}
[2023-04-26T01:43:31.163+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:31.164+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:31.165+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:31.166+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:31.167+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:31.168+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:31.169+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:31.170+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:31.172+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:31.172+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:31.173+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:31.174+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:31.175+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:31.176+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:31.178+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:31.179+0000] {subprocess.py:93} INFO - 23/04/26 01:43:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:31.181+0000] {subprocess.py:93} INFO - 23/04/26 01:43:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-124db41d-1d8c-42db-8707-9a6340da565e/_temporary/0/_temporary/attempt_202304260143314906528661208953694_0011_m_000000_14/' directory.
[2023-04-26T01:43:35.938+0000] {subprocess.py:93} INFO - 23/04/26 01:43:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-124db41d-1d8c-42db-8707-9a6340da565e/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:35.948+0000] {subprocess.py:93} INFO - 23/04/26 01:43:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-124db41d-1d8c-42db-8707-9a6340da565e/' directory.
[2023-04-26T01:43:35.956+0000] {subprocess.py:93} INFO - 23/04/26 01:43:32 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-124db41d-1d8c-42db-8707-9a6340da565e/part-00000-0968730d-4690-4c2a-a110-668b93c08ef5-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=78ab6396-e569-4234-a713-6870825762ea, location=europe-west6}
[2023-04-26T01:43:35.958+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=78ab6396-e569-4234-a713-6870825762ea, location=europe-west6}
[2023-04-26T01:43:35.971+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:35.976+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:35.980+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:35.984+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:35.992+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:35.998+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:36.004+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:36.016+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:36.034+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:36.055+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:36.064+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:36.072+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:36.101+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:36.114+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:36.135+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:36.168+0000] {subprocess.py:93} INFO - 23/04/26 01:43:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:36.182+0000] {subprocess.py:93} INFO - 23/04/26 01:43:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-3dea97a1-3935-4bba-8cc0-083234674b19/_temporary/0/_temporary/attempt_202304260143354201118816974039582_0012_m_000000_15/' directory.
[2023-04-26T01:43:36.186+0000] {subprocess.py:93} INFO - 23/04/26 01:43:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-3dea97a1-3935-4bba-8cc0-083234674b19/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:36.194+0000] {subprocess.py:93} INFO - 23/04/26 01:43:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-3dea97a1-3935-4bba-8cc0-083234674b19/' directory.
[2023-04-26T01:43:36.202+0000] {subprocess.py:93} INFO - 23/04/26 01:43:36 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-3dea97a1-3935-4bba-8cc0-083234674b19/part-00000-b47cec66-c6a6-45a3-8f31-b0bb73a33935-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=4349251f-57c5-42f0-99b1-75671592be3a, location=europe-west6}
[2023-04-26T01:43:38.986+0000] {subprocess.py:93} INFO - 23/04/26 01:43:39 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=4349251f-57c5-42f0-99b1-75671592be3a, location=europe-west6}
[2023-04-26T01:43:39.009+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.015+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.020+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:39.026+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:39.030+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:39.035+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:39.042+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:39.051+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:39.057+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:39.063+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:39.067+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:39.072+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:39.078+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:39.085+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:39.092+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:39.098+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:39.107+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.113+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.117+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:39.121+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:39.136+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:39.153+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:39.164+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:39.176+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:39.187+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:39.195+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:39.235+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:39.254+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:39.264+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:39.287+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:39.292+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:39.297+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:39.314+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.317+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.321+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:39.334+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:39.344+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:39.354+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:39.373+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:39.391+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:39.399+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:39.404+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:39.409+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:39.419+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:39.427+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:39.432+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:39.436+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:39.443+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:39.448+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.455+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:39.470+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:39.476+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:39.496+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:39.501+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:39.506+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:39.513+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:39.528+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:39.536+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:39.541+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:39.548+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:39.552+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:39.558+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:39.564+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:39.568+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:42.399+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/attempt_202304260143408277697978637620425_0016_m_000002_20/' directory.
[2023-04-26T01:43:42.442+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/attempt_202304260143408159619017227108231_0016_m_000003_21/' directory.
[2023-04-26T01:43:42.628+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/attempt_202304260143404818257725624816988_0016_m_000001_19/' directory.
[2023-04-26T01:43:42.639+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/attempt_20230426014340959324210577916822_0016_m_000000_18/' directory.
[2023-04-26T01:43:42.647+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:42.653+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:43:42.665+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:42.679+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:42.696+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:42.706+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:42.726+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:42.746+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:42.770+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:42.786+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:42.820+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:42.838+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:42.845+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:42.856+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:42.862+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:42.879+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:42.888+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:42.898+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:42.907+0000] {subprocess.py:93} INFO - 23/04/26 01:43:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:42.923+0000] {subprocess.py:93} INFO - 23/04/26 01:43:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/_temporary/0/_temporary/attempt_202304260143403133278829619187088_0016_m_000004_22/' directory.
[2023-04-26T01:43:42.937+0000] {subprocess.py:93} INFO - 23/04/26 01:43:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/' directory.
[2023-04-26T01:43:42.942+0000] {subprocess.py:93} INFO - 23/04/26 01:43:42 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/part-00001-cb0b4b7d-597c-44ba-beec-a33976c07839-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/part-00000-cb0b4b7d-597c-44ba-beec-a33976c07839-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/part-00002-cb0b4b7d-597c-44ba-beec-a33976c07839-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/part-00004-cb0b4b7d-597c-44ba-beec-a33976c07839-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-2f6fef83-a074-42f6-ad8a-761192662d0d/part-00003-cb0b4b7d-597c-44ba-beec-a33976c07839-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=db792fd9-459b-4262-a94a-bce63f2db377, location=europe-west6}
[2023-04-26T01:43:42.956+0000] {subprocess.py:93} INFO - 23/04/26 01:43:43 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=db792fd9-459b-4262-a94a-bce63f2db377, location=europe-west6}
[2023-04-26T01:43:49.430+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:49.536+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:49.548+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:49.578+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:49.582+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:49.589+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:49.594+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:49.598+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:49.601+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:49.605+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:49.608+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:49.612+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:49.616+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:49.620+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:49.625+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:49.629+0000] {subprocess.py:93} INFO - 23/04/26 01:43:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:49.633+0000] {subprocess.py:93} INFO - 23/04/26 01:43:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-46594c93-3bdc-4a14-a464-e6b23f86bbf5/_temporary/0/_temporary/attempt_202304260143443172100128546162200_0018_m_000000_24/' directory.
[2023-04-26T01:43:49.638+0000] {subprocess.py:93} INFO - 23/04/26 01:43:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-46594c93-3bdc-4a14-a464-e6b23f86bbf5/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:49.643+0000] {subprocess.py:93} INFO - 23/04/26 01:43:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-46594c93-3bdc-4a14-a464-e6b23f86bbf5/' directory.
[2023-04-26T01:43:49.648+0000] {subprocess.py:93} INFO - 23/04/26 01:43:47 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-46594c93-3bdc-4a14-a464-e6b23f86bbf5/part-00000-3cae4a15-109b-4292-ac9a-5f91dd3d6760-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=40522352-246e-477a-8111-d215472cf94e, location=europe-west6}
[2023-04-26T01:43:49.652+0000] {subprocess.py:93} INFO - 23/04/26 01:43:50 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=40522352-246e-477a-8111-d215472cf94e, location=europe-west6}
[2023-04-26T01:43:54.043+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.076+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.082+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:54.086+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:54.092+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:54.097+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:54.106+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:54.112+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:54.118+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:54.124+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:54.135+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:54.140+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:54.145+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:54.149+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:54.154+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:54.163+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:54.168+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.177+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.184+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:54.189+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:54.199+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:54.223+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:54.244+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:54.257+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:54.263+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:54.267+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:54.275+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:54.281+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:54.302+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:54.306+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:54.312+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:54.316+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:54.320+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.329+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.340+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.344+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:54.348+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:54.355+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:54.361+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:54.365+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:54.370+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:54.374+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:54.381+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:54.386+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:54.391+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:54.397+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:54.401+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:54.405+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:54.410+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:54.414+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:54.419+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:54.424+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:54.430+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:54.434+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:54.439+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:54.443+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:54.448+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:54.452+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:54.457+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:54.464+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:54.470+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:54.474+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:54.478+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:54.482+0000] {subprocess.py:93} INFO - 23/04/26 01:43:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:59.773+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/_temporary/0/_temporary/attempt_202304260143544154056996351385922_0022_m_000000_28/' directory.
[2023-04-26T01:43:59.812+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/_temporary/0/_temporary/attempt_202304260143546146709330215915902_0022_m_000003_31/' directory.
[2023-04-26T01:43:59.820+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/_temporary/0/_temporary/' directory.
[2023-04-26T01:43:59.826+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/_temporary/0/_temporary/attempt_202304260143545496071456003295390_0022_m_000001_29/' directory.
[2023-04-26T01:43:59.830+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:59.834+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:43:59.843+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:43:59.847+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:43:59.851+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:43:59.857+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:43:59.860+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:43:59.867+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:43:59.872+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:43:59.876+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:43:59.880+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:43:59.884+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:43:59.890+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:43:59.894+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:43:59.898+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:43:59.906+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:43:59.910+0000] {subprocess.py:93} INFO - 23/04/26 01:43:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/_temporary/0/_temporary/attempt_202304260143545484429745405140580_0022_m_000002_30/' directory.
[2023-04-26T01:43:59.915+0000] {subprocess.py:93} INFO - 23/04/26 01:43:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/_temporary/0/_temporary/attempt_202304260143544407075239537547333_0022_m_000004_32/' directory.
[2023-04-26T01:43:59.919+0000] {subprocess.py:93} INFO - 23/04/26 01:43:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/' directory.
[2023-04-26T01:43:59.924+0000] {subprocess.py:93} INFO - 23/04/26 01:43:56 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/part-00000-e5259f36-26d0-41bd-a593-5dbfe079bc28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/part-00002-e5259f36-26d0-41bd-a593-5dbfe079bc28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/part-00004-e5259f36-26d0-41bd-a593-5dbfe079bc28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/part-00001-e5259f36-26d0-41bd-a593-5dbfe079bc28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-f12681fd-e66b-4a07-818b-34c6b630abc1/part-00003-e5259f36-26d0-41bd-a593-5dbfe079bc28-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=b2227d24-d975-46e0-a355-cb1de69a83f8, location=europe-west6}
[2023-04-26T01:44:01.705+0000] {subprocess.py:93} INFO - 23/04/26 01:43:59 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=b2227d24-d975-46e0-a355-cb1de69a83f8, location=europe-west6}
[2023-04-26T01:44:01.719+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-26T01:44:04.686+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.690+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.694+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:44:04.699+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:44:04.704+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:44:04.708+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:44:04.712+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:44:04.717+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:44:04.721+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:44:04.724+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:44:04.729+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:44:04.735+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:44:04.739+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:44:04.743+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:44:04.748+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:44:04.753+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:44:04.757+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.761+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.765+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.770+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.773+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:44:04.777+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:44:04.780+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:44:04.784+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:44:04.787+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:44:04.791+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:44:04.794+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:44:04.797+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:44:04.800+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:44:04.803+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:44:04.806+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:44:04.809+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:44:04.813+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:44:04.816+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:44:04.820+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:44:04.823+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:44:04.826+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:44:04.829+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:44:04.832+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:44:04.835+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:44:04.838+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:44:04.841+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:44:04.845+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:44:04.848+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:44:04.851+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:44:04.854+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:44:04.857+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:44:04.859+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:44:04.863+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.867+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.870+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:44:04.873+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:44:04.877+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:44:04.880+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:44:04.883+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:44:04.886+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:44:04.889+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:44:04.892+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:44:04.895+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:44:04.898+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:44:04.903+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:44:04.910+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:44:04.915+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:44:04.918+0000] {subprocess.py:93} INFO - 23/04/26 01:44:01 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:44:04.923+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/attempt_202304260144017229432493004106016_0035_m_000003_59/' directory.
[2023-04-26T01:44:04.930+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/attempt_202304260144018004632292589175180_0035_m_000002_58/' directory.
[2023-04-26T01:44:04.935+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/attempt_202304260144011210576166582540445_0035_m_000001_57/' directory.
[2023-04-26T01:44:04.938+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/attempt_202304260144017909920031044029741_0035_m_000000_56/' directory.
[2023-04-26T01:44:04.940+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/' directory.
[2023-04-26T01:44:04.943+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:44:04.947+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/' directory.
[2023-04-26T01:44:04.950+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:44:04.952+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/' directory.
[2023-04-26T01:44:04.956+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T01:44:04.959+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/' directory.
[2023-04-26T01:44:04.962+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.966+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:04.969+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:44:04.972+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:44:04.975+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:44:04.979+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:44:04.981+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:44:04.984+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:44:04.989+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:44:04.992+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:44:04.996+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:44:05.000+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:44:05.005+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:44:05.008+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:44:05.011+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:44:05.015+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:44:05.018+0000] {subprocess.py:93} INFO - 23/04/26 01:44:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/_temporary/0/_temporary/attempt_202304260144014789600647575743874_0035_m_000004_60/' directory.
[2023-04-26T01:44:05.022+0000] {subprocess.py:93} INFO - 23/04/26 01:44:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/' directory.
[2023-04-26T01:44:05.030+0000] {subprocess.py:93} INFO - 23/04/26 01:44:03 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/part-00001-6e4f202c-194d-4f0f-824c-47fb7fefba53-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/part-00003-6e4f202c-194d-4f0f-824c-47fb7fefba53-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/part-00000-6e4f202c-194d-4f0f-824c-47fb7fefba53-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/part-00002-6e4f202c-194d-4f0f-824c-47fb7fefba53-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-8290a16d-0528-45e3-81ff-5a896c1ebd58/part-00004-6e4f202c-194d-4f0f-824c-47fb7fefba53-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=a6de8cdd-0ca2-4245-8e28-0dceb8d550c7, location=europe-west6}
[2023-04-26T01:44:05.032+0000] {subprocess.py:93} INFO - 23/04/26 01:44:05 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=a6de8cdd-0ca2-4245-8e28-0dceb8d550c7, location=europe-west6}
[2023-04-26T01:44:09.418+0000] {subprocess.py:93} INFO - 23/04/26 01:44:06 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.419+0000] {subprocess.py:93} INFO - 23/04/26 01:44:06 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.420+0000] {subprocess.py:93} INFO - 23/04/26 01:44:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.421+0000] {subprocess.py:93} INFO - 23/04/26 01:44:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.422+0000] {subprocess.py:93} INFO - 23/04/26 01:44:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.423+0000] {subprocess.py:93} INFO - 23/04/26 01:44:07 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.424+0000] {subprocess.py:93} INFO - 23/04/26 01:44:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.425+0000] {subprocess.py:93} INFO - 23/04/26 01:44:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.426+0000] {subprocess.py:93} INFO - 23/04/26 01:44:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.427+0000] {subprocess.py:93} INFO - 23/04/26 01:44:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.427+0000] {subprocess.py:93} INFO - 23/04/26 01:44:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.428+0000] {subprocess.py:93} INFO - 23/04/26 01:44:08 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.430+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.430+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.432+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.433+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.433+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.434+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.435+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.436+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.437+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.438+0000] {subprocess.py:93} INFO - 23/04/26 01:44:09 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T01:44:09.439+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:09.440+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T01:44:09.441+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T01:44:09.442+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T01:44:09.443+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T01:44:09.444+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T01:44:09.445+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T01:44:09.446+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T01:44:09.447+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T01:44:09.448+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T01:44:09.449+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T01:44:09.450+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T01:44:09.451+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T01:44:09.452+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T01:44:09.453+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T01:44:09.454+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T01:44:09.455+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-4ffebb37-7785-4590-a668-89b60dbaa51f/_temporary/0/_temporary/attempt_202304260144092595422824491419406_0080_m_000000_120/' directory.
[2023-04-26T01:44:09.457+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-4ffebb37-7785-4590-a668-89b60dbaa51f/_temporary/0/_temporary/' directory.
[2023-04-26T01:44:09.458+0000] {subprocess.py:93} INFO - 23/04/26 01:44:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-4ffebb37-7785-4590-a668-89b60dbaa51f/' directory.
[2023-04-26T01:44:11.963+0000] {subprocess.py:93} INFO - 23/04/26 01:44:11 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682473387236-4ffebb37-7785-4590-a668-89b60dbaa51f/part-00000-4ac7f06f-f816-4c8a-80cd-1d5c9a60f152-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=0f36f25c-861a-43a4-b846-4343ec573201, location=europe-west6}
[2023-04-26T01:44:11.965+0000] {subprocess.py:93} INFO - 23/04/26 01:44:12 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=0f36f25c-861a-43a4-b846-4343ec573201, location=europe-west6}
[2023-04-26T01:44:11.966+0000] {subprocess.py:93} INFO - 23/04/26 01:44:12 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@77b58774{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-26T01:44:15.215+0000] {subprocess.py:93} INFO - Job [25bc8f2afae249fd8580ea342a41faf6] finished successfully.
[2023-04-26T01:44:15.344+0000] {subprocess.py:93} INFO - done: true
[2023-04-26T01:44:15.346+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/25bc8f2afae249fd8580ea342a41faf6/
[2023-04-26T01:44:15.347+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/25bc8f2afae249fd8580ea342a41faf6/driveroutput
[2023-04-26T01:44:15.348+0000] {subprocess.py:93} INFO - jobUuid: 5dfb68e7-e118-3383-9f13-cb0ce442641c
[2023-04-26T01:44:15.349+0000] {subprocess.py:93} INFO - placement:
[2023-04-26T01:44:15.351+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-26T01:44:15.352+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-26T01:44:15.353+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-26T01:44:15.354+0000] {subprocess.py:93} INFO -   args:
[2023-04-26T01:44:15.355+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-26T01:44:15.356+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-26T01:44:15.356+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-26T01:44:15.358+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-26T01:44:15.360+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-26T01:44:15.362+0000] {subprocess.py:93} INFO - reference:
[2023-04-26T01:44:15.364+0000] {subprocess.py:93} INFO -   jobId: 25bc8f2afae249fd8580ea342a41faf6
[2023-04-26T01:44:15.365+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-26T01:44:15.367+0000] {subprocess.py:93} INFO - status:
[2023-04-26T01:44:15.369+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-26T01:44:15.372+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T01:44:13.623845Z'
[2023-04-26T01:44:15.373+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-26T01:44:15.375+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-26T01:44:15.377+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T01:42:57.723115Z'
[2023-04-26T01:44:15.378+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-26T01:44:15.379+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T01:42:57.769578Z'
[2023-04-26T01:44:15.382+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-26T01:44:15.384+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-26T01:44:15.386+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T01:42:58.071413Z'
[2023-04-26T01:44:16.027+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-26T01:44:16.067+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-26T01:44:16.410+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230426T014202, start_date=20230426T014248, end_date=20230426T014416
[2023-04-26T01:44:16.681+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-26T01:44:16.834+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
