[2023-04-24T15:03:54.959+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:54:30.025680+00:00 [queued]>
[2023-04-24T15:03:54.982+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:54:30.025680+00:00 [queued]>
[2023-04-24T15:03:54.984+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T15:03:54.987+0000] {taskinstance.py:1289} INFO - Starting attempt 3 of 4
[2023-04-24T15:03:54.989+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T15:03:55.017+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-24 14:54:30.025680+00:00
[2023-04-24T15:03:55.099+0000] {standard_task_runner.py:55} INFO - Started process 7214 to run task
[2023-04-24T15:03:55.104+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'manual__2023-04-24T14:54:30.025680+00:00', '--job-id', '1256', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpyp_ohv53']
[2023-04-24T15:03:55.110+0000] {standard_task_runner.py:83} INFO - Job 1256: Subtask task_pyspark
[2023-04-24T15:03:55.249+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-24T14:54:30.025680+00:00 [running]> on host 36bdc6733002
[2023-04-24T15:03:55.364+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-24T14:54:30.025680+00:00
AIRFLOW_CTX_TRY_NUMBER=3
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-24T14:54:30.025680+00:00
[2023-04-24T15:03:55.378+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-24T15:03:55.406+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-24T15:03:55.482+0000] {subprocess.py:86} INFO - Output:
[2023-04-24T15:04:01.505+0000] {subprocess.py:93} INFO - Job [22cee1b9735c42eaa179322625ce7e2f] submitted.
[2023-04-24T15:04:01.506+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-24T15:04:13.290+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-24T15:04:13.291+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-24T15:04:13.292+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-24T15:04:13.292+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-24T15:04:13.293+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.sparkproject.jetty.util.log: Logging initialized @5043ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-24T15:04:13.293+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-24T15:04:13.294+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.sparkproject.jetty.server.Server: Started @5202ms
[2023-04-24T15:04:13.295+0000] {subprocess.py:93} INFO - 23/04/24 15:04:10 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@5a7c66c3{HTTP/1.1, (http/1.1)}{0.0.0.0:40989}
[2023-04-24T15:04:17.909+0000] {subprocess.py:93} INFO - 23/04/24 15:04:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:20.700+0000] {subprocess.py:93} INFO - 23/04/24 15:04:19 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-24T15:04:20.702+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-24T15:04:23.508+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-24T15:04:30.006+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-24T15:04:30.007+0000] {subprocess.py:93} INFO - 23/04/24 15:04:27 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-24T15:04:30.008+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.805+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.806+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.807+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.808+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.808+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.809+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.811+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.812+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:32.812+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:32.813+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:32.814+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:32.815+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:32.816+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:32.817+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:32.818+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:32.820+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:32.820+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:32.821+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:32.821+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:32.822+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:32.822+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:32.823+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:32.824+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:32.825+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:32.826+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:32.826+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:32.827+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:32.827+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:32.828+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:32.828+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:32.829+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:32.830+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:32.831+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:32.832+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:32.833+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:32.833+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:32.834+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:32.834+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:32.835+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:32.835+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:32.836+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:32.836+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:32.837+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:32.839+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:32.840+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:32.840+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:32.841+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:32.842+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:32.842+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:32.843+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:32.844+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:32.846+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:32.846+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:32.847+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:32.848+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:32.849+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:32.850+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:32.851+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:32.852+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:32.852+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:32.853+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:32.854+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:32.854+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:32.855+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T15:04:32.856+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T15:04:32.857+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T15:04:32.857+0000] {subprocess.py:93} INFO - 23/04/24 15:04:28 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T15:04:32.858+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/attempt_202304241504289098550086433803794_0010_m_000001_10/' directory.
[2023-04-24T15:04:32.860+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/attempt_202304241504285703581310721959909_0010_m_000000_9/' directory.
[2023-04-24T15:04:32.860+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/attempt_202304241504283053409423891990434_0010_m_000002_11/' directory.
[2023-04-24T15:04:32.861+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/attempt_202304241504284923803531882769035_0010_m_000003_12/' directory.
[2023-04-24T15:04:32.862+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:32.863+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:32.864+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:32.865+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:32.866+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:32.866+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:32.867+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:32.868+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.870+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:32.870+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:32.871+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:32.872+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:32.873+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:32.874+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:32.875+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:32.876+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:32.877+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:32.878+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:32.879+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:32.880+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:32.881+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:32.882+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:32.883+0000] {subprocess.py:93} INFO - 23/04/24 15:04:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:32.884+0000] {subprocess.py:93} INFO - 23/04/24 15:04:30 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/_temporary/0/_temporary/attempt_202304241504281974101857694580989_0010_m_000004_13/' directory.
[2023-04-24T15:04:32.885+0000] {subprocess.py:93} INFO - 23/04/24 15:04:30 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/' directory.
[2023-04-24T15:04:35.062+0000] {subprocess.py:93} INFO - 23/04/24 15:04:30 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/part-00002-18edfa8e-1892-4499-880c-7f853902523a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/part-00003-18edfa8e-1892-4499-880c-7f853902523a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/part-00000-18edfa8e-1892-4499-880c-7f853902523a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/part-00001-18edfa8e-1892-4499-880c-7f853902523a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4e2610b8-41e5-4772-b6aa-3aac0616c094/part-00004-18edfa8e-1892-4499-880c-7f853902523a-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=f56ef822-7ef7-4243-890e-23326d42adeb, location=europe-west6}
[2023-04-24T15:04:37.854+0000] {subprocess.py:93} INFO - 23/04/24 15:04:35 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=f56ef822-7ef7-4243-890e-23326d42adeb, location=europe-west6}
[2023-04-24T15:04:42.482+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:42.483+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:42.483+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:42.484+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:42.485+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:42.485+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:42.486+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:42.486+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:42.487+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:42.487+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:42.488+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:42.489+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:42.490+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:42.490+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:42.491+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:42.492+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:42.492+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-3eeb05d8-ae75-4fc5-b4c6-97a42cb4e7a8/_temporary/0/_temporary/attempt_202304241504357786531591856152450_0011_m_000000_14/' directory.
[2023-04-24T15:04:42.493+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-3eeb05d8-ae75-4fc5-b4c6-97a42cb4e7a8/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:42.493+0000] {subprocess.py:93} INFO - 23/04/24 15:04:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-3eeb05d8-ae75-4fc5-b4c6-97a42cb4e7a8/' directory.
[2023-04-24T15:04:42.494+0000] {subprocess.py:93} INFO - 23/04/24 15:04:37 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-3eeb05d8-ae75-4fc5-b4c6-97a42cb4e7a8/part-00000-68fa03e5-ce25-4ba9-a1cb-b8b2b09e36fc-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=21a60783-2d76-409c-abbe-c9f783179027, location=europe-west6}
[2023-04-24T15:04:42.494+0000] {subprocess.py:93} INFO - 23/04/24 15:04:39 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=21a60783-2d76-409c-abbe-c9f783179027, location=europe-west6}
[2023-04-24T15:04:42.495+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:42.496+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:42.496+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:42.497+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:42.497+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:42.498+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:42.498+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:42.499+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:42.499+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:42.500+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:42.500+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:42.501+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:42.501+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:42.502+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:42.503+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:42.503+0000] {subprocess.py:93} INFO - 23/04/24 15:04:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:42.504+0000] {subprocess.py:93} INFO - 23/04/24 15:04:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-25d839c3-5f39-4542-8591-14a2a446106d/_temporary/0/_temporary/attempt_202304241504407314462048279702316_0012_m_000000_15/' directory.
[2023-04-24T15:04:44.994+0000] {subprocess.py:93} INFO - 23/04/24 15:04:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-25d839c3-5f39-4542-8591-14a2a446106d/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:44.995+0000] {subprocess.py:93} INFO - 23/04/24 15:04:41 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-25d839c3-5f39-4542-8591-14a2a446106d/' directory.
[2023-04-24T15:04:44.996+0000] {subprocess.py:93} INFO - 23/04/24 15:04:41 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-25d839c3-5f39-4542-8591-14a2a446106d/part-00000-8a0a04b5-0540-4ab8-8f02-3ba9c2cafac3-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=e3182b5a-2b6a-4f55-b977-b3edd4c6f9cd, location=europe-west6}
[2023-04-24T15:04:47.523+0000] {subprocess.py:93} INFO - 23/04/24 15:04:43 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=e3182b5a-2b6a-4f55-b977-b3edd4c6f9cd, location=europe-west6}
[2023-04-24T15:04:47.524+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.525+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.525+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:47.526+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:47.527+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:47.528+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:47.529+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:47.530+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:47.530+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:47.531+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:47.532+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:47.533+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:47.533+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:47.535+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:47.535+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:47.536+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:47.537+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.538+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.539+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:47.540+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:47.541+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:47.542+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:47.543+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:47.544+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:47.545+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:47.546+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:47.547+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:47.548+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:47.549+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:47.550+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:47.551+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:47.551+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:47.552+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.553+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.553+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:47.554+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:47.555+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:47.556+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:47.557+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:47.558+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:47.559+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:47.559+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:47.560+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:47.561+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:47.561+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:47.562+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:47.563+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:47.564+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:47.565+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.565+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:47.566+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:47.567+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:47.567+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:47.568+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:47.569+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:47.570+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:47.570+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:47.571+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:47.572+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:47.573+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:47.573+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:47.574+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:47.574+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:47.575+0000] {subprocess.py:93} INFO - 23/04/24 15:04:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:47.575+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/attempt_202304241504452066833050412842771_0016_m_000001_19/' directory.
[2023-04-24T15:04:47.576+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/attempt_202304241504453703582170749863179_0016_m_000003_21/' directory.
[2023-04-24T15:04:47.577+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/attempt_202304241504452799527521555887544_0016_m_000002_20/' directory.
[2023-04-24T15:04:47.578+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/attempt_202304241504454107137567824518603_0016_m_000000_18/' directory.
[2023-04-24T15:04:50.346+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:50.347+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:50.347+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:50.348+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:50.349+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:50.351+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:50.352+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:50.353+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:50.353+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:50.354+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:50.354+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:50.355+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:50.356+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:50.357+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:50.358+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:50.359+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:50.359+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:50.360+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:50.360+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:50.361+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:50.361+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:50.362+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:04:50.363+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:50.364+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/_temporary/0/_temporary/attempt_20230424150445232454442600702921_0016_m_000004_22/' directory.
[2023-04-24T15:04:50.365+0000] {subprocess.py:93} INFO - 23/04/24 15:04:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/' directory.
[2023-04-24T15:04:50.366+0000] {subprocess.py:93} INFO - 23/04/24 15:04:47 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/part-00000-29a427da-17d8-4cd1-b2d2-53c7c72156be-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/part-00002-29a427da-17d8-4cd1-b2d2-53c7c72156be-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/part-00004-29a427da-17d8-4cd1-b2d2-53c7c72156be-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/part-00001-29a427da-17d8-4cd1-b2d2-53c7c72156be-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-399740ac-f3bb-498d-b051-fd774888231c/part-00003-29a427da-17d8-4cd1-b2d2-53c7c72156be-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=4d22ea3d-91fc-4782-bcfa-3f1bc09bc7eb, location=europe-west6}
[2023-04-24T15:04:55.008+0000] {subprocess.py:93} INFO - 23/04/24 15:04:49 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=4d22ea3d-91fc-4782-bcfa-3f1bc09bc7eb, location=europe-west6}
[2023-04-24T15:04:55.009+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:55.009+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:04:55.010+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:04:55.010+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:04:55.011+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:04:55.012+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:04:55.012+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:04:55.013+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:04:55.013+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:04:55.014+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:04:55.015+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:04:55.015+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:04:55.016+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:04:55.016+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:04:55.017+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:04:55.017+0000] {subprocess.py:93} INFO - 23/04/24 15:04:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:04:55.018+0000] {subprocess.py:93} INFO - 23/04/24 15:04:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4481523f-af4c-4361-8699-fc674ee1ec1f/_temporary/0/_temporary/attempt_20230424150450930189126258800486_0018_m_000000_24/' directory.
[2023-04-24T15:04:55.018+0000] {subprocess.py:93} INFO - 23/04/24 15:04:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4481523f-af4c-4361-8699-fc674ee1ec1f/_temporary/0/_temporary/' directory.
[2023-04-24T15:04:55.019+0000] {subprocess.py:93} INFO - 23/04/24 15:04:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4481523f-af4c-4361-8699-fc674ee1ec1f/' directory.
[2023-04-24T15:04:57.801+0000] {subprocess.py:93} INFO - 23/04/24 15:04:53 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-4481523f-af4c-4361-8699-fc674ee1ec1f/part-00000-d9fd6ef0-f988-47f4-b561-b55bab2ee4db-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=b43a2639-f5e9-42a9-ac38-5a1934395953, location=europe-west6}
[2023-04-24T15:04:57.802+0000] {subprocess.py:93} INFO - 23/04/24 15:04:55 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=b43a2639-f5e9-42a9-ac38-5a1934395953, location=europe-west6}
[2023-04-24T15:05:03.112+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.114+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.114+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:03.115+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:03.116+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:03.117+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:03.118+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:03.119+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:03.119+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:03.120+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:03.120+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:03.121+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:03.121+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:03.122+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:03.122+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:03.123+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:03.124+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.124+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.125+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:03.125+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:03.126+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:03.126+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:03.127+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:03.127+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:03.128+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:03.128+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:03.129+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:03.129+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:03.130+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:03.131+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:03.132+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:03.132+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:03.134+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.135+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.136+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:03.136+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:03.137+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:03.138+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:03.138+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:03.139+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:03.140+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:03.141+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:03.142+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:03.142+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:03.143+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:03.144+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:03.144+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:03.145+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:03.145+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.146+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.146+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:03.147+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:03.147+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:03.148+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:03.148+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:03.148+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:03.149+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:03.149+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:03.150+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:03.150+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:03.151+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:03.151+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:03.152+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:03.152+0000] {subprocess.py:93} INFO - 23/04/24 15:04:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:03.153+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/attempt_202304241504596084311772887957555_0022_m_000001_29/' directory.
[2023-04-24T15:05:03.153+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/attempt_202304241504593985771185963323025_0022_m_000003_31/' directory.
[2023-04-24T15:05:03.154+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/attempt_202304241504593232120006968966845_0022_m_000000_28/' directory.
[2023-04-24T15:05:03.154+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/attempt_202304241504598828441147377496181_0022_m_000002_30/' directory.
[2023-04-24T15:05:03.155+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:03.155+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:05:03.156+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:03.156+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.157+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:03.157+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:03.158+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:03.158+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:03.159+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:03.159+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:03.160+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:03.160+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:03.161+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:03.161+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:03.162+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:03.162+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:03.162+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:03.163+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:03.163+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:03.164+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:05:03.164+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:07.889+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/_temporary/0/_temporary/attempt_202304241504591148675715974977754_0022_m_000004_32/' directory.
[2023-04-24T15:05:07.890+0000] {subprocess.py:93} INFO - 23/04/24 15:05:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/' directory.
[2023-04-24T15:05:07.891+0000] {subprocess.py:93} INFO - 23/04/24 15:05:01 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/part-00000-a7b519f4-cbf5-4fcb-8e5f-f16f70346bd5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/part-00002-a7b519f4-cbf5-4fcb-8e5f-f16f70346bd5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/part-00004-a7b519f4-cbf5-4fcb-8e5f-f16f70346bd5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/part-00001-a7b519f4-cbf5-4fcb-8e5f-f16f70346bd5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-7fcee2ce-9ca8-490c-94be-e04385970fc0/part-00003-a7b519f4-cbf5-4fcb-8e5f-f16f70346bd5-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=f68ee66a-ed98-4273-8030-b8be1f904b86, location=europe-west6}
[2023-04-24T15:05:07.891+0000] {subprocess.py:93} INFO - 23/04/24 15:05:03 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=f68ee66a-ed98-4273-8030-b8be1f904b86, location=europe-west6}
[2023-04-24T15:05:07.892+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-24T15:05:07.893+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.893+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.894+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:07.895+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:07.896+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:07.896+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:07.898+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:07.899+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:07.900+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:07.901+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:07.902+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:07.903+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:07.903+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:07.904+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:07.905+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:07.906+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:07.907+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.908+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.908+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:07.909+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:07.910+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:07.911+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:07.911+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:07.912+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:07.913+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:07.914+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:07.915+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:07.915+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:07.916+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:07.917+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:07.917+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.918+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:07.919+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:07.920+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.921+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:07.922+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:07.922+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:07.923+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:07.923+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:07.924+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:07.925+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:07.925+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:07.926+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:07.927+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:07.927+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:07.928+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:07.929+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:07.930+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:07.931+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.932+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:07.933+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:07.935+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:07.935+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:07.936+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:07.936+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:07.937+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:07.938+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:07.938+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:07.939+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:07.940+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:07.941+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:07.941+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:07.942+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:07.943+0000] {subprocess.py:93} INFO - 23/04/24 15:05:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:11.497+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/attempt_20230424150505906240014156185291_0035_m_000002_58/' directory.
[2023-04-24T15:05:11.498+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/attempt_202304241505056706216793675515339_0035_m_000003_59/' directory.
[2023-04-24T15:05:11.502+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/attempt_20230424150505618931162824223731_0035_m_000000_56/' directory.
[2023-04-24T15:05:11.504+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/attempt_202304241505053396352979676002135_0035_m_000001_57/' directory.
[2023-04-24T15:05:11.504+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:11.505+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:05:11.506+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:11.507+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:11.508+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:11.508+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:11.509+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:11.510+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:11.511+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:11.512+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:11.512+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:11.513+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:11.514+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:11.514+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:11.515+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:11.515+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:11.516+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:11.517+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:11.519+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:11.520+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T15:05:11.520+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:11.521+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/_temporary/0/_temporary/attempt_202304241505054403512751000414341_0035_m_000004_60/' directory.
[2023-04-24T15:05:11.521+0000] {subprocess.py:93} INFO - 23/04/24 15:05:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/' directory.
[2023-04-24T15:05:11.522+0000] {subprocess.py:93} INFO - 23/04/24 15:05:07 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/part-00003-4528f236-1c12-45c2-97ec-442cf928a1b5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/part-00001-4528f236-1c12-45c2-97ec-442cf928a1b5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/part-00000-4528f236-1c12-45c2-97ec-442cf928a1b5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/part-00002-4528f236-1c12-45c2-97ec-442cf928a1b5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-6245c577-99da-4ee0-9cdb-61da50789a2e/part-00004-4528f236-1c12-45c2-97ec-442cf928a1b5-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=91442ca4-e7ca-4bbd-ad09-2f54ad05be04, location=europe-west6}
[2023-04-24T15:05:11.523+0000] {subprocess.py:93} INFO - 23/04/24 15:05:09 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=91442ca4-e7ca-4bbd-ad09-2f54ad05be04, location=europe-west6}
[2023-04-24T15:05:16.114+0000] {subprocess.py:93} INFO - 23/04/24 15:05:10 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.114+0000] {subprocess.py:93} INFO - 23/04/24 15:05:10 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.115+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.116+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.116+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.117+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.117+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.118+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.118+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.119+0000] {subprocess.py:93} INFO - 23/04/24 15:05:11 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.120+0000] {subprocess.py:93} INFO - 23/04/24 15:05:12 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.121+0000] {subprocess.py:93} INFO - 23/04/24 15:05:12 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.121+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.122+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.123+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.123+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.124+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.124+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.125+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.125+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.125+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.126+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T15:05:16.127+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:16.127+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T15:05:16.128+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T15:05:16.128+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T15:05:16.129+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T15:05:16.130+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T15:05:16.131+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T15:05:16.131+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T15:05:16.131+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T15:05:16.132+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T15:05:16.132+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T15:05:16.133+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T15:05:16.133+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T15:05:16.134+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T15:05:16.134+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T15:05:16.135+0000] {subprocess.py:93} INFO - 23/04/24 15:05:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T15:05:16.135+0000] {subprocess.py:93} INFO - 23/04/24 15:05:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-74e7db1c-107a-49b7-ae9f-411f677ce577/_temporary/0/_temporary/attempt_202304241505138002534793374543610_0080_m_000000_120/' directory.
[2023-04-24T15:05:16.136+0000] {subprocess.py:93} INFO - 23/04/24 15:05:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-74e7db1c-107a-49b7-ae9f-411f677ce577/_temporary/0/_temporary/' directory.
[2023-04-24T15:05:18.922+0000] {subprocess.py:93} INFO - 23/04/24 15:05:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-74e7db1c-107a-49b7-ae9f-411f677ce577/' directory.
[2023-04-24T15:05:18.923+0000] {subprocess.py:93} INFO - 23/04/24 15:05:14 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682348650895-74e7db1c-107a-49b7-ae9f-411f677ce577/part-00000-3fa4c497-0e2c-4f3f-99b1-6de4ad63ed48-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=05521d0b-9e2a-45b6-beaf-96a4d028a924, location=europe-west6}
[2023-04-24T15:05:18.923+0000] {subprocess.py:93} INFO - 23/04/24 15:05:16 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=05521d0b-9e2a-45b6-beaf-96a4d028a924, location=europe-west6}
[2023-04-24T15:05:18.924+0000] {subprocess.py:93} INFO - 23/04/24 15:05:17 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@5a7c66c3{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-24T15:05:20.778+0000] {subprocess.py:93} INFO - Job [22cee1b9735c42eaa179322625ce7e2f] finished successfully.
[2023-04-24T15:05:20.798+0000] {subprocess.py:93} INFO - done: true
[2023-04-24T15:05:20.799+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/22cee1b9735c42eaa179322625ce7e2f/
[2023-04-24T15:05:20.800+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/22cee1b9735c42eaa179322625ce7e2f/driveroutput
[2023-04-24T15:05:20.800+0000] {subprocess.py:93} INFO - jobUuid: 3221bd1b-1372-3910-b5d7-2fc5943c28ff
[2023-04-24T15:05:20.801+0000] {subprocess.py:93} INFO - placement:
[2023-04-24T15:05:20.802+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-24T15:05:20.802+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-24T15:05:20.803+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-24T15:05:20.803+0000] {subprocess.py:93} INFO -   args:
[2023-04-24T15:05:20.804+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-24T15:05:20.804+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-24T15:05:20.805+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-24T15:05:20.806+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-24T15:05:20.806+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-24T15:05:20.807+0000] {subprocess.py:93} INFO - reference:
[2023-04-24T15:05:20.808+0000] {subprocess.py:93} INFO -   jobId: 22cee1b9735c42eaa179322625ce7e2f
[2023-04-24T15:05:20.808+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-24T15:05:20.809+0000] {subprocess.py:93} INFO - status:
[2023-04-24T15:05:20.810+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-24T15:05:20.810+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T15:05:17.722123Z'
[2023-04-24T15:05:20.811+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-24T15:05:20.811+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-24T15:05:20.812+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T15:04:01.687088Z'
[2023-04-24T15:05:20.812+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-24T15:05:20.813+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T15:04:01.728397Z'
[2023-04-24T15:05:20.814+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-24T15:05:20.814+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-24T15:05:20.816+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T15:04:02.010990Z'
[2023-04-24T15:05:20.973+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-24T15:05:20.975+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-24T15:05:20.988+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230424T145430, start_date=20230424T150354, end_date=20230424T150520
[2023-04-24T15:05:21.026+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-24T15:05:21.044+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
