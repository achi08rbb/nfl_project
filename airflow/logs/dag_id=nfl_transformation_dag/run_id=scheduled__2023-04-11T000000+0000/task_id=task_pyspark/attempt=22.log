[2023-04-12T13:53:37.761+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T13:53:37.774+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T13:53:37.775+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T13:53:37.776+0000] {taskinstance.py:1289} INFO - Starting attempt 22 of 23
[2023-04-12T13:53:37.777+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T13:53:37.796+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-12T13:53:37.797+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance Not Running' FAILED: Task is in the running state
[2023-04-12T13:53:37.799+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state.
[2023-04-12T13:53:37.808+0000] {standard_task_runner.py:55} INFO - Started process 10638 to run task
[2023-04-12T13:53:37.812+0000] {local_task_job.py:151} INFO - Task is not able to be run
[2023-04-12T13:53:37.813+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '957', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmp990p8o67']
[2023-04-12T13:53:37.817+0000] {standard_task_runner.py:83} INFO - Job 957: Subtask task_pyspark
[2023-04-12T13:53:37.900+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host a2e619923d04
[2023-04-12T13:53:37.977+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=22
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-12T13:53:37.983+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-12T13:53:37.998+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-de-project/code/transform_pyspark.py                 --                     --year=2020                     --season_type=2 ']
[2023-04-12T13:53:38.017+0000] {subprocess.py:86} INFO - Output:
[2023-04-12T13:53:44.713+0000] {subprocess.py:93} INFO - Job [f32c156639d24c058425c62db5b26d73] submitted.
[2023-04-12T13:53:44.720+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-12T13:53:55.867+0000] {subprocess.py:93} INFO - 23/04/12 13:53:53 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-12T13:53:55.868+0000] {subprocess.py:93} INFO - 23/04/12 13:53:53 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-12T13:53:55.869+0000] {subprocess.py:93} INFO - 23/04/12 13:53:53 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-12T13:53:55.870+0000] {subprocess.py:93} INFO - 23/04/12 13:53:53 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-12T13:53:55.871+0000] {subprocess.py:93} INFO - 23/04/12 13:53:54 INFO org.sparkproject.jetty.util.log: Logging initialized @4833ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-12T13:53:55.872+0000] {subprocess.py:93} INFO - 23/04/12 13:53:54 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-12T13:53:55.872+0000] {subprocess.py:93} INFO - 23/04/12 13:53:54 INFO org.sparkproject.jetty.server.Server: Started @4966ms
[2023-04-12T13:53:55.873+0000] {subprocess.py:93} INFO - 23/04/12 13:53:54 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@37ef5342{HTTP/1.1, (http/1.1)}{0.0.0.0:44183}
[2023-04-12T13:53:58.366+0000] {subprocess.py:93} INFO - 23/04/12 13:53:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:03.156+0000] {subprocess.py:93} INFO - 23/04/12 13:54:02 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-12T13:54:03.157+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-12T13:54:03.158+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-12T13:54:07.801+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-12T13:54:10.624+0000] {subprocess.py:93} INFO - 23/04/12 13:54:09 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-12T13:54:10.625+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.625+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.626+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.627+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.627+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.628+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.630+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.630+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:10.631+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:10.632+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:10.632+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:10.632+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:10.633+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:10.633+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:10.634+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:10.634+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:10.635+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:10.636+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:10.637+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:10.638+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:10.638+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:10.639+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:10.640+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:10.640+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:10.641+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:10.642+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:10.642+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:10.643+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:10.644+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:10.644+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:10.645+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:10.645+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:10.646+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:10.646+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:10.647+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:10.647+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:10.648+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:10.648+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:10.648+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:10.649+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:10.650+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:10.651+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:10.651+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:10.652+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:10.652+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:10.653+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:10.653+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:10.654+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:10.654+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:10.655+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:10.655+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:10.656+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:10.656+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:10.657+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:10.658+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:10.658+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:10.659+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:10.659+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:10.660+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:10.660+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:10.661+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:10.661+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:10.661+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:10.662+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:15.402+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T13:54:15.403+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T13:54:15.404+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T13:54:15.404+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T13:54:15.405+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/attempt_202304121354103268965614305773551_0010_m_000003_12/' directory.
[2023-04-12T13:54:15.405+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/attempt_202304121354107639468876273188857_0010_m_000002_11/' directory.
[2023-04-12T13:54:15.406+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/attempt_20230412135410426752937154101016_0010_m_000001_10/' directory.
[2023-04-12T13:54:15.406+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/attempt_202304121354104233165203475416722_0010_m_000000_9/' directory.
[2023-04-12T13:54:15.407+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:15.407+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:15.408+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:15.408+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:15.409+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:15.410+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:15.410+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:15.410+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:15.411+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:15.412+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:15.412+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:15.412+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:15.413+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:15.414+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:15.414+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:15.415+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:15.416+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:15.416+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:15.417+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:15.417+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:15.417+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:15.418+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:15.418+0000] {subprocess.py:93} INFO - 23/04/12 13:54:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:15.419+0000] {subprocess.py:93} INFO - 23/04/12 13:54:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/_temporary/0/_temporary/attempt_202304121354101123142221483878086_0010_m_000004_13/' directory.
[2023-04-12T13:54:15.419+0000] {subprocess.py:93} INFO - 23/04/12 13:54:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/' directory.
[2023-04-12T13:54:15.420+0000] {subprocess.py:93} INFO - 23/04/12 13:54:13 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=location, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isAllStar, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/part-00001-70dca589-7325-4805-9fb9-d70d76680778-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/part-00003-70dca589-7325-4805-9fb9-d70d76680778-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/part-00000-70dca589-7325-4805-9fb9-d70d76680778-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/part-00002-70dca589-7325-4805-9fb9-d70d76680778-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-b843c7df-133a-4a9e-a41f-10cacf7e4b2f/part-00004-70dca589-7325-4805-9fb9-d70d76680778-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=7899ba4b-31fe-41ea-9d5b-b265fab3033d, location=europe-west6}
[2023-04-12T13:54:18.214+0000] {subprocess.py:93} INFO - 23/04/12 13:54:17 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_2020_2. jobId: JobId{project=nfl-de-project, job=7899ba4b-31fe-41ea-9d5b-b265fab3033d, location=europe-west6}
[2023-04-12T13:54:21.039+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.041+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.041+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:21.042+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:21.043+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:21.044+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:21.045+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:21.046+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:21.047+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:21.048+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:21.049+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:21.050+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:21.051+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:21.053+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:21.054+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:21.054+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:21.055+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.056+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.058+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:21.060+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:21.062+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:21.063+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:21.063+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:21.064+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:21.066+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:21.067+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:21.068+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:21.069+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:21.070+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:21.071+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:21.072+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:21.073+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:21.074+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.075+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.076+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:21.077+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:21.078+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:21.080+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:21.081+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:21.082+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:21.082+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:21.083+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:21.084+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:21.085+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:21.085+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:21.087+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:21.088+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:21.088+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:21.089+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.090+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.091+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:21.093+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:21.094+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:21.095+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:21.096+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:21.097+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:21.097+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:21.098+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:21.099+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:21.100+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:21.101+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:21.102+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:21.102+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:21.103+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:21.104+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/attempt_202304121354198955726151729730000_0014_m_000003_19/' directory.
[2023-04-12T13:54:21.105+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/attempt_202304121354198325876654087350731_0014_m_000001_17/' directory.
[2023-04-12T13:54:21.106+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/attempt_202304121354191700857860215727012_0014_m_000000_16/' directory.
[2023-04-12T13:54:21.107+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:21.108+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/attempt_202304121354191783222182474777696_0014_m_000002_18/' directory.
[2023-04-12T13:54:21.109+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:21.110+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:21.110+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.111+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:21.112+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:21.113+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:21.113+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:21.115+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:21.116+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:21.117+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:21.118+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:21.118+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:21.119+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:21.120+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:21.121+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:21.122+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:21.123+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:21.123+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:21.124+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:21.125+0000] {subprocess.py:93} INFO - 23/04/12 13:54:19 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:21.126+0000] {subprocess.py:93} INFO - 23/04/12 13:54:20 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/_temporary/0/_temporary/attempt_202304121354196787758193884985419_0014_m_000004_20/' directory.
[2023-04-12T13:54:21.126+0000] {subprocess.py:93} INFO - 23/04/12 13:54:20 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/' directory.
[2023-04-12T13:54:21.127+0000] {subprocess.py:93} INFO - 23/04/12 13:54:20 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/part-00001-8cede17e-64d4-48e6-ac42-bddf5a17e89f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/part-00002-8cede17e-64d4-48e6-ac42-bddf5a17e89f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/part-00004-8cede17e-64d4-48e6-ac42-bddf5a17e89f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/part-00000-8cede17e-64d4-48e6-ac42-bddf5a17e89f-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-003de084-f225-4d8e-bfd0-8f801976f023/part-00003-8cede17e-64d4-48e6-ac42-bddf5a17e89f-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=ce3d5ff0-8cc8-4538-b043-bf009ef97ee5, location=europe-west6}
[2023-04-12T13:54:25.694+0000] {subprocess.py:93} INFO - 23/04/12 13:54:22 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_stats_2020_2. jobId: JobId{project=nfl-de-project, job=ce3d5ff0-8cc8-4538-b043-bf009ef97ee5, location=europe-west6}
[2023-04-12T13:54:25.713+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:25.714+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:25.715+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:25.716+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:25.717+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:25.718+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:25.719+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:25.720+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:25.721+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:25.722+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:25.723+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:25.724+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:25.725+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:25.726+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:25.727+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:25.728+0000] {subprocess.py:93} INFO - 23/04/12 13:54:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:28.543+0000] {subprocess.py:93} INFO - 23/04/12 13:54:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-2ef7f6b5-670a-4ea7-8322-939fff390eeb/_temporary/0/_temporary/attempt_202304121354245158442898930755382_0016_m_000000_22/' directory.
[2023-04-12T13:54:28.544+0000] {subprocess.py:93} INFO - 23/04/12 13:54:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-2ef7f6b5-670a-4ea7-8322-939fff390eeb/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:28.545+0000] {subprocess.py:93} INFO - 23/04/12 13:54:27 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-2ef7f6b5-670a-4ea7-8322-939fff390eeb/' directory.
[2023-04-12T13:54:28.545+0000] {subprocess.py:93} INFO - 23/04/12 13:54:27 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=athletesathletes_statsleaders_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weight_lbs, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=height_inches, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayHeight, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-2ef7f6b5-670a-4ea7-8322-939fff390eeb/part-00000-68713956-da4f-4600-999f-2ded1fe814f8-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=1e354d2d-7eb6-4a75-9a14-0f71e17d5a08, location=europe-west6}
[2023-04-12T13:54:31.449+0000] {subprocess.py:93} INFO - 23/04/12 13:54:30 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.athletesathletes_statsleaders_2020_2. jobId: JobId{project=nfl-de-project, job=1e354d2d-7eb6-4a75-9a14-0f71e17d5a08, location=europe-west6}
[2023-04-12T13:54:35.837+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.838+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.840+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:35.841+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:35.841+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:35.842+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:35.843+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:35.843+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:35.844+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:35.844+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:35.845+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:35.846+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:35.847+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:35.848+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:35.849+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:35.849+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:35.850+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.851+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.851+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:35.852+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:35.853+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:35.855+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:35.856+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:35.856+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:35.857+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:35.858+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:35.859+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:35.859+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:35.860+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:35.862+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:35.862+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:35.863+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:35.863+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.864+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.865+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:35.865+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:35.866+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:35.868+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:35.869+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:35.870+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:35.871+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:35.872+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:35.874+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:35.875+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:35.877+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:35.877+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:35.878+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:35.879+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:35.880+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.882+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.883+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:35.884+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:35.885+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:35.885+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:35.886+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:35.886+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:35.887+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:35.888+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:35.889+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:35.890+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:35.892+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:35.893+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:35.894+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:35.894+0000] {subprocess.py:93} INFO - 23/04/12 13:54:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:35.896+0000] {subprocess.py:93} INFO - 23/04/12 13:54:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/_temporary/0/_temporary/attempt_202304121354331144547947273875627_0020_m_000002_28/' directory.
[2023-04-12T13:54:35.897+0000] {subprocess.py:93} INFO - 23/04/12 13:54:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:35.898+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/_temporary/0/_temporary/attempt_20230412135433251543677419083987_0020_m_000001_27/' directory.
[2023-04-12T13:54:35.898+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.899+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:35.899+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:35.900+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:35.900+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:35.901+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:35.902+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:35.904+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:35.904+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:35.905+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:35.906+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:35.906+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:35.907+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:35.907+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:35.908+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:35.909+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:35.910+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/_temporary/0/_temporary/attempt_202304121354337272154554477164250_0020_m_000003_29/' directory.
[2023-04-12T13:54:35.911+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/_temporary/0/_temporary/attempt_202304121354332501865918323068112_0020_m_000000_26/' directory.
[2023-04-12T13:54:35.912+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/_temporary/0/_temporary/attempt_20230412135433117503745918491876_0020_m_000004_30/' directory.
[2023-04-12T13:54:35.912+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/' directory.
[2023-04-12T13:54:35.913+0000] {subprocess.py:93} INFO - 23/04/12 13:54:35 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_defense_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/part-00001-bc1fdd1d-9dec-43c8-ab42-1285f2fd4088-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/part-00002-bc1fdd1d-9dec-43c8-ab42-1285f2fd4088-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/part-00000-bc1fdd1d-9dec-43c8-ab42-1285f2fd4088-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/part-00003-bc1fdd1d-9dec-43c8-ab42-1285f2fd4088-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3e39defe-4b25-43d9-afeb-692388364127/part-00004-bc1fdd1d-9dec-43c8-ab42-1285f2fd4088-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=60807051-9803-4dfb-a014-99073660c467, location=europe-west6}
[2023-04-12T13:54:38.961+0000] {subprocess.py:93} INFO - 23/04/12 13:54:38 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_defense_stats_2020_2. jobId: JobId{project=nfl-de-project, job=60807051-9803-4dfb-a014-99073660c467, location=europe-west6}
[2023-04-12T13:54:38.962+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-12T13:54:51.341+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.342+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.343+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:51.344+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:51.345+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:51.345+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:51.346+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:51.346+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:51.346+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:51.347+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:51.347+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:51.349+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:51.351+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:51.352+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:51.353+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:51.354+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:51.355+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.356+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.357+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:51.358+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:51.358+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:51.359+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:51.359+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:51.360+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:51.360+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:51.360+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:51.361+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:51.361+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:51.362+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:51.363+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:51.363+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:51.364+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:51.365+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.366+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.367+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:51.367+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:51.368+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:51.368+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:51.369+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:51.369+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:51.369+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:51.370+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:51.371+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:51.372+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:51.372+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:51.373+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:51.373+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:51.374+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:51.374+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.375+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.376+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:51.376+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:51.376+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:51.377+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:51.378+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:51.379+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:51.379+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:51.380+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:51.380+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:51.381+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:51.382+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:51.382+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:51.383+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:51.383+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:51.383+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/attempt_202304121354486200653775195479038_0080_m_000002_127/' directory.
[2023-04-12T13:54:51.384+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/attempt_202304121354484570150761160273073_0080_m_000003_128/' directory.
[2023-04-12T13:54:51.384+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/attempt_202304121354487806466458788926515_0080_m_000001_126/' directory.
[2023-04-12T13:54:51.385+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/attempt_202304121354483297310831917247972_0080_m_000000_125/' directory.
[2023-04-12T13:54:51.386+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:51.386+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:51.387+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:51.387+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:51.388+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:51.388+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.389+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:51.389+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:51.390+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:51.390+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:51.391+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:51.391+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:51.391+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:51.392+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:51.393+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:51.394+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:51.394+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:51.394+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:51.395+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:51.395+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:51.395+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:51.396+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:51.397+0000] {subprocess.py:93} INFO - 23/04/12 13:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:51.399+0000] {subprocess.py:93} INFO - 23/04/12 13:54:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/_temporary/0/_temporary/attempt_20230412135448662005262237200742_0080_m_000004_129/' directory.
[2023-04-12T13:54:51.400+0000] {subprocess.py:93} INFO - 23/04/12 13:54:50 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/' directory.
[2023-04-12T13:54:52.830+0000] {subprocess.py:93} INFO - 23/04/12 13:54:50 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=leaders_teammates_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=averageValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/part-00000-2055c1bc-4cee-4906-a3c9-e19a3ae20e11-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/part-00002-2055c1bc-4cee-4906-a3c9-e19a3ae20e11-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/part-00004-2055c1bc-4cee-4906-a3c9-e19a3ae20e11-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/part-00001-2055c1bc-4cee-4906-a3c9-e19a3ae20e11-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-a97aeae3-7bd3-4481-bb17-5c87332b7a47/part-00003-2055c1bc-4cee-4906-a3c9-e19a3ae20e11-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=2bb9ad56-2729-493d-952a-3cb66c736df5, location=europe-west6}
[2023-04-12T13:54:55.668+0000] {subprocess.py:93} INFO - 23/04/12 13:54:53 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.leaders_teammates_2020_2. jobId: JobId{project=nfl-de-project, job=2bb9ad56-2729-493d-952a-3cb66c736df5, location=europe-west6}
[2023-04-12T13:54:55.669+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.670+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.671+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:55.672+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:55.672+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:55.673+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:55.674+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:55.676+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:55.676+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:55.677+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:55.678+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:55.678+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:55.679+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:55.680+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:55.680+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:55.682+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:55.682+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.683+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.684+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.685+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.686+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:55.688+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:55.689+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:55.690+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:55.691+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:55.692+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:55.693+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:55.693+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:55.695+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:55.696+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:55.697+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:55.698+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:55.699+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:55.700+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:55.701+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:55.703+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:55.704+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:55.705+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:55.706+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:55.707+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:55.708+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:55.709+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:55.710+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:55.711+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:55.713+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:55.714+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:55.714+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:55.715+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:55.716+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.717+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.718+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:55.719+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:55.720+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:55.720+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:55.721+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:55.722+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:55.724+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:55.725+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:55.726+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:55.727+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:55.728+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:55.728+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:55.729+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:55.731+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:55.732+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/attempt_202304121354541065539035083824997_0093_m_000001_154/' directory.
[2023-04-12T13:54:55.732+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/attempt_202304121354541227629281277918963_0093_m_000003_156/' directory.
[2023-04-12T13:54:55.733+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/attempt_202304121354546366716246364717339_0093_m_000002_155/' directory.
[2023-04-12T13:54:55.734+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/attempt_202304121354543957346718131824976_0093_m_000000_153/' directory.
[2023-04-12T13:54:55.735+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:55.737+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:55.738+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:55.739+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.740+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T13:54:55.740+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T13:54:55.741+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T13:54:55.742+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T13:54:55.743+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T13:54:55.745+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T13:54:55.746+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T13:54:55.746+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T13:54:55.747+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T13:54:55.748+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T13:54:55.749+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T13:54:55.750+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T13:54:55.751+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T13:54:55.752+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T13:54:55.753+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T13:54:55.754+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T13:54:55.754+0000] {subprocess.py:93} INFO - 23/04/12 13:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/' directory.
[2023-04-12T13:54:55.755+0000] {subprocess.py:93} INFO - 23/04/12 13:54:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/_temporary/0/_temporary/attempt_202304121354542882921606203978195_0093_m_000004_157/' directory.
[2023-04-12T13:54:55.756+0000] {subprocess.py:93} INFO - 23/04/12 13:54:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/' directory.
[2023-04-12T13:55:00.298+0000] {subprocess.py:93} INFO - 23/04/12 13:54:56 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=radar_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/part-00000-c6ef3cf1-3356-46a0-a4c8-b5d0d039d0a4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/part-00002-c6ef3cf1-3356-46a0-a4c8-b5d0d039d0a4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/part-00004-c6ef3cf1-3356-46a0-a4c8-b5d0d039d0a4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/part-00001-c6ef3cf1-3356-46a0-a4c8-b5d0d039d0a4-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-3ba93010-05f2-4663-9381-7286c074c6bf/part-00003-c6ef3cf1-3356-46a0-a4c8-b5d0d039d0a4-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=f4d27cd2-f54d-48d1-bb3b-59a9359ae942, location=europe-west6}
[2023-04-12T13:55:00.299+0000] {subprocess.py:93} INFO - 23/04/12 13:54:59 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.radar_stats_2020_2. jobId: JobId{project=nfl-de-project, job=f4d27cd2-f54d-48d1-bb3b-59a9359ae942, location=europe-west6}
[2023-04-12T13:55:00.300+0000] {subprocess.py:93} INFO - 23/04/12 13:55:00 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T13:55:00.301+0000] {subprocess.py:93} INFO - 23/04/12 13:55:00 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T13:55:03.150+0000] {subprocess.py:93} INFO - 23/04/12 13:55:00 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T13:55:03.151+0000] {subprocess.py:93} INFO - 23/04/12 13:55:00 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T13:55:03.152+0000] {subprocess.py:93} INFO - 23/04/12 13:55:00 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 0fe8cc8b-355d-4f03-a717-6cc61c0d414f.
[2023-04-12T13:55:03.152+0000] {subprocess.py:93} INFO - org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[2023-04-12T13:55:03.153+0000] {subprocess.py:93} INFO - ShuffleQueryStage 9
[2023-04-12T13:55:03.153+0000] {subprocess.py:93} INFO - +- Exchange hashpartitioning(name#2183, 200), ENSURE_REQUIREMENTS, [id=#10115]
[2023-04-12T13:55:03.154+0000] {subprocess.py:93} INFO -    +- *(10) Project [teamId#309, name#2183, value#2184]
[2023-04-12T13:55:03.154+0000] {subprocess.py:93} INFO -       +- *(10) BroadcastHashJoin [team#1945], [displayName#9], Inner, BuildRight, false
[2023-04-12T13:55:03.155+0000] {subprocess.py:93} INFO -          :- Generate stack(9, gamesPlayed, gamesPlayed#2065, totalYDS, totalYDS#2077, totalYDSG, totalYDSG#2089, passingYDS, passingYDS#2101, passingYDSG, passingYDSG#2113, rushingYDS, rushingYDS#2125, rushingYDSG, rushingYDSG#2137, points, points#2149, pointsPerGame, pointsPerGame#2161), [team#1945], false, [name#2183, value#2184]
[2023-04-12T13:55:03.157+0000] {subprocess.py:93} INFO -          :  +- *(9) Project [Team#1868 AS team#1945, cast(('Unnamed: 0_level_0', 'GP')#1869 as double) AS gamesPlayed#2065, cast(('Total', 'YDS')#1870 as double) AS totalYDS#2077, cast(('Total', 'YDS/G')#1871 as double) AS totalYDSG#2089, cast(('Passing', 'YDS')#1872 as double) AS passingYDS#2101, cast(('Passing', 'YDS/G')#1873 as double) AS passingYDSG#2113, cast(('Rushing', 'YDS')#1874 as double) AS rushingYDS#2125, cast(('Rushing', 'YDS/G')#1875 as double) AS rushingYDSG#2137, cast(('Points', 'PTS')#1876 as double) AS points#2149, cast(('Points', 'PTS/G')#1877 as double) AS pointsPerGame#2161]
[2023-04-12T13:55:03.158+0000] {subprocess.py:93} INFO -          :     +- *(9) Filter isnotnull(Team#1868)
[2023-04-12T13:55:03.159+0000] {subprocess.py:93} INFO -          :        +- InMemoryTableScan [('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Unnamed: 0_level_0', 'GP')#1869, Team#1868], [isnotnull(Team#1868)]
[2023-04-12T13:55:03.160+0000] {subprocess.py:93} INFO -          :              +- InMemoryRelation [Team#1868, ('Unnamed: 0_level_0', 'GP')#1869, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, __index_level_0__#1878L], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T13:55:03.161+0000] {subprocess.py:93} INFO -          :                    +- *(1) ColumnarToRow
[2023-04-12T13:55:03.162+0000] {subprocess.py:93} INFO -          :                       +- FileScan parquet [Team#1868,('Unnamed: 0_level_0', 'GP')#1869,('Total', 'YDS')#1870,('Total', 'YDS/G')#1871,('Passing', 'YDS')#1872,('Passing', 'YDS/G')#1873,('Rushing', 'YDS')#1874,('Rushing', 'YDS/G')#1875,('Points', 'PTS')#1876,('Points', 'PTS/G')#1877,__index_level_0__#1878L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams_defense_stats/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Team:string,('Unnamed: 0_level_0', 'GP'):string,('Total', 'YDS'):string,('Total', 'YDS/G')...
[2023-04-12T13:55:03.164+0000] {subprocess.py:93} INFO -          +- BroadcastQueryStage 3
[2023-04-12T13:55:03.165+0000] {subprocess.py:93} INFO -             +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#9677]
[2023-04-12T13:55:03.166+0000] {subprocess.py:93} INFO -                +- *(4) Project [displayName#9, cast(id#1 as int) AS teamId#309]
[2023-04-12T13:55:03.166+0000] {subprocess.py:93} INFO -                   +- *(4) Filter isnotnull(displayName#9)
[2023-04-12T13:55:03.167+0000] {subprocess.py:93} INFO -                      +- InMemoryTableScan [displayName#9, id#1], [isnotnull(displayName#9)]
[2023-04-12T13:55:03.168+0000] {subprocess.py:93} INFO -                            +- InMemoryRelation [$ref#0, id#1, guid#2, uid#3, slug#4, location#5, name#6, nickname#7, abbreviation#8, displayName#9, shortDisplayName#10, color#11, alternateColor#12, isActive#13, isAllStar#14, logos#15, links#16, alternateIds.sdr#17, record.$ref#18, venue.$ref#19, venue.id#20, venue.fullName#21, venue.address.city#22, venue.address.state#23, ... 19 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T13:55:03.171+0000] {subprocess.py:93} INFO -                                  +- *(1) ColumnarToRow
[2023-04-12T13:55:03.173+0000] {subprocess.py:93} INFO -                                     +- FileScan parquet [$ref#0,id#1,guid#2,uid#3,slug#4,location#5,name#6,nickname#7,abbreviation#8,displayName#9,shortDisplayName#10,color#11,alternateColor#12,isActive#13,isAllStar#14,logos#15,links#16,alternateIds.sdr#17,record.$ref#18,venue.$ref#19,venue.id#20,venue.fullName#21,venue.address.city#22,venue.address.state#23,... 19 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<$ref:string,id:string,guid:string,uid:string,slug:string,location:string,name:string,nickn...
[2023-04-12T13:55:03.175+0000] {subprocess.py:93} INFO - 
[2023-04-12T13:55:03.176+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[2023-04-12T13:55:03.177+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:162)
[2023-04-12T13:55:03.178+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:80)
[2023-04-12T13:55:03.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.181+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:80)
[2023-04-12T13:55:03.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:195)
[2023-04-12T13:55:03.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T13:55:03.184+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T13:55:03.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T13:55:03.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T13:55:03.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:179)
[2023-04-12T13:55:03.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
[2023-04-12T13:55:03.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T13:55:03.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T13:55:03.193+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T13:55:03.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T13:55:03.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T13:55:03.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.198+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.199+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.199+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.200+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T13:55:03.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T13:55:03.202+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.202+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T13:55:03.203+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T13:55:03.203+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T13:55:03.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T13:55:03.206+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T13:55:03.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T13:55:03.209+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T13:55:03.210+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T13:55:03.211+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T13:55:03.212+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T13:55:03.213+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T13:55:03.214+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T13:55:03.215+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T13:55:03.215+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T13:55:03.216+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T13:55:03.216+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T13:55:03.217+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.217+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T13:55:03.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T13:55:03.222+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T13:55:03.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T13:55:03.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T13:55:03.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T13:55:03.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T13:55:03.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T13:55:03.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T13:55:03.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T13:55:03.229+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T13:55:03.229+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T13:55:03.230+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T13:55:03.230+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T13:55:03.231+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T13:55:03.231+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T13:55:03.232+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T13:55:03.233+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T13:55:03.234+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T13:55:03.235+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T13:55:03.236+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T13:55:03.237+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T13:55:03.237+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T13:55:03.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T13:55:03.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T13:55:03.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T13:55:03.240+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T13:55:03.240+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T13:55:03.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T13:55:03.242+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T13:55:03.243+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T13:55:03.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T13:55:03.245+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T13:55:03.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T13:55:03.252+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T13:55:03.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T13:55:03.255+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T13:55:03.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T13:55:03.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T13:55:03.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T13:55:03.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T13:55:03.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T13:55:03.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T13:55:03.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T13:55:03.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T13:55:03.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T13:55:03.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T13:55:03.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T13:55:03.277+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T13:55:03.278+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)
[2023-04-12T13:55:03.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T13:55:03.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T13:55:03.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T13:55:03.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.289+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T13:55:03.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T13:55:03.292+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T13:55:03.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T13:55:03.293+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
[2023-04-12T13:55:03.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T13:55:03.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T13:55:03.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.300+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
[2023-04-12T13:55:03.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
[2023-04-12T13:55:03.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:122)
[2023-04-12T13:55:03.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:121)
[2023-04-12T13:55:03.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:162)
[2023-04-12T13:55:03.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[2023-04-12T13:55:03.307+0000] {subprocess.py:93} INFO - 	... 76 more
[2023-04-12T13:55:03.307+0000] {subprocess.py:93} INFO - 23/04/12 13:55:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681307634475-e6330f7a-d483-463c-a2cd-5723c47732ed/' directory.
[2023-04-12T13:55:03.308+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-04-12T13:55:03.308+0000] {subprocess.py:93} INFO -   File "/tmp/f32c156639d24c058425c62db5b26d73/transform_pyspark.py", line 533, in <module>
[2023-04-12T13:55:03.309+0000] {subprocess.py:93} INFO -     classify.write.format('bigquery') \
[2023-04-12T13:55:03.309+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1107, in save
[2023-04-12T13:55:03.311+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2023-04-12T13:55:03.312+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2023-04-12T13:55:03.312+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-04-12T13:55:03.313+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o315.save.
[2023-04-12T13:55:03.314+0000] {subprocess.py:93} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-04-12T13:55:03.314+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:111)
[2023-04-12T13:55:03.315+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T13:55:03.316+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T13:55:03.317+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T13:55:03.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T13:55:03.320+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T13:55:03.321+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T13:55:03.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T13:55:03.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T13:55:03.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T13:55:03.329+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.330+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T13:55:03.331+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T13:55:03.332+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T13:55:03.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T13:55:03.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T13:55:03.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T13:55:03.336+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T13:55:03.336+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T13:55:03.337+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T13:55:03.337+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T13:55:03.338+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T13:55:03.339+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T13:55:03.340+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T13:55:03.341+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T13:55:03.342+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T13:55:03.343+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T13:55:03.343+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T13:55:03.344+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T13:55:03.344+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T13:55:03.345+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job aborted.
[2023-04-12T13:55:03.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
[2023-04-12T13:55:03.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T13:55:03.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T13:55:03.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T13:55:03.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T13:55:03.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.351+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.351+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.352+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T13:55:03.353+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T13:55:03.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T13:55:03.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T13:55:03.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T13:55:03.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T13:55:03.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T13:55:03.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T13:55:03.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T13:55:03.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T13:55:03.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T13:55:03.361+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T13:55:03.361+0000] {subprocess.py:93} INFO - 	... 35 more
[2023-04-12T13:55:03.362+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[2023-04-12T13:55:03.362+0000] {subprocess.py:93} INFO - ShuffleQueryStage 9
[2023-04-12T13:55:03.363+0000] {subprocess.py:93} INFO - +- Exchange hashpartitioning(name#2183, 200), ENSURE_REQUIREMENTS, [id=#10115]
[2023-04-12T13:55:03.364+0000] {subprocess.py:93} INFO -    +- *(10) Project [teamId#309, name#2183, value#2184]
[2023-04-12T13:55:03.365+0000] {subprocess.py:93} INFO -       +- *(10) BroadcastHashJoin [team#1945], [displayName#9], Inner, BuildRight, false
[2023-04-12T13:55:03.365+0000] {subprocess.py:93} INFO -          :- Generate stack(9, gamesPlayed, gamesPlayed#2065, totalYDS, totalYDS#2077, totalYDSG, totalYDSG#2089, passingYDS, passingYDS#2101, passingYDSG, passingYDSG#2113, rushingYDS, rushingYDS#2125, rushingYDSG, rushingYDSG#2137, points, points#2149, pointsPerGame, pointsPerGame#2161), [team#1945], false, [name#2183, value#2184]
[2023-04-12T13:55:03.366+0000] {subprocess.py:93} INFO -          :  +- *(9) Project [Team#1868 AS team#1945, cast(('Unnamed: 0_level_0', 'GP')#1869 as double) AS gamesPlayed#2065, cast(('Total', 'YDS')#1870 as double) AS totalYDS#2077, cast(('Total', 'YDS/G')#1871 as double) AS totalYDSG#2089, cast(('Passing', 'YDS')#1872 as double) AS passingYDS#2101, cast(('Passing', 'YDS/G')#1873 as double) AS passingYDSG#2113, cast(('Rushing', 'YDS')#1874 as double) AS rushingYDS#2125, cast(('Rushing', 'YDS/G')#1875 as double) AS rushingYDSG#2137, cast(('Points', 'PTS')#1876 as double) AS points#2149, cast(('Points', 'PTS/G')#1877 as double) AS pointsPerGame#2161]
[2023-04-12T13:55:03.367+0000] {subprocess.py:93} INFO -          :     +- *(9) Filter isnotnull(Team#1868)
[2023-04-12T13:55:03.368+0000] {subprocess.py:93} INFO -          :        +- InMemoryTableScan [('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Unnamed: 0_level_0', 'GP')#1869, Team#1868], [isnotnull(Team#1868)]
[2023-04-12T13:55:03.368+0000] {subprocess.py:93} INFO -          :              +- InMemoryRelation [Team#1868, ('Unnamed: 0_level_0', 'GP')#1869, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, __index_level_0__#1878L], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T13:55:03.369+0000] {subprocess.py:93} INFO -          :                    +- *(1) ColumnarToRow
[2023-04-12T13:55:03.369+0000] {subprocess.py:93} INFO -          :                       +- FileScan parquet [Team#1868,('Unnamed: 0_level_0', 'GP')#1869,('Total', 'YDS')#1870,('Total', 'YDS/G')#1871,('Passing', 'YDS')#1872,('Passing', 'YDS/G')#1873,('Rushing', 'YDS')#1874,('Rushing', 'YDS/G')#1875,('Points', 'PTS')#1876,('Points', 'PTS/G')#1877,__index_level_0__#1878L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams_defense_stats/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Team:string,('Unnamed: 0_level_0', 'GP'):string,('Total', 'YDS'):string,('Total', 'YDS/G')...
[2023-04-12T13:55:03.370+0000] {subprocess.py:93} INFO -          +- BroadcastQueryStage 3
[2023-04-12T13:55:03.371+0000] {subprocess.py:93} INFO -             +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#9677]
[2023-04-12T13:55:03.371+0000] {subprocess.py:93} INFO -                +- *(4) Project [displayName#9, cast(id#1 as int) AS teamId#309]
[2023-04-12T13:55:03.372+0000] {subprocess.py:93} INFO -                   +- *(4) Filter isnotnull(displayName#9)
[2023-04-12T13:55:03.373+0000] {subprocess.py:93} INFO -                      +- InMemoryTableScan [displayName#9, id#1], [isnotnull(displayName#9)]
[2023-04-12T13:55:03.374+0000] {subprocess.py:93} INFO -                            +- InMemoryRelation [$ref#0, id#1, guid#2, uid#3, slug#4, location#5, name#6, nickname#7, abbreviation#8, displayName#9, shortDisplayName#10, color#11, alternateColor#12, isActive#13, isAllStar#14, logos#15, links#16, alternateIds.sdr#17, record.$ref#18, venue.$ref#19, venue.id#20, venue.fullName#21, venue.address.city#22, venue.address.state#23, ... 19 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T13:55:03.375+0000] {subprocess.py:93} INFO -                                  +- *(1) ColumnarToRow
[2023-04-12T13:55:03.375+0000] {subprocess.py:93} INFO -                                     +- FileScan parquet [$ref#0,id#1,guid#2,uid#3,slug#4,location#5,name#6,nickname#7,abbreviation#8,displayName#9,shortDisplayName#10,color#11,alternateColor#12,isActive#13,isAllStar#14,logos#15,links#16,alternateIds.sdr#17,record.$ref#18,venue.$ref#19,venue.id#20,venue.fullName#21,venue.address.city#22,venue.address.state#23,... 19 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<$ref:string,id:string,guid:string,uid:string,slug:string,location:string,name:string,nickn...
[2023-04-12T13:55:03.376+0000] {subprocess.py:93} INFO - 
[2023-04-12T13:55:03.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[2023-04-12T13:55:03.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:162)
[2023-04-12T13:55:03.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:80)
[2023-04-12T13:55:03.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:80)
[2023-04-12T13:55:03.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:195)
[2023-04-12T13:55:03.382+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T13:55:03.383+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T13:55:03.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T13:55:03.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T13:55:03.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:179)
[2023-04-12T13:55:03.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
[2023-04-12T13:55:03.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.389+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T13:55:03.390+0000] {subprocess.py:93} INFO - 	... 57 more
[2023-04-12T13:55:03.390+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T13:55:03.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T13:55:03.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T13:55:03.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T13:55:03.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T13:55:03.394+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T13:55:03.395+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T13:55:03.396+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T13:55:03.397+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T13:55:03.397+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T13:55:03.398+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T13:55:03.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T13:55:03.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T13:55:03.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T13:55:03.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T13:55:03.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T13:55:03.408+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T13:55:03.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T13:55:03.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T13:55:03.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T13:55:03.414+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T13:55:03.415+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T13:55:03.416+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T13:55:03.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T13:55:03.417+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.418+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.419+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.420+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.421+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T13:55:03.422+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T13:55:03.423+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T13:55:03.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T13:55:03.424+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)
[2023-04-12T13:55:03.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T13:55:03.425+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T13:55:03.426+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.427+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.428+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.429+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.430+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T13:55:03.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.431+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.432+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.433+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.434+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T13:55:03.435+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T13:55:03.436+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T13:55:03.437+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T13:55:03.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
[2023-04-12T13:55:03.438+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T13:55:03.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T13:55:03.439+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T13:55:03.440+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T13:55:03.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T13:55:03.441+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T13:55:03.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T13:55:03.442+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
[2023-04-12T13:55:03.443+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
[2023-04-12T13:55:03.444+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:122)
[2023-04-12T13:55:03.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:121)
[2023-04-12T13:55:03.445+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:162)
[2023-04-12T13:55:03.446+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[2023-04-12T13:55:03.447+0000] {subprocess.py:93} INFO - 	... 76 more
[2023-04-12T13:55:03.447+0000] {subprocess.py:93} INFO - 
[2023-04-12T13:55:03.448+0000] {subprocess.py:93} INFO - 23/04/12 13:55:01 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@37ef5342{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-12T13:55:06.469+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [f32c156639d24c058425c62db5b26d73] failed with error:
[2023-04-12T13:55:06.470+0000] {subprocess.py:93} INFO - Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
[2023-04-12T13:55:06.471+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/f32c156639d24c058425c62db5b26d73?project=nfl-de-project&region=europe-west6
[2023-04-12T13:55:06.471+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait 'f32c156639d24c058425c62db5b26d73' --region 'europe-west6' --project 'nfl-de-project'
[2023-04-12T13:55:06.472+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/f32c156639d24c058425c62db5b26d73/
[2023-04-12T13:55:06.472+0000] {subprocess.py:93} INFO - gs://nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/f32c156639d24c058425c62db5b26d73/driveroutput
[2023-04-12T13:55:07.599+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2023-04-12T13:55:07.961+0000] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py", line 217, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion.py", line 224, in task_pyspark
    operator.execute(context={})
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-04-12T13:55:08.052+0000] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230412T135337, end_date=20230412T135508
[2023-04-12T13:55:08.311+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 957 for task task_pyspark (Bash command failed. The command returned a non-zero exit code 1.; 10638)
[2023-04-12T13:55:08.395+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-04-12T13:55:08.523+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
