[2023-04-12T14:20:59.942+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T14:20:59.964+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T14:20:59.966+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T14:20:59.968+0000] {taskinstance.py:1289} INFO - Starting attempt 24 of 25
[2023-04-12T14:20:59.969+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T14:20:59.995+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-12T14:20:59.997+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance Not Running' FAILED: Task is in the running state
[2023-04-12T14:20:59.999+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state.
[2023-04-12T14:21:00.015+0000] {standard_task_runner.py:55} INFO - Started process 11771 to run task
[2023-04-12T14:21:00.018+0000] {local_task_job.py:151} INFO - Task is not able to be run
[2023-04-12T14:21:00.023+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '959', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmp28r3a1j0']
[2023-04-12T14:21:00.040+0000] {standard_task_runner.py:83} INFO - Job 959: Subtask task_pyspark
[2023-04-12T14:21:00.322+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host a2e619923d04
[2023-04-12T14:21:00.416+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=24
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-12T14:21:00.423+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-12T14:21:00.436+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-de-project/code/transform_pyspark.py                 --                     --year=2020                     --season_type=2 ']
[2023-04-12T14:21:00.455+0000] {subprocess.py:86} INFO - Output:
[2023-04-12T14:21:07.912+0000] {subprocess.py:93} INFO - Job [d91b390178e94bf9a0fd788dd85171f2] submitted.
[2023-04-12T14:21:07.914+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-12T14:21:18.513+0000] {subprocess.py:93} INFO - 23/04/12 14:21:17 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-12T14:21:18.514+0000] {subprocess.py:93} INFO - 23/04/12 14:21:17 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-12T14:21:18.515+0000] {subprocess.py:93} INFO - 23/04/12 14:21:17 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-12T14:21:18.516+0000] {subprocess.py:93} INFO - 23/04/12 14:21:17 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-12T14:21:18.517+0000] {subprocess.py:93} INFO - 23/04/12 14:21:17 INFO org.sparkproject.jetty.util.log: Logging initialized @4747ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-12T14:21:18.517+0000] {subprocess.py:93} INFO - 23/04/12 14:21:18 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-12T14:21:18.518+0000] {subprocess.py:93} INFO - 23/04/12 14:21:18 INFO org.sparkproject.jetty.server.Server: Started @4880ms
[2023-04-12T14:21:18.519+0000] {subprocess.py:93} INFO - 23/04/12 14:21:18 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@5bb3b418{HTTP/1.1, (http/1.1)}{0.0.0.0:38535}
[2023-04-12T14:21:21.512+0000] {subprocess.py:93} INFO - 23/04/12 14:21:19 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:28.027+0000] {subprocess.py:93} INFO - 23/04/12 14:21:26 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-12T14:21:28.028+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-12T14:21:28.029+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-12T14:21:30.582+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-12T14:21:33.590+0000] {subprocess.py:93} INFO - 23/04/12 14:21:33 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-12T14:21:38.325+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.326+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.326+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.327+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.327+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.328+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.329+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.329+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.330+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:38.331+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:38.331+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:38.332+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:38.332+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:38.333+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:38.333+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:38.334+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:38.334+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:38.335+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:38.336+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:38.336+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:38.337+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:38.337+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:38.338+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:38.339+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:38.339+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:38.340+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:38.341+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:38.341+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:38.342+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:38.342+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:38.343+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:38.343+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:38.344+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:38.345+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:38.346+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:38.346+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:38.347+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:38.347+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:38.348+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:38.349+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:38.349+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:38.350+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:38.350+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:38.351+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:38.351+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:38.352+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:38.352+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:38.353+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:38.353+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:38.354+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:38.354+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:38.355+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:38.355+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:38.356+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:38.356+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:38.357+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:38.357+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:38.357+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:38.358+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:38.358+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:38.359+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:38.359+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:38.360+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:38.360+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:38.361+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:21:38.361+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:21:38.362+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:21:38.362+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:21:38.364+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/attempt_20230412142134867767988725491203_0010_m_000001_10/' directory.
[2023-04-12T14:21:38.365+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/attempt_202304121421344361237428232022870_0010_m_000003_12/' directory.
[2023-04-12T14:21:38.366+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/attempt_202304121421341932684116666261176_0010_m_000000_9/' directory.
[2023-04-12T14:21:38.366+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/attempt_202304121421344013252463590103811_0010_m_000002_11/' directory.
[2023-04-12T14:21:38.368+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:38.368+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:38.369+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:38.370+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:38.371+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:38.371+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:38.372+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:38.372+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.373+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:38.373+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:38.374+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:38.374+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:38.374+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:38.375+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:38.376+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:38.377+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:38.378+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:38.379+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:38.380+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:38.380+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:38.382+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:38.382+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:38.383+0000] {subprocess.py:93} INFO - 23/04/12 14:21:35 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:38.383+0000] {subprocess.py:93} INFO - 23/04/12 14:21:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/_temporary/0/_temporary/attempt_202304121421342367413677285648134_0010_m_000004_13/' directory.
[2023-04-12T14:21:38.384+0000] {subprocess.py:93} INFO - 23/04/12 14:21:36 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/' directory.
[2023-04-12T14:21:38.385+0000] {subprocess.py:93} INFO - 23/04/12 14:21:37 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=location, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isAllStar, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/part-00003-1ea49b84-3ccf-4ef3-9f86-9caab7e1ce78-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/part-00000-1ea49b84-3ccf-4ef3-9f86-9caab7e1ce78-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/part-00002-1ea49b84-3ccf-4ef3-9f86-9caab7e1ce78-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/part-00004-1ea49b84-3ccf-4ef3-9f86-9caab7e1ce78-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-1b8e3507-ec97-46c8-a4aa-a1d0710e6abd/part-00001-1ea49b84-3ccf-4ef3-9f86-9caab7e1ce78-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=6998abb8-0f2e-4124-ae0a-7fb2c72ceb0f, location=europe-west6}
[2023-04-12T14:21:41.132+0000] {subprocess.py:93} INFO - 23/04/12 14:21:40 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_2020_2. jobId: JobId{project=nfl-de-project, job=6998abb8-0f2e-4124-ae0a-7fb2c72ceb0f, location=europe-west6}
[2023-04-12T14:21:43.966+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:43.967+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:43.967+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:43.968+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:43.968+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:43.969+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:43.970+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:43.971+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:43.973+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:43.974+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:43.974+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:43.975+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:43.976+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:43.977+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:43.977+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:43.979+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:43.979+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:43.980+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:43.980+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:43.981+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:43.982+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:43.983+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:43.984+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:43.986+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:43.987+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:43.989+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:43.991+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:43.994+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:43.996+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:43.998+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:43.999+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:44.000+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:44.001+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:44.002+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:44.003+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:44.004+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:44.006+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:44.007+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:44.007+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:44.008+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:44.009+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:44.010+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:44.010+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:44.011+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:44.011+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:44.011+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:44.012+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:44.013+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:44.014+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:44.014+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:44.014+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:44.015+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:44.016+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:44.017+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:44.017+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:44.018+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:44.019+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:44.020+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:44.020+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:44.021+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:44.022+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:44.023+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:44.024+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:44.024+0000] {subprocess.py:93} INFO - 23/04/12 14:21:41 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:44.024+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/attempt_202304121421412988660140213228486_0014_m_000000_16/' directory.
[2023-04-12T14:21:44.025+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/attempt_202304121421417025085602894636759_0014_m_000003_19/' directory.
[2023-04-12T14:21:44.026+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/attempt_202304121421414963911646796098499_0014_m_000002_18/' directory.
[2023-04-12T14:21:44.026+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/attempt_202304121421411066901620920791747_0014_m_000001_17/' directory.
[2023-04-12T14:21:44.027+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:44.028+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:44.028+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:44.029+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:44.030+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:44.030+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:44.031+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:44.031+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:44.031+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:44.032+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:44.032+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:44.033+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:44.034+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:44.035+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:44.035+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:44.036+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:44.037+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:44.038+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:44.038+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:44.039+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:44.039+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:44.040+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:21:44.041+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:44.042+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/_temporary/0/_temporary/attempt_20230412142141633267559775135739_0014_m_000004_20/' directory.
[2023-04-12T14:21:44.043+0000] {subprocess.py:93} INFO - 23/04/12 14:21:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/' directory.
[2023-04-12T14:21:44.098+0000] {subprocess.py:93} INFO - 23/04/12 14:21:43 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/part-00000-ca64d263-724f-4863-bd09-9ed299398e26-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/part-00004-ca64d263-724f-4863-bd09-9ed299398e26-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/part-00002-ca64d263-724f-4863-bd09-9ed299398e26-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/part-00001-ca64d263-724f-4863-bd09-9ed299398e26-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-216150a2-84b1-4c43-ac28-4fbeffb456be/part-00003-ca64d263-724f-4863-bd09-9ed299398e26-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=d0e4a7ef-840c-4538-9eb9-25ad9576934c, location=europe-west6}
[2023-04-12T14:21:48.693+0000] {subprocess.py:93} INFO - 23/04/12 14:21:45 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_stats_2020_2. jobId: JobId{project=nfl-de-project, job=d0e4a7ef-840c-4538-9eb9-25ad9576934c, location=europe-west6}
[2023-04-12T14:21:51.542+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:51.543+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:51.544+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:51.544+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:51.545+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:51.545+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:51.546+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:51.546+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:51.546+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:51.547+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:51.547+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:51.548+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:51.548+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:51.549+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:51.550+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:51.551+0000] {subprocess.py:93} INFO - 23/04/12 14:21:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:51.551+0000] {subprocess.py:93} INFO - 23/04/12 14:21:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-ddb26696-94c1-4adc-b14c-65af58cea5df/_temporary/0/_temporary/attempt_202304121421461904241414833022954_0016_m_000000_22/' directory.
[2023-04-12T14:21:51.552+0000] {subprocess.py:93} INFO - 23/04/12 14:21:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-ddb26696-94c1-4adc-b14c-65af58cea5df/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:51.552+0000] {subprocess.py:93} INFO - 23/04/12 14:21:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-ddb26696-94c1-4adc-b14c-65af58cea5df/' directory.
[2023-04-12T14:21:51.552+0000] {subprocess.py:93} INFO - 23/04/12 14:21:50 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=athletesathletes_statsleaders_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weight_lbs, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=height_inches, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayHeight, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-ddb26696-94c1-4adc-b14c-65af58cea5df/part-00000-aa3dfd7a-be87-4f41-bbfd-1328c0463a8c-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=82a51dac-8641-4438-9dcc-10b2be4a0347, location=europe-west6}
[2023-04-12T14:21:54.095+0000] {subprocess.py:93} INFO - 23/04/12 14:21:52 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.athletesathletes_statsleaders_2020_2. jobId: JobId{project=nfl-de-project, job=82a51dac-8641-4438-9dcc-10b2be4a0347, location=europe-west6}
[2023-04-12T14:21:58.787+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.788+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.788+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:58.789+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:58.790+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:58.791+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:58.791+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:58.792+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:58.793+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:58.793+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:58.794+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:58.794+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:58.795+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:58.796+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:58.797+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:58.797+0000] {subprocess.py:93} INFO - 23/04/12 14:21:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:58.798+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.799+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.800+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:58.800+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:58.801+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:58.801+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:58.801+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:58.802+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:58.802+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:58.803+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:58.803+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:58.804+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:58.805+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:58.806+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:58.807+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:58.808+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:58.809+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.810+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.811+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:58.812+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:58.813+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:58.814+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:58.814+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:58.815+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:58.815+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:58.816+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:58.816+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:58.817+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:58.817+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:58.818+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:58.818+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:58.819+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:58.820+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.821+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.822+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:58.823+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:58.824+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:58.824+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:58.825+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:58.825+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:58.826+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:58.827+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:58.827+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:58.828+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:58.828+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:58.829+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:58.829+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:58.830+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:58.830+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/_temporary/0/_temporary/attempt_202304121421556658210017096876895_0020_m_000000_26/' directory.
[2023-04-12T14:21:58.831+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/_temporary/0/_temporary/' directory.
[2023-04-12T14:21:58.832+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/_temporary/0/_temporary/attempt_20230412142155465522389631101368_0020_m_000001_27/' directory.
[2023-04-12T14:21:58.832+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/_temporary/0/_temporary/attempt_202304121421554792423865418657266_0020_m_000003_29/' directory.
[2023-04-12T14:21:58.833+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.834+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:21:58.834+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:21:58.835+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:21:58.836+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:21:58.836+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:21:58.836+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:21:58.837+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:21:58.837+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:21:58.838+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:21:58.839+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:21:58.840+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:21:58.841+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:21:58.842+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/_temporary/0/_temporary/attempt_202304121421552780521673872876039_0020_m_000002_28/' directory.
[2023-04-12T14:21:58.843+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:21:58.843+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:21:58.844+0000] {subprocess.py:93} INFO - 23/04/12 14:21:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:21:58.844+0000] {subprocess.py:93} INFO - 23/04/12 14:21:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/_temporary/0/_temporary/attempt_202304121421555448243430899949345_0020_m_000004_30/' directory.
[2023-04-12T14:21:58.845+0000] {subprocess.py:93} INFO - 23/04/12 14:21:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/' directory.
[2023-04-12T14:21:58.846+0000] {subprocess.py:93} INFO - 23/04/12 14:21:57 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_defense_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/part-00000-927609ad-ba90-4c5a-9726-082f372294f0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/part-00003-927609ad-ba90-4c5a-9726-082f372294f0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/part-00004-927609ad-ba90-4c5a-9726-082f372294f0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/part-00001-927609ad-ba90-4c5a-9726-082f372294f0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-b717749d-f8e1-4cc6-8ba7-dcdc9560d997/part-00002-927609ad-ba90-4c5a-9726-082f372294f0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=d6bbfa58-491e-4bf4-b7df-1f13d7497ab1, location=europe-west6}
[2023-04-12T14:22:01.672+0000] {subprocess.py:93} INFO - 23/04/12 14:22:00 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_defense_stats_2020_2. jobId: JobId{project=nfl-de-project, job=d6bbfa58-491e-4bf4-b7df-1f13d7497ab1, location=europe-west6}
[2023-04-12T14:22:01.674+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-12T14:22:13.748+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.749+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.749+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:13.750+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:13.751+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:13.751+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:13.752+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:13.752+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:13.753+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:13.754+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:13.754+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:13.755+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:13.755+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:13.756+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:13.756+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:13.757+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:13.758+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.758+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.759+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:13.759+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:13.760+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:13.761+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:13.761+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:13.762+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:13.762+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:13.763+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:13.763+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:13.764+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:13.765+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:13.766+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:13.767+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:13.767+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:13.768+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.769+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.771+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:13.772+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:13.773+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:13.775+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:13.776+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:13.777+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:13.777+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:13.778+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:13.779+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:13.780+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:13.781+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:13.781+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:13.782+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:13.783+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:13.784+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.784+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:13.785+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:13.785+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:13.786+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:13.786+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:13.787+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:13.788+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:13.789+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:13.789+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:13.790+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:13.791+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:13.791+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:13.792+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:13.792+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:13.793+0000] {subprocess.py:93} INFO - 23/04/12 14:22:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:15.167+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/attempt_202304121422121948749031793039501_0091_m_000000_131/' directory.
[2023-04-12T14:22:15.167+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/attempt_202304121422123850845824315869806_0091_m_000001_132/' directory.
[2023-04-12T14:22:15.168+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/attempt_202304121422123639214798736274954_0091_m_000002_133/' directory.
[2023-04-12T14:22:15.169+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/attempt_202304121422122950935875413854474_0091_m_000003_134/' directory.
[2023-04-12T14:22:15.169+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/' directory.
[2023-04-12T14:22:15.170+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:15.170+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:15.171+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:15.171+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:15.172+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:15.172+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:15.173+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:15.174+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:15.174+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:15.175+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:15.175+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:15.176+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:15.176+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:15.177+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:15.177+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:15.178+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:15.178+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:22:15.179+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/' directory.
[2023-04-12T14:22:15.179+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/_temporary/0/_temporary/attempt_202304121422126555758571205486248_0091_m_000004_135/' directory.
[2023-04-12T14:22:15.179+0000] {subprocess.py:93} INFO - 23/04/12 14:22:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/' directory.
[2023-04-12T14:22:17.981+0000] {subprocess.py:93} INFO - 23/04/12 14:22:14 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=leaders_teammates_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=averageValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/part-00000-92dc07fd-f1a6-4dce-a8b5-357319adfeb5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/part-00004-92dc07fd-f1a6-4dce-a8b5-357319adfeb5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/part-00002-92dc07fd-f1a6-4dce-a8b5-357319adfeb5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/part-00001-92dc07fd-f1a6-4dce-a8b5-357319adfeb5-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-57d0ca1d-bf37-4b72-8d6d-6fc5c94b8dfb/part-00003-92dc07fd-f1a6-4dce-a8b5-357319adfeb5-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=ee0eb8e5-5ab1-4531-b639-4abd12cb825a, location=europe-west6}
[2023-04-12T14:22:17.982+0000] {subprocess.py:93} INFO - 23/04/12 14:22:16 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.leaders_teammates_2020_2. jobId: JobId{project=nfl-de-project, job=ee0eb8e5-5ab1-4531-b639-4abd12cb825a, location=europe-west6}
[2023-04-12T14:22:17.982+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:17.983+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:17.984+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:17.985+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:17.985+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:17.986+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:17.987+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:17.988+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:17.989+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:17.990+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:17.990+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:17.991+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:17.991+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:17.992+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:17.993+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:17.994+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:17.994+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:17.995+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:17.996+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:17.997+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:17.998+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:17.999+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:18.000+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:18.001+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:18.002+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:18.002+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:18.003+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:18.004+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:18.005+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:18.007+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:18.009+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:18.011+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:18.013+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:18.015+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:18.016+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:18.017+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:18.018+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:18.019+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:18.020+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:18.021+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:18.021+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:18.022+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:18.022+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:18.023+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:18.024+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:18.025+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:18.025+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:18.026+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:18.026+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:18.027+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:18.027+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:18.028+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:18.029+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:18.029+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:18.030+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:18.030+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:18.031+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:18.032+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:18.032+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:18.033+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:18.033+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:18.034+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:18.034+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:18.035+0000] {subprocess.py:93} INFO - 23/04/12 14:22:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:22.722+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/attempt_202304121422175328272028862278458_0104_m_000002_161/' directory.
[2023-04-12T14:22:22.723+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/attempt_202304121422172739765178302141087_0104_m_000001_160/' directory.
[2023-04-12T14:22:22.724+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/attempt_2023041214221786512254525652719_0104_m_000000_159/' directory.
[2023-04-12T14:22:22.725+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/' directory.
[2023-04-12T14:22:22.726+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:22:22.727+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/' directory.
[2023-04-12T14:22:22.727+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:22.728+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:22:22.728+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:22:22.729+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:22:22.731+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:22:22.732+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:22:22.734+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:22:22.735+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:22:22.736+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:22:22.737+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:22:22.739+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:22:22.739+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:22:22.740+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:22:22.741+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:22:22.742+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:22:22.743+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:22:22.744+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/attempt_202304121422174783442612429346500_0104_m_000003_162/' directory.
[2023-04-12T14:22:22.746+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:22:22.746+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/' directory.
[2023-04-12T14:22:22.748+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/_temporary/0/_temporary/attempt_202304121422173096715646506531202_0104_m_000004_163/' directory.
[2023-04-12T14:22:22.748+0000] {subprocess.py:93} INFO - 23/04/12 14:22:18 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/' directory.
[2023-04-12T14:22:22.749+0000] {subprocess.py:93} INFO - 23/04/12 14:22:19 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=radar_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/part-00001-52066e1f-8730-4746-b4d8-31f293a17332-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/part-00003-52066e1f-8730-4746-b4d8-31f293a17332-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/part-00000-52066e1f-8730-4746-b4d8-31f293a17332-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/part-00002-52066e1f-8730-4746-b4d8-31f293a17332-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-cd2a586b-2791-4bf3-9c58-6b49f55bb3c7/part-00004-52066e1f-8730-4746-b4d8-31f293a17332-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=4d1f3d67-e9a8-4fd8-b097-7db70757ab5f, location=europe-west6}
[2023-04-12T14:22:22.750+0000] {subprocess.py:93} INFO - 23/04/12 14:22:20 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.radar_stats_2020_2. jobId: JobId{project=nfl-de-project, job=4d1f3d67-e9a8-4fd8-b097-7db70757ab5f, location=europe-west6}
[2023-04-12T14:22:22.751+0000] {subprocess.py:93} INFO - 23/04/12 14:22:21 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:22:22.752+0000] {subprocess.py:93} INFO - 23/04/12 14:22:21 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:22:22.753+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:22:22.753+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:22:22.754+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:22:22.755+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:22:22.756+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 6d55e5ef-eff9-45b0-8c12-969568e8bbcd.
[2023-04-12T14:22:22.757+0000] {subprocess.py:93} INFO - org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[2023-04-12T14:22:22.758+0000] {subprocess.py:93} INFO - ShuffleQueryStage 11
[2023-04-12T14:22:22.759+0000] {subprocess.py:93} INFO - +- Exchange hashpartitioning(name#2183, 200), ENSURE_REQUIREMENTS, [id=#11407]
[2023-04-12T14:22:22.760+0000] {subprocess.py:93} INFO -    +- *(11) Project [teamId#309, name#2183, value#2184]
[2023-04-12T14:22:22.761+0000] {subprocess.py:93} INFO -       +- *(11) BroadcastHashJoin [team#1945], [displayName#9], Inner, BuildRight, false
[2023-04-12T14:22:22.762+0000] {subprocess.py:93} INFO -          :- Generate stack(9, gamesPlayed, gamesPlayed#2065, totalYDS, totalYDS#2077, totalYDSG, totalYDSG#2089, passingYDS, passingYDS#2101, passingYDSG, passingYDSG#2113, rushingYDS, rushingYDS#2125, rushingYDSG, rushingYDSG#2137, points, points#2149, pointsPerGame, pointsPerGame#2161), [team#1945], false, [name#2183, value#2184]
[2023-04-12T14:22:22.762+0000] {subprocess.py:93} INFO -          :  +- *(10) Project [Team#1868 AS team#1945, cast(('Unnamed: 0_level_0', 'GP')#1869 as double) AS gamesPlayed#2065, cast(('Total', 'YDS')#1870 as double) AS totalYDS#2077, cast(('Total', 'YDS/G')#1871 as double) AS totalYDSG#2089, cast(('Passing', 'YDS')#1872 as double) AS passingYDS#2101, cast(('Passing', 'YDS/G')#1873 as double) AS passingYDSG#2113, cast(('Rushing', 'YDS')#1874 as double) AS rushingYDS#2125, cast(('Rushing', 'YDS/G')#1875 as double) AS rushingYDSG#2137, cast(('Points', 'PTS')#1876 as double) AS points#2149, cast(('Points', 'PTS/G')#1877 as double) AS pointsPerGame#2161]
[2023-04-12T14:22:22.763+0000] {subprocess.py:93} INFO -          :     +- *(10) Filter isnotnull(Team#1868)
[2023-04-12T14:22:22.764+0000] {subprocess.py:93} INFO -          :        +- InMemoryTableScan [('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Unnamed: 0_level_0', 'GP')#1869, Team#1868], [isnotnull(Team#1868)]
[2023-04-12T14:22:22.765+0000] {subprocess.py:93} INFO -          :              +- InMemoryRelation [Team#1868, ('Unnamed: 0_level_0', 'GP')#1869, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, __index_level_0__#1878L], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:22:22.767+0000] {subprocess.py:93} INFO -          :                    +- *(1) ColumnarToRow
[2023-04-12T14:22:22.767+0000] {subprocess.py:93} INFO -          :                       +- FileScan parquet [Team#1868,('Unnamed: 0_level_0', 'GP')#1869,('Total', 'YDS')#1870,('Total', 'YDS/G')#1871,('Passing', 'YDS')#1872,('Passing', 'YDS/G')#1873,('Rushing', 'YDS')#1874,('Rushing', 'YDS/G')#1875,('Points', 'PTS')#1876,('Points', 'PTS/G')#1877,__index_level_0__#1878L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams_defense_stats/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Team:string,('Unnamed: 0_level_0', 'GP'):string,('Total', 'YDS'):string,('Total', 'YDS/G')...
[2023-04-12T14:22:22.771+0000] {subprocess.py:93} INFO -          +- BroadcastQueryStage 3
[2023-04-12T14:22:22.773+0000] {subprocess.py:93} INFO -             +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#10668]
[2023-04-12T14:22:22.775+0000] {subprocess.py:93} INFO -                +- *(4) Project [displayName#9, cast(id#1 as int) AS teamId#309]
[2023-04-12T14:22:22.776+0000] {subprocess.py:93} INFO -                   +- *(4) Filter isnotnull(displayName#9)
[2023-04-12T14:22:22.777+0000] {subprocess.py:93} INFO -                      +- InMemoryTableScan [displayName#9, id#1], [isnotnull(displayName#9)]
[2023-04-12T14:22:22.778+0000] {subprocess.py:93} INFO -                            +- InMemoryRelation [$ref#0, id#1, guid#2, uid#3, slug#4, location#5, name#6, nickname#7, abbreviation#8, displayName#9, shortDisplayName#10, color#11, alternateColor#12, isActive#13, isAllStar#14, logos#15, links#16, alternateIds.sdr#17, record.$ref#18, venue.$ref#19, venue.id#20, venue.fullName#21, venue.address.city#22, venue.address.state#23, ... 19 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:22:22.780+0000] {subprocess.py:93} INFO -                                  +- *(1) ColumnarToRow
[2023-04-12T14:22:22.781+0000] {subprocess.py:93} INFO -                                     +- FileScan parquet [$ref#0,id#1,guid#2,uid#3,slug#4,location#5,name#6,nickname#7,abbreviation#8,displayName#9,shortDisplayName#10,color#11,alternateColor#12,isActive#13,isAllStar#14,logos#15,links#16,alternateIds.sdr#17,record.$ref#18,venue.$ref#19,venue.id#20,venue.fullName#21,venue.address.city#22,venue.address.state#23,... 19 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<$ref:string,id:string,guid:string,uid:string,slug:string,location:string,name:string,nickn...
[2023-04-12T14:22:22.783+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:22:22.784+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[2023-04-12T14:22:22.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:162)
[2023-04-12T14:22:22.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:80)
[2023-04-12T14:22:22.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:80)
[2023-04-12T14:22:22.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:195)
[2023-04-12T14:22:22.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:22:22.795+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:22:22.797+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:22:22.798+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:22:22.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:179)
[2023-04-12T14:22:22.803+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
[2023-04-12T14:22:22.806+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.811+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.817+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.821+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.824+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.825+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T14:22:22.826+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T14:22:22.828+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T14:22:22.830+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T14:22:22.832+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T14:22:22.834+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.834+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.835+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.836+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.837+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.838+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:22:22.839+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:22:22.839+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:22:22.840+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:22:22.841+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:22:22.842+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:22:22.844+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:22:22.844+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:22:22.845+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:22:22.846+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:22:22.846+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:22:22.847+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T14:22:22.848+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T14:22:22.849+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T14:22:22.850+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T14:22:22.851+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T14:22:22.851+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T14:22:22.852+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T14:22:22.853+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T14:22:22.854+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T14:22:22.854+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.855+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.856+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.857+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.859+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:22:22.865+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:22:22.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:22:22.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:22:22.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:22:22.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:22:22.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:22:22.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:22:22.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:22:22.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:22:22.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:22:22.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T14:22:22.887+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T14:22:22.888+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T14:22:22.889+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T14:22:22.891+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T14:22:22.893+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T14:22:22.894+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T14:22:22.895+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T14:22:22.895+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T14:22:22.896+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T14:22:22.897+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T14:22:22.898+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T14:22:22.899+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T14:22:22.900+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T14:22:22.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T14:22:22.901+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T14:22:22.902+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T14:22:22.903+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:22:22.903+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T14:22:22.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T14:22:22.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T14:22:22.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T14:22:22.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T14:22:22.908+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:22:22.908+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:22:22.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T14:22:22.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:22:22.913+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.914+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.915+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.916+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:22:22.916+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T14:22:22.917+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:22:22.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.924+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T14:22:22.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T14:22:22.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T14:22:22.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T14:22:22.931+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T14:22:22.933+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T14:22:22.935+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.937+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.938+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.941+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.942+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.944+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:22:22.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:22:22.945+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:22:22.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:22:22.946+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)
[2023-04-12T14:22:22.948+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:22:22.949+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:22:22.950+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.951+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.952+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.953+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.954+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.956+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T14:22:22.957+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.958+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.959+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.960+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.961+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:22:22.962+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:22:22.963+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:22:22.964+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:22:22.965+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
[2023-04-12T14:22:22.966+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:22:22.967+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:22:22.968+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.969+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.970+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.971+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.972+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
[2023-04-12T14:22:22.972+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
[2023-04-12T14:22:22.973+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:122)
[2023-04-12T14:22:22.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:121)
[2023-04-12T14:22:22.974+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:162)
[2023-04-12T14:22:22.975+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[2023-04-12T14:22:22.976+0000] {subprocess.py:93} INFO - 	... 76 more
[2023-04-12T14:22:22.977+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 118.0 (TID 183) (nfl-spark-cluster-m.europe-west6-a.c.nfl-de-project.internal executor driver): TaskKilled (Stage cancelled)
[2023-04-12T14:22:22.977+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 118.0 (TID 182) (nfl-spark-cluster-m.europe-west6-a.c.nfl-de-project.internal executor driver): TaskKilled (Stage cancelled)
[2023-04-12T14:22:22.978+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 118.0 (TID 184) (nfl-spark-cluster-m.europe-west6-a.c.nfl-de-project.internal executor driver): TaskKilled (Stage cancelled)
[2023-04-12T14:22:22.978+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 118.0 (TID 185) (nfl-spark-cluster-m.europe-west6-a.c.nfl-de-project.internal executor driver): TaskKilled (Stage cancelled)
[2023-04-12T14:22:22.979+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309278283-56897c8d-491e-4365-9e74-cc943154cd04/' directory.
[2023-04-12T14:22:22.979+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-04-12T14:22:22.980+0000] {subprocess.py:93} INFO -   File "/tmp/d91b390178e94bf9a0fd788dd85171f2/transform_pyspark.py", line 533, in <module>
[2023-04-12T14:22:22.981+0000] {subprocess.py:93} INFO -     classify.write.format('bigquery') \
[2023-04-12T14:22:22.981+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1107, in save
[2023-04-12T14:22:22.982+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2023-04-12T14:22:22.983+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2023-04-12T14:22:22.984+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-04-12T14:22:22.985+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o315.save.
[2023-04-12T14:22:22.986+0000] {subprocess.py:93} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-04-12T14:22:22.986+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:111)
[2023-04-12T14:22:22.987+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T14:22:22.987+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T14:22:22.988+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T14:22:22.988+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T14:22:22.989+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T14:22:22.990+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T14:22:22.991+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T14:22:22.992+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:22.992+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:22.993+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:22.994+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:22.994+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:22.995+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:22:22.996+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:22:22.997+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:22:22.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:22:22.998+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:22:22.999+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:22:23.001+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:22:23.002+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:22:23.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:22:23.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:22:23.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:22:23.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T14:22:23.008+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T14:22:23.009+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T14:22:23.011+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T14:22:23.012+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T14:22:23.013+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T14:22:23.014+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T14:22:23.015+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T14:22:23.016+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T14:22:23.017+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T14:22:23.018+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T14:22:23.019+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T14:22:23.020+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job aborted.
[2023-04-12T14:22:23.020+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
[2023-04-12T14:22:23.021+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T14:22:23.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T14:22:23.022+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T14:22:23.023+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T14:22:23.024+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.025+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.026+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:22:23.030+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:22:23.030+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:22:23.031+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:22:23.033+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:22:23.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:22:23.035+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:22:23.036+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:22:23.037+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:22:23.037+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:22:23.038+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:22:23.039+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T14:22:23.040+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T14:22:23.041+0000] {subprocess.py:93} INFO - 	... 35 more
[2023-04-12T14:22:23.042+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[2023-04-12T14:22:23.042+0000] {subprocess.py:93} INFO - ShuffleQueryStage 11
[2023-04-12T14:22:23.043+0000] {subprocess.py:93} INFO - +- Exchange hashpartitioning(name#2183, 200), ENSURE_REQUIREMENTS, [id=#11407]
[2023-04-12T14:22:23.044+0000] {subprocess.py:93} INFO -    +- *(11) Project [teamId#309, name#2183, value#2184]
[2023-04-12T14:22:23.044+0000] {subprocess.py:93} INFO -       +- *(11) BroadcastHashJoin [team#1945], [displayName#9], Inner, BuildRight, false
[2023-04-12T14:22:23.045+0000] {subprocess.py:93} INFO -          :- Generate stack(9, gamesPlayed, gamesPlayed#2065, totalYDS, totalYDS#2077, totalYDSG, totalYDSG#2089, passingYDS, passingYDS#2101, passingYDSG, passingYDSG#2113, rushingYDS, rushingYDS#2125, rushingYDSG, rushingYDSG#2137, points, points#2149, pointsPerGame, pointsPerGame#2161), [team#1945], false, [name#2183, value#2184]
[2023-04-12T14:22:23.046+0000] {subprocess.py:93} INFO -          :  +- *(10) Project [Team#1868 AS team#1945, cast(('Unnamed: 0_level_0', 'GP')#1869 as double) AS gamesPlayed#2065, cast(('Total', 'YDS')#1870 as double) AS totalYDS#2077, cast(('Total', 'YDS/G')#1871 as double) AS totalYDSG#2089, cast(('Passing', 'YDS')#1872 as double) AS passingYDS#2101, cast(('Passing', 'YDS/G')#1873 as double) AS passingYDSG#2113, cast(('Rushing', 'YDS')#1874 as double) AS rushingYDS#2125, cast(('Rushing', 'YDS/G')#1875 as double) AS rushingYDSG#2137, cast(('Points', 'PTS')#1876 as double) AS points#2149, cast(('Points', 'PTS/G')#1877 as double) AS pointsPerGame#2161]
[2023-04-12T14:22:23.047+0000] {subprocess.py:93} INFO -          :     +- *(10) Filter isnotnull(Team#1868)
[2023-04-12T14:22:23.048+0000] {subprocess.py:93} INFO -          :        +- InMemoryTableScan [('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Unnamed: 0_level_0', 'GP')#1869, Team#1868], [isnotnull(Team#1868)]
[2023-04-12T14:22:23.049+0000] {subprocess.py:93} INFO -          :              +- InMemoryRelation [Team#1868, ('Unnamed: 0_level_0', 'GP')#1869, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, __index_level_0__#1878L], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:22:23.050+0000] {subprocess.py:93} INFO -          :                    +- *(1) ColumnarToRow
[2023-04-12T14:22:23.050+0000] {subprocess.py:93} INFO -          :                       +- FileScan parquet [Team#1868,('Unnamed: 0_level_0', 'GP')#1869,('Total', 'YDS')#1870,('Total', 'YDS/G')#1871,('Passing', 'YDS')#1872,('Passing', 'YDS/G')#1873,('Rushing', 'YDS')#1874,('Rushing', 'YDS/G')#1875,('Points', 'PTS')#1876,('Points', 'PTS/G')#1877,__index_level_0__#1878L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams_defense_stats/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Team:string,('Unnamed: 0_level_0', 'GP'):string,('Total', 'YDS'):string,('Total', 'YDS/G')...
[2023-04-12T14:22:23.051+0000] {subprocess.py:93} INFO -          +- BroadcastQueryStage 3
[2023-04-12T14:22:23.051+0000] {subprocess.py:93} INFO -             +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#10668]
[2023-04-12T14:22:23.052+0000] {subprocess.py:93} INFO -                +- *(4) Project [displayName#9, cast(id#1 as int) AS teamId#309]
[2023-04-12T14:22:23.053+0000] {subprocess.py:93} INFO -                   +- *(4) Filter isnotnull(displayName#9)
[2023-04-12T14:22:23.055+0000] {subprocess.py:93} INFO -                      +- InMemoryTableScan [displayName#9, id#1], [isnotnull(displayName#9)]
[2023-04-12T14:22:23.056+0000] {subprocess.py:93} INFO -                            +- InMemoryRelation [$ref#0, id#1, guid#2, uid#3, slug#4, location#5, name#6, nickname#7, abbreviation#8, displayName#9, shortDisplayName#10, color#11, alternateColor#12, isActive#13, isAllStar#14, logos#15, links#16, alternateIds.sdr#17, record.$ref#18, venue.$ref#19, venue.id#20, venue.fullName#21, venue.address.city#22, venue.address.state#23, ... 19 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:22:23.056+0000] {subprocess.py:93} INFO -                                  +- *(1) ColumnarToRow
[2023-04-12T14:22:23.057+0000] {subprocess.py:93} INFO -                                     +- FileScan parquet [$ref#0,id#1,guid#2,uid#3,slug#4,location#5,name#6,nickname#7,abbreviation#8,displayName#9,shortDisplayName#10,color#11,alternateColor#12,isActive#13,isAllStar#14,logos#15,links#16,alternateIds.sdr#17,record.$ref#18,venue.$ref#19,venue.id#20,venue.fullName#21,venue.address.city#22,venue.address.state#23,... 19 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<$ref:string,id:string,guid:string,uid:string,slug:string,location:string,name:string,nickn...
[2023-04-12T14:22:23.057+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:22:23.058+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[2023-04-12T14:22:23.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:162)
[2023-04-12T14:22:23.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:80)
[2023-04-12T14:22:23.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.063+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.065+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:80)
[2023-04-12T14:22:23.066+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:195)
[2023-04-12T14:22:23.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:22:23.069+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:22:23.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:22:23.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:22:23.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:179)
[2023-04-12T14:22:23.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
[2023-04-12T14:22:23.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.076+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.077+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.078+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.079+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.081+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T14:22:23.082+0000] {subprocess.py:93} INFO - 	... 57 more
[2023-04-12T14:22:23.082+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T14:22:23.083+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T14:22:23.084+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T14:22:23.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T14:22:23.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T14:22:23.086+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:22:23.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T14:22:23.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T14:22:23.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T14:22:23.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T14:22:23.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T14:22:23.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:22:23.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:22:23.094+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T14:22:23.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:22:23.096+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:22:23.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T14:22:23.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:22:23.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.103+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.104+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.107+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.109+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.111+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T14:22:23.113+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T14:22:23.114+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T14:22:23.116+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T14:22:23.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T14:22:23.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T14:22:23.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:22:23.126+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:22:23.127+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:22:23.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:22:23.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)
[2023-04-12T14:22:23.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:22:23.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:22:23.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.136+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.137+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T14:22:23.138+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.140+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.141+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.142+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.143+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.145+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:22:23.147+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:22:23.148+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:22:23.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:22:23.151+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
[2023-04-12T14:22:23.153+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:22:23.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:22:23.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:22:23.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:22:23.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:22:23.161+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:22:23.161+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:22:23.162+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
[2023-04-12T14:22:23.164+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
[2023-04-12T14:22:23.166+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:122)
[2023-04-12T14:22:23.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:121)
[2023-04-12T14:22:23.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:162)
[2023-04-12T14:22:23.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[2023-04-12T14:22:23.173+0000] {subprocess.py:93} INFO - 	... 76 more
[2023-04-12T14:22:23.174+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:22:23.175+0000] {subprocess.py:93} INFO - 23/04/12 14:22:22 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@5bb3b418{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-12T14:22:27.890+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [d91b390178e94bf9a0fd788dd85171f2] failed with error:
[2023-04-12T14:22:27.891+0000] {subprocess.py:93} INFO - Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
[2023-04-12T14:22:27.892+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/d91b390178e94bf9a0fd788dd85171f2?project=nfl-de-project&region=europe-west6
[2023-04-12T14:22:27.892+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait 'd91b390178e94bf9a0fd788dd85171f2' --region 'europe-west6' --project 'nfl-de-project'
[2023-04-12T14:22:27.893+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/d91b390178e94bf9a0fd788dd85171f2/
[2023-04-12T14:22:27.894+0000] {subprocess.py:93} INFO - gs://nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/d91b390178e94bf9a0fd788dd85171f2/driveroutput
[2023-04-12T14:22:28.774+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2023-04-12T14:22:29.242+0000] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py", line 217, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion.py", line 224, in task_pyspark
    operator.execute(context={})
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-04-12T14:22:29.387+0000] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230412T142059, end_date=20230412T142229
[2023-04-12T14:22:29.915+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 959 for task task_pyspark (Bash command failed. The command returned a non-zero exit code 1.; 11771)
[2023-04-12T14:22:30.013+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-04-12T14:22:30.229+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
