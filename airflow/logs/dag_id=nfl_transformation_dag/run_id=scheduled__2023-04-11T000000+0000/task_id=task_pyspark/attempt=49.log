[2023-04-24T08:52:24.313+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [None]>
[2023-04-24T08:52:24.328+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [None]>
[2023-04-24T08:52:24.329+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T08:52:24.330+0000] {taskinstance.py:1289} INFO - Starting attempt 49 of 51
[2023-04-24T08:52:24.331+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-24T08:52:24.347+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-24T08:52:24.354+0000] {standard_task_runner.py:55} INFO - Started process 1597 to run task
[2023-04-24T08:52:24.357+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '1235', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmp83_7gbn5']
[2023-04-24T08:52:24.359+0000] {standard_task_runner.py:83} INFO - Job 1235: Subtask task_pyspark
[2023-04-24T08:52:24.427+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host 235fa867d994
[2023-04-24T08:52:24.527+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=49
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-24T08:52:24.531+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-24T08:52:24.532+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-24T08:52:24.548+0000] {subprocess.py:86} INFO - Output:
[2023-04-24T08:52:29.761+0000] {subprocess.py:93} INFO - Job [99d80573d9ef4967b6fecb1c8ccf61cc] submitted.
[2023-04-24T08:52:29.762+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-24T08:52:43.973+0000] {subprocess.py:93} INFO - 23/04/24 08:52:36 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-24T08:52:43.974+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-24T08:52:43.975+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-24T08:52:43.976+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-24T08:52:43.976+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.sparkproject.jetty.util.log: Logging initialized @4393ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-24T08:52:43.977+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-24T08:52:43.978+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.sparkproject.jetty.server.Server: Started @4532ms
[2023-04-24T08:52:43.979+0000] {subprocess.py:93} INFO - 23/04/24 08:52:37 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@23a7bbee{HTTP/1.1, (http/1.1)}{0.0.0.0:37023}
[2023-04-24T08:52:43.980+0000] {subprocess.py:93} INFO - 23/04/24 08:52:39 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:52:53.702+0000] {subprocess.py:93} INFO - 23/04/24 08:52:45 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-24T08:52:53.703+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-24T08:52:53.703+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-24T08:52:53.704+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-24T08:52:58.969+0000] {subprocess.py:93} INFO - 23/04/24 08:52:52 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-24T08:52:58.970+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.971+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.973+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.974+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.975+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.976+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.977+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.979+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:58.980+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:52:58.981+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:52:58.981+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:52:58.982+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:52:58.983+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:52:58.984+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:52:58.985+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:52:58.986+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:52:58.987+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:52:58.988+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:52:58.989+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:52:58.990+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:52:58.991+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:52:58.993+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:52:58.994+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:52:58.994+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:52:58.995+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:52:58.996+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:52:58.998+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:52:59.000+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:52:59.001+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:52:59.001+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:52:59.002+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:52:59.003+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:52:59.004+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:52:59.005+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:52:59.006+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:52:59.008+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:52:59.009+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:52:59.010+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:52:59.011+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:52:59.012+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:52:59.014+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:52:59.015+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:52:59.016+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:52:59.017+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:52:59.018+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:52:59.021+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:52:59.022+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:52:59.023+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:52:59.024+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:52:59.025+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:52:59.026+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:52:59.028+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:52:59.029+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:52:59.030+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:52:59.031+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:52:59.031+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:52:59.032+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:52:59.032+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:52:59.033+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:52:59.034+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:52:59.035+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:52:59.036+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:52:59.037+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:52:59.037+0000] {subprocess.py:93} INFO - 23/04/24 08:52:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:52:59.038+0000] {subprocess.py:93} INFO - 23/04/24 08:52:54 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T08:52:59.038+0000] {subprocess.py:93} INFO - 23/04/24 08:52:54 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T08:52:59.039+0000] {subprocess.py:93} INFO - 23/04/24 08:52:54 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T08:52:59.039+0000] {subprocess.py:93} INFO - 23/04/24 08:52:54 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-24T08:52:59.040+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/attempt_20230424085253687241089323573299_0010_m_000001_10/' directory.
[2023-04-24T08:52:59.041+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/attempt_20230424085253676589750482003646_0010_m_000002_11/' directory.
[2023-04-24T08:52:59.042+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/attempt_202304240852538696494024985487635_0010_m_000000_9/' directory.
[2023-04-24T08:52:59.043+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/attempt_202304240852535102809870815230195_0010_m_000003_12/' directory.
[2023-04-24T08:52:59.044+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/' directory.
[2023-04-24T08:52:59.045+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:52:59.046+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/' directory.
[2023-04-24T08:52:59.047+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:52:59.048+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:52:59.049+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/' directory.
[2023-04-24T08:52:59.050+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/' directory.
[2023-04-24T08:52:59.051+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:59.051+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:52:59.052+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:52:59.052+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:52:59.057+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:52:59.058+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:52:59.058+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:52:59.059+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:52:59.060+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:52:59.061+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:52:59.061+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:52:59.063+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:52:59.064+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:52:59.065+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:52:59.065+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:52:59.065+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:03.392+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/_temporary/0/_temporary/attempt_202304240852535134390731518208105_0010_m_000004_13/' directory.
[2023-04-24T08:53:03.393+0000] {subprocess.py:93} INFO - 23/04/24 08:52:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/' directory.
[2023-04-24T08:53:03.394+0000] {subprocess.py:93} INFO - 23/04/24 08:52:56 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/part-00000-7cf28aa4-65ca-4238-ab5d-492a3e0d4bdf-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/part-00004-7cf28aa4-65ca-4238-ab5d-492a3e0d4bdf-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/part-00001-7cf28aa4-65ca-4238-ab5d-492a3e0d4bdf-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/part-00003-7cf28aa4-65ca-4238-ab5d-492a3e0d4bdf-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-29b3436e-f02f-40d9-92d2-64e568d1446e/part-00002-7cf28aa4-65ca-4238-ab5d-492a3e0d4bdf-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=136a2fc8-3023-4bcc-bf11-bcaf8636cd1b, location=europe-west6}
[2023-04-24T08:53:03.395+0000] {subprocess.py:93} INFO - 23/04/24 08:52:57 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=136a2fc8-3023-4bcc-bf11-bcaf8636cd1b, location=europe-west6}
[2023-04-24T08:53:03.396+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:03.396+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:03.397+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:03.398+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:03.399+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:03.401+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:03.402+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:03.403+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:03.404+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:03.404+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:03.405+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:03.406+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:03.407+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:03.408+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:03.409+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:03.410+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:03.411+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-e6305ca7-aa34-481a-95da-c444bda02bc3/_temporary/0/_temporary/attempt_20230424085258816894847092261409_0011_m_000000_14/' directory.
[2023-04-24T08:53:03.412+0000] {subprocess.py:93} INFO - 23/04/24 08:52:58 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-e6305ca7-aa34-481a-95da-c444bda02bc3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:03.412+0000] {subprocess.py:93} INFO - 23/04/24 08:52:59 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-e6305ca7-aa34-481a-95da-c444bda02bc3/' directory.
[2023-04-24T08:53:03.413+0000] {subprocess.py:93} INFO - 23/04/24 08:52:59 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-e6305ca7-aa34-481a-95da-c444bda02bc3/part-00000-fed1ceeb-ea5b-40c0-bfdd-da88b95801c1-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=38a64330-b3a2-4b04-9923-a67b38a60f5f, location=europe-west6}
[2023-04-24T08:53:08.052+0000] {subprocess.py:93} INFO - 23/04/24 08:53:02 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=38a64330-b3a2-4b04-9923-a67b38a60f5f, location=europe-west6}
[2023-04-24T08:53:08.053+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:08.054+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:08.055+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:08.056+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:08.057+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:08.058+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:08.059+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:08.060+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:08.061+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:08.062+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:08.063+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:08.064+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:08.065+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:08.066+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:08.067+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:08.068+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:08.069+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-17e0e627-ab94-47fb-bc74-1dec8ad328ad/_temporary/0/_temporary/attempt_202304240853027136019743629842798_0012_m_000000_15/' directory.
[2023-04-24T08:53:08.070+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-17e0e627-ab94-47fb-bc74-1dec8ad328ad/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:08.071+0000] {subprocess.py:93} INFO - 23/04/24 08:53:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-17e0e627-ab94-47fb-bc74-1dec8ad328ad/' directory.
[2023-04-24T08:53:08.071+0000] {subprocess.py:93} INFO - 23/04/24 08:53:04 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-17e0e627-ab94-47fb-bc74-1dec8ad328ad/part-00000-b1eafe84-1168-41d7-a69f-078374fd75a0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=9334ed8d-0017-424f-93c7-32b485d4da4a, location=europe-west6}
[2023-04-24T08:53:10.832+0000] {subprocess.py:93} INFO - 23/04/24 08:53:06 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=9334ed8d-0017-424f-93c7-32b485d4da4a, location=europe-west6}
[2023-04-24T08:53:13.687+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.688+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.688+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:13.689+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:13.690+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:13.691+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:13.692+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:13.692+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:13.693+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:13.694+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:13.694+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:13.695+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:13.696+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:13.697+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:13.697+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:13.698+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:13.698+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.699+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.700+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.700+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:13.701+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:13.701+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:13.702+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.703+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:13.704+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.705+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:13.705+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:13.707+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:13.708+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:13.708+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:13.709+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:13.710+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:13.711+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:13.712+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:13.712+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:13.713+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:13.714+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:13.714+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:13.715+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:13.716+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:13.717+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:13.718+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:13.719+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:13.720+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:13.720+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:13.721+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:13.722+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:13.722+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:13.723+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:13.724+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:13.725+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:13.725+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:13.726+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:13.726+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:13.727+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:13.728+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:13.728+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:13.729+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:13.729+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:13.730+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:13.730+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:13.732+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:13.732+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:13.733+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:13.734+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/attempt_202304240853084954690451690907066_0016_m_000000_18/' directory.
[2023-04-24T08:53:13.735+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/attempt_202304240853084266061725210594649_0016_m_000002_20/' directory.
[2023-04-24T08:53:13.735+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/attempt_202304240853082321875076946705417_0016_m_000003_21/' directory.
[2023-04-24T08:53:13.736+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/attempt_202304240853088832880697798195144_0016_m_000001_19/' directory.
[2023-04-24T08:53:13.736+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:13.737+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:53:13.738+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:13.739+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:53:13.740+0000] {subprocess.py:93} INFO - 23/04/24 08:53:08 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:13.741+0000] {subprocess.py:93} INFO - 23/04/24 08:53:09 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:53:13.742+0000] {subprocess.py:93} INFO - 23/04/24 08:53:09 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:13.742+0000] {subprocess.py:93} INFO - 23/04/24 08:53:09 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/' directory.
[2023-04-24T08:53:13.743+0000] {subprocess.py:93} INFO - 23/04/24 08:53:09 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/part-00003-8b19a764-2739-4865-9ea4-d4ec2806f6cc-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/part-00000-8b19a764-2739-4865-9ea4-d4ec2806f6cc-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/part-00002-8b19a764-2739-4865-9ea4-d4ec2806f6cc-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-bc3a6dbe-1455-4060-8d2e-183d69ce62a3/part-00001-8b19a764-2739-4865-9ea4-d4ec2806f6cc-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=504b8c0a-5687-4d6a-8a7a-f7e35c6f0dff, location=europe-west6}
[2023-04-24T08:53:18.373+0000] {subprocess.py:93} INFO - 23/04/24 08:53:11 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=504b8c0a-5687-4d6a-8a7a-f7e35c6f0dff, location=europe-west6}
[2023-04-24T08:53:18.376+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:18.377+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:18.378+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:18.379+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:18.380+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:18.381+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:18.382+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:18.383+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:18.384+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:18.385+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:18.386+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:18.387+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:18.388+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:18.388+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:18.390+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:18.391+0000] {subprocess.py:93} INFO - 23/04/24 08:53:14 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:18.392+0000] {subprocess.py:93} INFO - 23/04/24 08:53:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-39dc9cc1-115e-4f57-9999-860c4a540a38/_temporary/0/_temporary/attempt_202304240853127846517039005610209_0018_m_000000_23/' directory.
[2023-04-24T08:53:18.394+0000] {subprocess.py:93} INFO - 23/04/24 08:53:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-39dc9cc1-115e-4f57-9999-860c4a540a38/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:18.395+0000] {subprocess.py:93} INFO - 23/04/24 08:53:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-39dc9cc1-115e-4f57-9999-860c4a540a38/' directory.
[2023-04-24T08:53:18.396+0000] {subprocess.py:93} INFO - 23/04/24 08:53:16 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-39dc9cc1-115e-4f57-9999-860c4a540a38/part-00000-ae0f89cd-7688-4e2e-b337-a8087caba625-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=3595e5dd-a705-4088-aeaa-371e291f17e1, location=europe-west6}
[2023-04-24T08:53:22.958+0000] {subprocess.py:93} INFO - 23/04/24 08:53:20 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=3595e5dd-a705-4088-aeaa-371e291f17e1, location=europe-west6}
[2023-04-24T08:53:29.448+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.449+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.450+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:29.451+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:29.452+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:29.453+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:29.454+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:29.455+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:29.456+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:29.457+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:29.458+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:29.458+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:29.459+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:29.460+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:29.461+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:29.462+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:29.462+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.463+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.463+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:29.464+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:29.465+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:29.466+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:29.467+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:29.468+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:29.468+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:29.469+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:29.469+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:29.470+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:29.471+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:29.471+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:29.472+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:29.472+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:29.473+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.474+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.475+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:29.475+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:29.476+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:29.476+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:29.477+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:29.478+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:29.478+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:29.479+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:29.479+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:29.480+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:29.481+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:29.482+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:29.483+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:29.483+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:29.484+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.485+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.485+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:29.486+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:29.486+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:29.487+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:29.488+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:29.489+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:29.490+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:29.490+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:29.491+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:29.492+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:29.492+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:29.493+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:29.494+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:29.495+0000] {subprocess.py:93} INFO - 23/04/24 08:53:23 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:29.496+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/attempt_20230424085323874279802043684191_0022_m_000003_30/' directory.
[2023-04-24T08:53:29.497+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/attempt_202304240853232580766966606339271_0022_m_000000_27/' directory.
[2023-04-24T08:53:29.497+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/attempt_202304240853239218082662917025481_0022_m_000001_28/' directory.
[2023-04-24T08:53:29.498+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:29.499+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/attempt_202304240853237560110247407595981_0022_m_000002_29/' directory.
[2023-04-24T08:53:29.499+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:53:29.500+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:29.501+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.502+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:29.503+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:29.504+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:29.505+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:29.506+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:29.506+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:29.507+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:29.508+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:29.509+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:29.510+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:29.510+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:29.511+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:29.512+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:29.513+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:29.513+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:29.514+0000] {subprocess.py:93} INFO - 23/04/24 08:53:24 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/_temporary/0/_temporary/attempt_202304240853235434518052126730370_0022_m_000004_31/' directory.
[2023-04-24T08:53:29.515+0000] {subprocess.py:93} INFO - 23/04/24 08:53:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/' directory.
[2023-04-24T08:53:29.516+0000] {subprocess.py:93} INFO - 23/04/24 08:53:25 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/part-00001-873d0d21-657c-4b7a-8171-9b6464f925d0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/part-00003-873d0d21-657c-4b7a-8171-9b6464f925d0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/part-00000-873d0d21-657c-4b7a-8171-9b6464f925d0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/part-00002-873d0d21-657c-4b7a-8171-9b6464f925d0-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-34f5a588-1790-4f79-a6fc-e560ce463eff/part-00004-873d0d21-657c-4b7a-8171-9b6464f925d0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=94910f79-a805-4ab7-bbb4-98db31b0eb23, location=europe-west6}
[2023-04-24T08:53:32.648+0000] {subprocess.py:93} INFO - 23/04/24 08:53:28 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=94910f79-a805-4ab7-bbb4-98db31b0eb23, location=europe-west6}
[2023-04-24T08:53:32.649+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-24T08:53:35.501+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.503+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.504+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:35.506+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:35.507+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:35.508+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:35.509+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:35.511+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:35.512+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:35.514+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:35.515+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:35.516+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:35.517+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:35.519+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:35.520+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:35.521+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:35.522+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.523+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.525+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.526+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:35.527+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:35.528+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:35.530+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:35.531+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:35.533+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:35.534+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:35.535+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:35.536+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:35.537+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:35.539+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:35.541+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:35.542+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:35.543+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:35.545+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.546+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.548+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:35.549+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:35.551+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:35.552+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:35.554+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.556+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:35.557+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:35.559+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:35.561+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:35.564+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:35.566+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:35.567+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:35.569+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:35.570+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:35.572+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:35.573+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:35.575+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:35.577+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:35.578+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:35.580+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:35.581+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:35.583+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:35.585+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:35.587+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:35.589+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:35.591+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:35.592+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:35.594+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:35.595+0000] {subprocess.py:93} INFO - 23/04/24 08:53:30 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:35.597+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/attempt_202304240853304359659948914077166_0035_m_000001_56/' directory.
[2023-04-24T08:53:35.598+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/attempt_20230424085330397036870879677378_0035_m_000003_58/' directory.
[2023-04-24T08:53:35.599+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:35.601+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/attempt_202304240853304887325361471635512_0035_m_000000_55/' directory.
[2023-04-24T08:53:35.603+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.605+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:35.606+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:35.608+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:35.610+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:35.612+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:35.614+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:35.615+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:35.617+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:35.619+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:35.621+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:35.623+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:35.624+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:35.626+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:35.628+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:35.630+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:35.632+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-24T08:53:35.633+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:35.635+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/attempt_202304240853301123694005371159929_0035_m_000002_57/' directory.
[2023-04-24T08:53:35.637+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/_temporary/0/_temporary/attempt_202304240853308636209587354994189_0035_m_000004_59/' directory.
[2023-04-24T08:53:35.639+0000] {subprocess.py:93} INFO - 23/04/24 08:53:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/' directory.
[2023-04-24T08:53:35.641+0000] {subprocess.py:93} INFO - 23/04/24 08:53:32 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/part-00002-65afa0ef-b4cc-473b-8651-3417b3bdcb28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/part-00003-65afa0ef-b4cc-473b-8651-3417b3bdcb28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/part-00000-65afa0ef-b4cc-473b-8651-3417b3bdcb28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/part-00001-65afa0ef-b4cc-473b-8651-3417b3bdcb28-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-eaa3c696-5240-41d3-b101-d3ffcf4154e3/part-00004-65afa0ef-b4cc-473b-8651-3417b3bdcb28-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=6f9ab610-c859-4924-9714-1072ef9ff247, location=europe-west6}
[2023-04-24T08:53:41.639+0000] {subprocess.py:93} INFO - 23/04/24 08:53:36 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=6f9ab610-c859-4924-9714-1072ef9ff247, location=europe-west6}
[2023-04-24T08:53:41.641+0000] {subprocess.py:93} INFO - 23/04/24 08:53:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.642+0000] {subprocess.py:93} INFO - 23/04/24 08:53:37 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.643+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.644+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.645+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.646+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.647+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.648+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.649+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.650+0000] {subprocess.py:93} INFO - 23/04/24 08:53:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.651+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.652+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.653+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.654+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.654+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:41.655+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:44.431+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:44.432+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:44.433+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:44.433+0000] {subprocess.py:93} INFO - 23/04/24 08:53:39 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-24T08:53:44.434+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:44.435+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-24T08:53:44.436+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-24T08:53:44.437+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-24T08:53:44.438+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-24T08:53:44.438+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-24T08:53:44.439+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-24T08:53:44.440+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-24T08:53:44.441+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-24T08:53:44.441+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-24T08:53:44.442+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-24T08:53:44.443+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-24T08:53:44.444+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-24T08:53:44.444+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-24T08:53:44.445+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-24T08:53:44.446+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-24T08:53:44.447+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-6db3fae0-efb9-4004-833d-5ede4c92c0c6/_temporary/0/_temporary/attempt_202304240853408578130294092252417_0080_m_000000_119/' directory.
[2023-04-24T08:53:44.448+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-6db3fae0-efb9-4004-833d-5ede4c92c0c6/_temporary/0/_temporary/' directory.
[2023-04-24T08:53:44.448+0000] {subprocess.py:93} INFO - 23/04/24 08:53:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-6db3fae0-efb9-4004-833d-5ede4c92c0c6/' directory.
[2023-04-24T08:53:44.449+0000] {subprocess.py:93} INFO - 23/04/24 08:53:41 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682326357633-6db3fae0-efb9-4004-833d-5ede4c92c0c6/part-00000-b1e7c3fb-eaa3-4f79-8065-a43d4d160ede-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=20a07d1f-a855-4749-b344-0ada372a6e84, location=europe-west6}
[2023-04-24T08:53:52.713+0000] {subprocess.py:93} INFO - 23/04/24 08:53:47 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=20a07d1f-a855-4749-b344-0ada372a6e84, location=europe-west6}
[2023-04-24T08:53:52.714+0000] {subprocess.py:93} INFO - 23/04/24 08:53:47 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@23a7bbee{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-24T08:53:53.124+0000] {subprocess.py:93} INFO - Job [99d80573d9ef4967b6fecb1c8ccf61cc] finished successfully.
[2023-04-24T08:53:53.274+0000] {subprocess.py:93} INFO - done: true
[2023-04-24T08:53:53.275+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/99d80573d9ef4967b6fecb1c8ccf61cc/
[2023-04-24T08:53:53.276+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/99d80573d9ef4967b6fecb1c8ccf61cc/driveroutput
[2023-04-24T08:53:53.278+0000] {subprocess.py:93} INFO - jobUuid: 43c1b004-6598-3cf9-840d-2686380858c2
[2023-04-24T08:53:53.278+0000] {subprocess.py:93} INFO - placement:
[2023-04-24T08:53:53.280+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-24T08:53:53.281+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-24T08:53:53.283+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-24T08:53:53.285+0000] {subprocess.py:93} INFO -   args:
[2023-04-24T08:53:53.287+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-24T08:53:53.288+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-24T08:53:53.289+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-24T08:53:53.290+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-24T08:53:53.291+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-24T08:53:53.292+0000] {subprocess.py:93} INFO - reference:
[2023-04-24T08:53:53.294+0000] {subprocess.py:93} INFO -   jobId: 99d80573d9ef4967b6fecb1c8ccf61cc
[2023-04-24T08:53:53.295+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-24T08:53:53.296+0000] {subprocess.py:93} INFO - status:
[2023-04-24T08:53:53.297+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-24T08:53:53.298+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T08:53:48.054649Z'
[2023-04-24T08:53:53.298+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-24T08:53:53.299+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-24T08:53:53.300+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T08:52:28.894618Z'
[2023-04-24T08:53:53.301+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-24T08:53:53.302+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T08:52:28.933130Z'
[2023-04-24T08:53:53.303+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-24T08:53:53.304+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-24T08:53:53.305+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-24T08:52:29.157995Z'
[2023-04-24T08:53:54.268+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-24T08:53:54.345+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-24T08:53:54.691+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230424T085224, end_date=20230424T085354
[2023-04-24T08:53:54.922+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-24T08:53:55.069+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
