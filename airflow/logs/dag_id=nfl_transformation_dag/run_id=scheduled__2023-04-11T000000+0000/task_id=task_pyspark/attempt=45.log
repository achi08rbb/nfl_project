[2023-04-23T16:16:01.048+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-23T16:16:01.064+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-23T16:16:01.065+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T16:16:01.066+0000] {taskinstance.py:1289} INFO - Starting attempt 45 of 47
[2023-04-23T16:16:01.066+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T16:16:01.083+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-23T16:16:01.085+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance Not Running' FAILED: Task is in the running state
[2023-04-23T16:16:01.086+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state.
[2023-04-23T16:16:01.098+0000] {standard_task_runner.py:55} INFO - Started process 13499 to run task
[2023-04-23T16:16:01.101+0000] {local_task_job.py:151} INFO - Task is not able to be run
[2023-04-23T16:16:01.102+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '1225', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpmjae_pdh']
[2023-04-23T16:16:01.105+0000] {standard_task_runner.py:83} INFO - Job 1225: Subtask task_pyspark
[2023-04-23T16:16:01.180+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host 235fa867d994
[2023-04-23T16:16:01.255+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=45
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-23T16:16:01.258+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-23T16:16:01.270+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-23T16:16:01.290+0000] {subprocess.py:86} INFO - Output:
[2023-04-23T16:16:06.611+0000] {subprocess.py:93} INFO - Job [f219df6fb3564fd1a89b51fea99b46ad] submitted.
[2023-04-23T16:16:06.611+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-23T16:16:18.165+0000] {subprocess.py:93} INFO - 23/04/23 16:16:15 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-23T16:16:18.166+0000] {subprocess.py:93} INFO - 23/04/23 16:16:15 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-23T16:16:18.167+0000] {subprocess.py:93} INFO - 23/04/23 16:16:15 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-23T16:16:18.167+0000] {subprocess.py:93} INFO - 23/04/23 16:16:15 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-23T16:16:18.168+0000] {subprocess.py:93} INFO - 23/04/23 16:16:15 INFO org.sparkproject.jetty.util.log: Logging initialized @4439ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-23T16:16:18.169+0000] {subprocess.py:93} INFO - 23/04/23 16:16:15 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-23T16:16:18.170+0000] {subprocess.py:93} INFO - 23/04/23 16:16:16 INFO org.sparkproject.jetty.server.Server: Started @4576ms
[2023-04-23T16:16:18.170+0000] {subprocess.py:93} INFO - 23/04/23 16:16:16 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@70e6a49e{HTTP/1.1, (http/1.1)}{0.0.0.0:40881}
[2023-04-23T16:16:22.933+0000] {subprocess.py:93} INFO - 23/04/23 16:16:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:16:30.097+0000] {subprocess.py:93} INFO - 23/04/23 16:16:24 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-23T16:16:30.098+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-23T16:16:30.099+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-23T16:16:34.381+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-23T16:16:34.382+0000] {subprocess.py:93} INFO - 23/04/23 16:16:31 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-23T16:16:34.383+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.384+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.385+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.386+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.387+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.388+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.389+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.389+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.390+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:34.391+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:34.392+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:34.394+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:34.395+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:34.396+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:34.397+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:34.397+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:34.398+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:34.399+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:34.400+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:34.401+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:34.401+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:34.402+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:34.403+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:34.404+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:34.405+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:34.407+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:34.408+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:34.410+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:34.411+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:34.412+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:34.413+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:34.414+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:34.414+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:34.416+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:34.417+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:34.419+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:34.420+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:34.421+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:34.422+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:34.423+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:34.424+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:34.424+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:34.425+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:34.426+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:34.426+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:34.427+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:34.428+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:34.429+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:34.429+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:34.431+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:34.432+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:34.434+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:34.435+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:34.436+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:34.437+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:34.438+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:34.439+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:34.440+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:34.441+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:34.442+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:34.443+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:34.444+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:34.445+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:34.445+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:34.446+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T16:16:34.447+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T16:16:34.448+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T16:16:34.449+0000] {subprocess.py:93} INFO - 23/04/23 16:16:32 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T16:16:34.451+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/attempt_202304231616324509578194355196544_0010_m_000002_11/' directory.
[2023-04-23T16:16:34.452+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/attempt_202304231616322278530247195643518_0010_m_000000_9/' directory.
[2023-04-23T16:16:34.453+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/attempt_202304231616323830616003271876647_0010_m_000003_12/' directory.
[2023-04-23T16:16:34.454+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/attempt_202304231616328323193510553787_0010_m_000001_10/' directory.
[2023-04-23T16:16:34.455+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:34.456+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:16:34.457+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:34.458+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:16:34.459+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:16:34.460+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:34.461+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:34.462+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.464+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:34.467+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:34.468+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:34.469+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:34.470+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:34.471+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:34.473+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:34.474+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:34.477+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:34.480+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:34.481+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:34.483+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:34.485+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:34.486+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:34.488+0000] {subprocess.py:93} INFO - 23/04/23 16:16:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:39.918+0000] {subprocess.py:93} INFO - 23/04/23 16:16:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/_temporary/0/_temporary/attempt_202304231616324973467041284444095_0010_m_000004_13/' directory.
[2023-04-23T16:16:39.919+0000] {subprocess.py:93} INFO - 23/04/23 16:16:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/' directory.
[2023-04-23T16:16:39.920+0000] {subprocess.py:93} INFO - 23/04/23 16:16:35 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=teams_2022_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/part-00000-f8d1ac83-3892-4377-9882-f0029e8fe640-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/part-00003-f8d1ac83-3892-4377-9882-f0029e8fe640-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/part-00004-f8d1ac83-3892-4377-9882-f0029e8fe640-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/part-00001-f8d1ac83-3892-4377-9882-f0029e8fe640-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-4fd8ec41-df85-47e1-8ec6-3c481b6b0dd5/part-00002-f8d1ac83-3892-4377-9882-f0029e8fe640-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=66cb3dc4-7198-4ebd-b674-d2990e0c86c4, location=europe-west6}
[2023-04-23T16:16:39.921+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.teams_2022_2. jobId: JobId{project=nfl-project-de, job=66cb3dc4-7198-4ebd-b674-d2990e0c86c4, location=europe-west6}
[2023-04-23T16:16:39.922+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:39.923+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:39.924+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:39.925+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:39.927+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:39.928+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:39.929+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:39.931+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:39.932+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:39.934+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:39.935+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:39.935+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:39.936+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:39.937+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:39.938+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:39.939+0000] {subprocess.py:93} INFO - 23/04/23 16:16:37 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:39.940+0000] {subprocess.py:93} INFO - 23/04/23 16:16:38 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-0bfec2d5-652d-48cf-9d0b-2f5da584a793/_temporary/0/_temporary/attempt_202304231616378106476085197085621_0011_m_000000_14/' directory.
[2023-04-23T16:16:39.941+0000] {subprocess.py:93} INFO - 23/04/23 16:16:38 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-0bfec2d5-652d-48cf-9d0b-2f5da584a793/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:39.942+0000] {subprocess.py:93} INFO - 23/04/23 16:16:38 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-0bfec2d5-652d-48cf-9d0b-2f5da584a793/' directory.
[2023-04-23T16:16:39.943+0000] {subprocess.py:93} INFO - 23/04/23 16:16:38 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=leaders_2022_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-0bfec2d5-652d-48cf-9d0b-2f5da584a793/part-00000-9e88065d-d52d-4971-8db4-bfdcffeb1d57-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=0e2a0dd1-2c0c-448d-adb8-1c5f3f8b92fd, location=europe-west6}
[2023-04-23T16:16:42.741+0000] {subprocess.py:93} INFO - 23/04/23 16:16:41 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.leaders_2022_2. jobId: JobId{project=nfl-project-de, job=0e2a0dd1-2c0c-448d-adb8-1c5f3f8b92fd, location=europe-west6}
[2023-04-23T16:16:45.679+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:45.680+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:45.681+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:45.682+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:45.684+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:45.685+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:45.686+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:45.686+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:45.687+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:45.688+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:45.689+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:45.689+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:45.690+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:45.691+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:45.692+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:45.692+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:45.693+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-291817ff-bf69-4122-bb9f-4d95b73874d4/_temporary/0/_temporary/attempt_202304231616423147640154983533781_0012_m_000000_15/' directory.
[2023-04-23T16:16:45.693+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-291817ff-bf69-4122-bb9f-4d95b73874d4/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:45.694+0000] {subprocess.py:93} INFO - 23/04/23 16:16:42 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-291817ff-bf69-4122-bb9f-4d95b73874d4/' directory.
[2023-04-23T16:16:45.696+0000] {subprocess.py:93} INFO - 23/04/23 16:16:43 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=teams_defense_stats_2022_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-291817ff-bf69-4122-bb9f-4d95b73874d4/part-00000-0a233049-db6b-4718-b9e0-d9e1110318f8-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=ff7615ff-28da-4a15-8298-cb182fd738f2, location=europe-west6}
[2023-04-23T16:16:45.696+0000] {subprocess.py:93} INFO - 23/04/23 16:16:44 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.teams_defense_stats_2022_2. jobId: JobId{project=nfl-project-de, job=ff7615ff-28da-4a15-8298-cb182fd738f2, location=europe-west6}
[2023-04-23T16:16:50.227+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.228+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.228+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:50.229+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:50.230+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:50.231+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:50.231+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:50.232+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:50.233+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:50.234+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:50.234+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:50.235+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:50.236+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:50.237+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:50.238+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:50.238+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:50.239+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.239+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.240+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:50.241+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:50.241+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:50.242+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:50.243+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:50.244+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:50.244+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:50.245+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:50.246+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:50.246+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:50.247+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:50.248+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:50.249+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:50.250+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:50.251+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.251+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.252+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:50.253+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:50.254+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:50.255+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:50.256+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:50.256+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:50.257+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:50.258+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:50.258+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:50.259+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:50.259+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:50.260+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:50.261+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:50.261+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:50.262+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.263+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.265+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:50.266+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:50.267+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:50.267+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:50.268+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:50.269+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:50.270+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:50.271+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:50.272+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:50.272+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:50.273+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:50.274+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:50.274+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:50.275+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:50.275+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/attempt_202304231616456401621066456318830_0016_m_000001_19/' directory.
[2023-04-23T16:16:50.276+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/attempt_202304231616457479608858370488321_0016_m_000003_21/' directory.
[2023-04-23T16:16:50.277+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/attempt_202304231616453178741159377403322_0016_m_000000_18/' directory.
[2023-04-23T16:16:50.277+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/attempt_202304231616451943359253458542608_0016_m_000002_20/' directory.
[2023-04-23T16:16:50.278+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:50.278+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:16:50.280+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:50.281+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.282+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:50.282+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:50.283+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:50.283+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:50.284+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:50.284+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:50.285+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:50.286+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:50.287+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:50.287+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:50.288+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:50.288+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:50.289+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:50.290+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:50.290+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:50.291+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:16:50.291+0000] {subprocess.py:93} INFO - 23/04/23 16:16:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:50.293+0000] {subprocess.py:93} INFO - 23/04/23 16:16:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/_temporary/0/_temporary/attempt_202304231616452794267253221948658_0016_m_000004_22/' directory.
[2023-04-23T16:16:50.294+0000] {subprocess.py:93} INFO - 23/04/23 16:16:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/' directory.
[2023-04-23T16:16:50.294+0000] {subprocess.py:93} INFO - 23/04/23 16:16:47 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=teams_stats_2022_2_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatperGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/part-00004-289e90f0-07cb-4ea2-ba8f-a11949ec2a21-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/part-00000-289e90f0-07cb-4ea2-ba8f-a11949ec2a21-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/part-00002-289e90f0-07cb-4ea2-ba8f-a11949ec2a21-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/part-00001-289e90f0-07cb-4ea2-ba8f-a11949ec2a21-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2c99b50e-9222-4519-b8e3-7ed0df2b0cb7/part-00003-289e90f0-07cb-4ea2-ba8f-a11949ec2a21-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=073b768d-45c0-4aae-9fd5-a2841d6ce16c, location=europe-west6}
[2023-04-23T16:16:50.295+0000] {subprocess.py:93} INFO - 23/04/23 16:16:50 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.teams_stats_2022_2_PARTITIONED. jobId: JobId{project=nfl-project-de, job=073b768d-45c0-4aae-9fd5-a2841d6ce16c, location=europe-west6}
[2023-04-23T16:16:54.865+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:54.866+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:16:54.867+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:16:54.867+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:16:54.868+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:16:54.869+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:16:54.869+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:16:54.870+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:16:54.870+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:16:54.871+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:16:54.871+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:16:54.872+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:16:54.872+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:16:54.873+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:16:54.873+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:16:54.874+0000] {subprocess.py:93} INFO - 23/04/23 16:16:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:16:54.875+0000] {subprocess.py:93} INFO - 23/04/23 16:16:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2b88f82b-8903-4f36-8f98-4492fb647e9a/_temporary/0/_temporary/attempt_202304231616517228426554544143925_0018_m_000000_24/' directory.
[2023-04-23T16:16:54.876+0000] {subprocess.py:93} INFO - 23/04/23 16:16:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2b88f82b-8903-4f36-8f98-4492fb647e9a/_temporary/0/_temporary/' directory.
[2023-04-23T16:16:54.877+0000] {subprocess.py:93} INFO - 23/04/23 16:16:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2b88f82b-8903-4f36-8f98-4492fb647e9a/' directory.
[2023-04-23T16:16:58.515+0000] {subprocess.py:93} INFO - 23/04/23 16:16:54 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=athletes_2022_2_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-2b88f82b-8903-4f36-8f98-4492fb647e9a/part-00000-adfaf72c-964a-47f7-bd62-16339951191e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=987dba03-641a-4ebc-960d-080f433d3e0d, location=europe-west6}
[2023-04-23T16:16:58.516+0000] {subprocess.py:93} INFO - 23/04/23 16:16:56 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.athletes_2022_2_PARTITIONED. jobId: JobId{project=nfl-project-de, job=987dba03-641a-4ebc-960d-080f433d3e0d, location=europe-west6}
[2023-04-23T16:17:04.024+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.025+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.026+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:17:04.027+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:17:04.028+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:17:04.028+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:17:04.029+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:17:04.030+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:17:04.030+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:17:04.031+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:17:04.032+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:17:04.033+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:17:04.033+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:17:04.034+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:17:04.035+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:17:04.036+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:17:04.037+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.038+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.039+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:17:04.040+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:17:04.041+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:17:04.042+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:17:04.043+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:17:04.044+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:17:04.045+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:17:04.046+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:17:04.047+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:17:04.048+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:17:04.049+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:17:04.049+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:17:04.050+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:17:04.051+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:17:04.052+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.053+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.053+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:17:04.054+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:17:04.055+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:17:04.056+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:17:04.057+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:17:04.058+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:17:04.059+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:17:04.060+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:17:04.061+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:17:04.062+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:17:04.062+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:17:04.064+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:17:04.064+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:17:04.065+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:17:04.067+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.068+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.070+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:17:04.071+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:17:04.072+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:17:04.073+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:17:04.074+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:17:04.075+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:17:04.076+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:17:04.077+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:17:04.078+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:17:04.079+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:17:04.080+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:17:04.081+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:17:04.081+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:17:04.083+0000] {subprocess.py:93} INFO - 23/04/23 16:17:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:17:04.084+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/attempt_20230423161700382897460357292745_0022_m_000001_29/' directory.
[2023-04-23T16:17:04.085+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/attempt_202304231617002730060017166942203_0022_m_000002_30/' directory.
[2023-04-23T16:17:04.086+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/attempt_202304231617008111732594802195571_0022_m_000003_31/' directory.
[2023-04-23T16:17:04.088+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/attempt_202304231617005613683479425298607_0022_m_000000_28/' directory.
[2023-04-23T16:17:04.089+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/' directory.
[2023-04-23T16:17:04.090+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:17:04.091+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/' directory.
[2023-04-23T16:17:04.093+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:17:04.094+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/' directory.
[2023-04-23T16:17:04.095+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.096+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T16:17:04.098+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T16:17:04.099+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T16:17:04.100+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T16:17:04.101+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T16:17:04.102+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T16:17:04.103+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T16:17:04.104+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T16:17:04.105+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T16:17:04.106+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T16:17:04.107+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T16:17:04.108+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T16:17:04.110+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T16:17:04.110+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T16:17:04.112+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T16:17:04.113+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T16:17:04.114+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/' directory.
[2023-04-23T16:17:04.115+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/_temporary/0/_temporary/attempt_202304231617002380970304436128874_0022_m_000004_32/' directory.
[2023-04-23T16:17:04.116+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/' directory.
[2023-04-23T16:17:04.117+0000] {subprocess.py:93} INFO - 23/04/23 16:17:02 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=athletes_stats_2022_2_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/part-00001-e24033c8-4c8a-46ab-9309-24e0d1aea40b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/part-00002-e24033c8-4c8a-46ab-9309-24e0d1aea40b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/part-00000-e24033c8-4c8a-46ab-9309-24e0d1aea40b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/part-00003-e24033c8-4c8a-46ab-9309-24e0d1aea40b-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682266576267-70c473be-a892-4595-8e46-db13ca48b2ea/part-00004-e24033c8-4c8a-46ab-9309-24e0d1aea40b-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=3e23a0c8-d03b-44a4-8c1d-59601f375a9d, location=europe-west6}
[2023-04-23T16:17:10.174+0000] {subprocess.py:93} INFO - 23/04/23 16:17:05 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.athletes_stats_2022_2_PARTITIONED. jobId: JobId{project=nfl-project-de, job=3e23a0c8-d03b-44a4-8c1d-59601f375a9d, location=europe-west6}
[2023-04-23T16:17:10.175+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-23T16:17:10.176+0000] {subprocess.py:93} INFO - 23/04/23 16:17:05 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@70e6a49e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-23T16:17:13.425+0000] {subprocess.py:93} INFO - Job [f219df6fb3564fd1a89b51fea99b46ad] finished successfully.
[2023-04-23T16:17:13.447+0000] {subprocess.py:93} INFO - done: true
[2023-04-23T16:17:13.448+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/f219df6fb3564fd1a89b51fea99b46ad/
[2023-04-23T16:17:13.449+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/f219df6fb3564fd1a89b51fea99b46ad/driveroutput
[2023-04-23T16:17:13.449+0000] {subprocess.py:93} INFO - jobUuid: 71f3ac49-9e80-31cc-a842-8508c7c4f673
[2023-04-23T16:17:13.450+0000] {subprocess.py:93} INFO - placement:
[2023-04-23T16:17:13.450+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-23T16:17:13.451+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-23T16:17:13.451+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-23T16:17:13.452+0000] {subprocess.py:93} INFO -   args:
[2023-04-23T16:17:13.452+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-23T16:17:13.453+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-23T16:17:13.454+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-23T16:17:13.454+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-23T16:17:13.455+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-23T16:17:13.455+0000] {subprocess.py:93} INFO - reference:
[2023-04-23T16:17:13.456+0000] {subprocess.py:93} INFO -   jobId: f219df6fb3564fd1a89b51fea99b46ad
[2023-04-23T16:17:13.457+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-23T16:17:13.457+0000] {subprocess.py:93} INFO - status:
[2023-04-23T16:17:13.458+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-23T16:17:13.459+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T16:17:06.130Z'
[2023-04-23T16:17:13.460+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-23T16:17:13.460+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-23T16:17:13.462+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T16:16:08.176874Z'
[2023-04-23T16:17:13.462+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-23T16:17:13.463+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T16:16:08.215137Z'
[2023-04-23T16:17:13.463+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-23T16:17:13.464+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-23T16:17:13.464+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T16:16:08.432502Z'
[2023-04-23T16:17:13.654+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-23T16:17:13.656+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-23T16:17:13.673+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230423T161601, end_date=20230423T161713
[2023-04-23T16:17:13.735+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-23T16:17:13.753+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
