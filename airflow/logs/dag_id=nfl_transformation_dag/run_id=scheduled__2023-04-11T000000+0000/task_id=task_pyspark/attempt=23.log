[2023-04-12T14:00:14.467+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T14:00:14.481+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T14:00:14.482+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T14:00:14.483+0000] {taskinstance.py:1289} INFO - Starting attempt 23 of 23
[2023-04-12T14:00:14.484+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T14:00:14.504+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-12T14:00:14.513+0000] {standard_task_runner.py:55} INFO - Started process 10916 to run task
[2023-04-12T14:00:14.522+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '958', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpvv44yicu']
[2023-04-12T14:00:14.529+0000] {standard_task_runner.py:83} INFO - Job 958: Subtask task_pyspark
[2023-04-12T14:00:14.626+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host a2e619923d04
[2023-04-12T14:00:14.716+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=23
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-12T14:00:14.722+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-12T14:00:14.742+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-de-project/code/transform_pyspark.py                 --                     --year=2020                     --season_type=2 ']
[2023-04-12T14:00:14.763+0000] {subprocess.py:86} INFO - Output:
[2023-04-12T14:00:20.952+0000] {subprocess.py:93} INFO - Job [deced7ab8fd44c70859e6f80ac439a5f] submitted.
[2023-04-12T14:00:20.953+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-12T14:00:31.010+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-12T14:00:31.011+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-12T14:00:31.012+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-12T14:00:31.014+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-12T14:00:31.015+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.sparkproject.jetty.util.log: Logging initialized @4416ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-12T14:00:31.016+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-12T14:00:31.017+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.sparkproject.jetty.server.Server: Started @4547ms
[2023-04-12T14:00:31.019+0000] {subprocess.py:93} INFO - 23/04/12 14:00:30 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@6140d3c5{HTTP/1.1, (http/1.1)}{0.0.0.0:34185}
[2023-04-12T14:00:33.833+0000] {subprocess.py:93} INFO - 23/04/12 14:00:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:38.650+0000] {subprocess.py:93} INFO - 23/04/12 14:00:38 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-12T14:00:38.650+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-12T14:00:38.651+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-12T14:00:43.314+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-12T14:00:46.179+0000] {subprocess.py:93} INFO - 23/04/12 14:00:45 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-12T14:00:50.687+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.688+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.689+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.690+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.690+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.691+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.692+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.693+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.693+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:50.694+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:50.695+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:50.695+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:50.696+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:50.696+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:50.697+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:50.698+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:50.698+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:50.699+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:50.700+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:50.700+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:50.701+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:50.701+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:50.702+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:50.702+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:50.704+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:50.704+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:50.705+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:50.706+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:50.707+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:50.708+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:50.709+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:50.709+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:50.710+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:50.711+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:50.711+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:50.712+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:50.712+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:50.713+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:50.713+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:50.714+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:50.715+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:50.715+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:50.716+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:50.716+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:50.717+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:50.717+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:50.718+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:50.719+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:50.720+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:50.720+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:50.721+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:50.722+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:50.723+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:50.723+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:50.724+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:50.725+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:50.725+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:50.726+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:50.727+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:50.728+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:50.729+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:50.729+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:50.730+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:50.730+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:50.731+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:00:50.731+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:00:50.732+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:00:50.732+0000] {subprocess.py:93} INFO - 23/04/12 14:00:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:00:50.732+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/attempt_202304121400468760155980432002156_0010_m_000001_10/' directory.
[2023-04-12T14:00:50.733+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/attempt_202304121400464055086196555338842_0010_m_000000_9/' directory.
[2023-04-12T14:00:50.733+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/attempt_202304121400467457190017351352722_0010_m_000002_11/' directory.
[2023-04-12T14:00:50.734+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/attempt_202304121400461889841083019480663_0010_m_000003_12/' directory.
[2023-04-12T14:00:50.735+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:50.736+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:50.736+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:50.737+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:50.737+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:50.738+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:50.739+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:50.740+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.740+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:50.741+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:50.741+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:50.742+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:50.743+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:50.744+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:50.744+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:50.745+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:50.745+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:50.746+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:50.746+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:50.746+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:50.747+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:50.747+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:50.748+0000] {subprocess.py:93} INFO - 23/04/12 14:00:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:50.748+0000] {subprocess.py:93} INFO - 23/04/12 14:00:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/_temporary/0/_temporary/attempt_202304121400464271025612528649002_0010_m_000004_13/' directory.
[2023-04-12T14:00:50.749+0000] {subprocess.py:93} INFO - 23/04/12 14:00:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/' directory.
[2023-04-12T14:00:50.749+0000] {subprocess.py:93} INFO - 23/04/12 14:00:48 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=location, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isAllStar, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/part-00000-274690ad-15cf-4554-a2ab-158c6f6c0470-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/part-00003-274690ad-15cf-4554-a2ab-158c6f6c0470-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/part-00004-274690ad-15cf-4554-a2ab-158c6f6c0470-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/part-00001-274690ad-15cf-4554-a2ab-158c6f6c0470-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-a9e0bd43-435f-4144-a5f2-551120b178a1/part-00002-274690ad-15cf-4554-a2ab-158c6f6c0470-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=f860d938-a02d-4f25-af48-dd3e3379312e, location=europe-west6}
[2023-04-12T14:00:53.497+0000] {subprocess.py:93} INFO - 23/04/12 14:00:51 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_2020_2. jobId: JobId{project=nfl-de-project, job=f860d938-a02d-4f25-af48-dd3e3379312e, location=europe-west6}
[2023-04-12T14:00:53.498+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.499+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.499+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:53.500+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:53.500+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:53.501+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:53.502+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:53.502+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:53.503+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:53.503+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:53.504+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:53.504+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:53.504+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:53.505+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:53.505+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:53.506+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:53.506+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.507+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.507+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:53.508+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:53.508+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:53.509+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:53.510+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:53.510+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:53.511+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:53.511+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:53.512+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:53.512+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:53.513+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:53.513+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:53.514+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:53.514+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:53.515+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.515+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.516+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:53.516+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:53.516+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:53.517+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:53.517+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:53.518+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:53.518+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:53.519+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:53.519+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:53.519+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:53.520+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:53.520+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:53.521+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:53.521+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:53.522+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.522+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.523+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:53.523+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:53.524+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:53.525+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:53.525+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:53.526+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:53.526+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:53.526+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:53.527+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:53.527+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:53.528+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:53.528+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:53.529+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:53.529+0000] {subprocess.py:93} INFO - 23/04/12 14:00:52 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:53.530+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/attempt_202304121400529082298839722135370_0014_m_000000_16/' directory.
[2023-04-12T14:00:53.531+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/attempt_20230412140052154067389139756509_0014_m_000001_17/' directory.
[2023-04-12T14:00:53.531+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/attempt_202304121400522827111996672791277_0014_m_000002_18/' directory.
[2023-04-12T14:00:53.532+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/attempt_202304121400526564459811445347151_0014_m_000003_19/' directory.
[2023-04-12T14:00:53.532+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:53.532+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.533+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:00:53.534+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:00:53.535+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:00:53.535+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:00:53.536+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:00:53.536+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:00:53.537+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:00:53.537+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:00:53.538+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:00:53.538+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:00:53.539+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:00:53.539+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:00:53.539+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:00:53.540+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:00:53.540+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:00:53.541+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:53.541+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:53.542+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:53.542+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:53.542+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:00:53.543+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/' directory.
[2023-04-12T14:00:56.038+0000] {subprocess.py:93} INFO - 23/04/12 14:00:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/_temporary/0/_temporary/attempt_202304121400528603424069259770557_0014_m_000004_20/' directory.
[2023-04-12T14:00:56.039+0000] {subprocess.py:93} INFO - 23/04/12 14:00:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/' directory.
[2023-04-12T14:00:56.041+0000] {subprocess.py:93} INFO - 23/04/12 14:00:54 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/part-00001-5c5acc4a-f545-44a2-b0f6-a88354977f47-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/part-00000-5c5acc4a-f545-44a2-b0f6-a88354977f47-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/part-00002-5c5acc4a-f545-44a2-b0f6-a88354977f47-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/part-00004-5c5acc4a-f545-44a2-b0f6-a88354977f47-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-e6f71d20-2552-4323-a386-f074b8e42ccb/part-00003-5c5acc4a-f545-44a2-b0f6-a88354977f47-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=83638466-e597-44e8-b630-77eb044f7ed1, location=europe-west6}
[2023-04-12T14:01:00.700+0000] {subprocess.py:93} INFO - 23/04/12 14:00:59 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_stats_2020_2. jobId: JobId{project=nfl-de-project, job=83638466-e597-44e8-b630-77eb044f7ed1, location=europe-west6}
[2023-04-12T14:01:03.524+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:03.525+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:03.526+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:03.527+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:03.527+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:03.528+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:03.529+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:03.530+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:03.530+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:03.531+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:03.532+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:03.533+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:03.534+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:03.535+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:03.535+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:03.536+0000] {subprocess.py:93} INFO - 23/04/12 14:01:02 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:03.536+0000] {subprocess.py:93} INFO - 23/04/12 14:01:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-4c7109c7-263f-460d-89f4-214f9faacd6a/_temporary/0/_temporary/attempt_202304121401006510312864876437205_0016_m_000000_22/' directory.
[2023-04-12T14:01:03.537+0000] {subprocess.py:93} INFO - 23/04/12 14:01:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-4c7109c7-263f-460d-89f4-214f9faacd6a/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:06.341+0000] {subprocess.py:93} INFO - 23/04/12 14:01:03 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-4c7109c7-263f-460d-89f4-214f9faacd6a/' directory.
[2023-04-12T14:01:06.342+0000] {subprocess.py:93} INFO - 23/04/12 14:01:04 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=athletesathletes_statsleaders_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weight_lbs, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=height_inches, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayHeight, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-4c7109c7-263f-460d-89f4-214f9faacd6a/part-00000-d56f18b5-d725-4bb2-a59a-7c1bfd203b7e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=3897b38a-9ed6-4fd3-9aaf-48ce652769cd, location=europe-west6}
[2023-04-12T14:01:11.031+0000] {subprocess.py:93} INFO - 23/04/12 14:01:07 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.athletesathletes_statsleaders_2020_2. jobId: JobId{project=nfl-de-project, job=3897b38a-9ed6-4fd3-9aaf-48ce652769cd, location=europe-west6}
[2023-04-12T14:01:13.868+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.869+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.870+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:13.870+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:13.871+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:13.871+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:13.872+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:13.872+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:13.873+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:13.873+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:13.874+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:13.874+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:13.874+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:13.875+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:13.875+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:13.876+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:13.876+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.877+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.877+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:13.877+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:13.878+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:13.878+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:13.879+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:13.880+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:13.880+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:13.881+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:13.881+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:13.882+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:13.883+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:13.883+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:13.884+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:13.884+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:13.884+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.885+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.885+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:13.886+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:13.886+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:13.887+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:13.887+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:13.888+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:13.889+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:13.889+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:13.889+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:13.890+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:13.890+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:13.891+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:13.891+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:13.891+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:13.892+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.892+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.893+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:13.893+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:13.893+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:13.894+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:13.894+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:13.894+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:13.895+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:13.896+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:13.896+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:13.897+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:13.897+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:13.898+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:13.899+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:13.899+0000] {subprocess.py:93} INFO - 23/04/12 14:01:11 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:13.900+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/attempt_202304121401118481635388951225934_0020_m_000001_27/' directory.
[2023-04-12T14:01:13.900+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/attempt_202304121401112532325473594411878_0020_m_000000_26/' directory.
[2023-04-12T14:01:13.901+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/attempt_202304121401112639394979309633773_0020_m_000002_28/' directory.
[2023-04-12T14:01:13.901+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/attempt_202304121401118686791150729913368_0020_m_000003_29/' directory.
[2023-04-12T14:01:13.902+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:13.902+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.903+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:13.903+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:13.904+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:13.904+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:13.905+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:13.905+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:13.906+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:13.906+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:13.906+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:13.907+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:13.908+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:13.908+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:13.909+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:13.910+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:13.910+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:13.911+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:01:13.911+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:13.912+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:01:13.912+0000] {subprocess.py:93} INFO - 23/04/12 14:01:12 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:13.913+0000] {subprocess.py:93} INFO - 23/04/12 14:01:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/_temporary/0/_temporary/attempt_202304121401117719081987287349014_0020_m_000004_30/' directory.
[2023-04-12T14:01:13.914+0000] {subprocess.py:93} INFO - 23/04/12 14:01:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/' directory.
[2023-04-12T14:01:13.914+0000] {subprocess.py:93} INFO - 23/04/12 14:01:13 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_defense_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/part-00001-5ff8f861-7c0c-4547-9179-a4bb7b38c21c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/part-00003-5ff8f861-7c0c-4547-9179-a4bb7b38c21c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/part-00004-5ff8f861-7c0c-4547-9179-a4bb7b38c21c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/part-00000-5ff8f861-7c0c-4547-9179-a4bb7b38c21c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-bb3d9f9c-a162-48b0-8b2a-46b4cbca6a5a/part-00002-5ff8f861-7c0c-4547-9179-a4bb7b38c21c-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=f3379b39-5ef8-4e72-93e5-b23bee074e25, location=europe-west6}
[2023-04-12T14:01:20.670+0000] {subprocess.py:93} INFO - 23/04/12 14:01:17 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_defense_stats_2020_2. jobId: JobId{project=nfl-de-project, job=f3379b39-5ef8-4e72-93e5-b23bee074e25, location=europe-west6}
[2023-04-12T14:01:20.671+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-12T14:01:30.628+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.629+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.630+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:30.631+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:30.632+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:30.633+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:30.634+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:30.635+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:30.636+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:30.637+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:30.639+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:30.640+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:30.640+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:30.641+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:30.641+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:30.642+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:30.642+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.643+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.643+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:30.644+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:30.644+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:30.645+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:30.646+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:30.646+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:30.647+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:30.648+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:30.649+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:30.649+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:30.650+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:30.651+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:30.651+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:30.651+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:30.652+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.652+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.653+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:30.654+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:30.654+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:30.654+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:30.655+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:30.655+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:30.656+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:30.656+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:30.657+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:30.657+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:30.657+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:30.658+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:30.658+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:30.659+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:30.659+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.660+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.660+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:30.661+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:30.661+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:30.662+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:30.662+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:30.662+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:30.663+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:30.663+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:30.664+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:30.665+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:30.665+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:30.666+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:30.666+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:30.666+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:30.667+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/attempt_202304121401285892110885609752179_0086_m_000001_126/' directory.
[2023-04-12T14:01:30.667+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/attempt_20230412140128879046967325072780_0086_m_000002_127/' directory.
[2023-04-12T14:01:30.668+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/attempt_202304121401286205153741526246846_0086_m_000003_128/' directory.
[2023-04-12T14:01:30.669+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/attempt_202304121401286798845981097820177_0086_m_000000_125/' directory.
[2023-04-12T14:01:30.669+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:30.670+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:01:30.671+0000] {subprocess.py:93} INFO - 23/04/12 14:01:28 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:30.672+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.672+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:30.673+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:30.673+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:30.674+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:30.674+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:30.674+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:30.675+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:30.675+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:30.676+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:30.676+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:30.677+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:30.678+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:30.679+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:30.679+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:30.680+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:30.681+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:01:30.681+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:32.107+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/_temporary/0/_temporary/attempt_202304121401284306567985103175387_0086_m_000004_129/' directory.
[2023-04-12T14:01:32.108+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/' directory.
[2023-04-12T14:01:32.109+0000] {subprocess.py:93} INFO - 23/04/12 14:01:29 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=leaders_teammates_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=averageValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/part-00001-3c4a587c-b673-4cd9-8224-5f56d4d44805-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/part-00000-3c4a587c-b673-4cd9-8224-5f56d4d44805-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/part-00002-3c4a587c-b673-4cd9-8224-5f56d4d44805-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/part-00004-3c4a587c-b673-4cd9-8224-5f56d4d44805-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-96e9d2c0-5848-43aa-a599-f3b7efefda75/part-00003-3c4a587c-b673-4cd9-8224-5f56d4d44805-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=5b2235cc-b049-46ed-8da4-1aa1556d7baa, location=europe-west6}
[2023-04-12T14:01:34.656+0000] {subprocess.py:93} INFO - 23/04/12 14:01:31 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.leaders_teammates_2020_2. jobId: JobId{project=nfl-de-project, job=5b2235cc-b049-46ed-8da4-1aa1556d7baa, location=europe-west6}
[2023-04-12T14:01:34.657+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.658+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.658+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.659+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.659+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:34.660+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:34.660+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:34.661+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:34.661+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:34.662+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:34.662+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:34.663+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:34.663+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:34.664+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:34.664+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:34.665+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:34.665+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:34.666+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:34.666+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.667+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.667+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:34.667+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:34.668+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:34.668+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:34.669+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:34.670+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:34.670+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:34.671+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:34.671+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:34.672+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:34.673+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:34.673+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:34.674+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:34.674+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:34.675+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.676+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:34.676+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:34.677+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:34.677+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:34.678+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:34.678+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:34.678+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:34.679+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:34.679+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:34.680+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:34.681+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:34.681+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:34.682+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:34.682+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:34.683+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.683+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:34.684+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:34.684+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:34.685+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:34.685+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:34.686+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:34.686+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:34.687+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:34.688+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:34.688+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:34.689+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:34.690+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:34.690+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:34.691+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:34.691+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/attempt_202304121401332503927682197302483_0099_m_000000_153/' directory.
[2023-04-12T14:01:34.692+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/attempt_202304121401333302303346516777562_0099_m_000002_155/' directory.
[2023-04-12T14:01:34.692+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/attempt_202304121401335707298435423757972_0099_m_000001_154/' directory.
[2023-04-12T14:01:34.693+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/attempt_202304121401338747625259265252480_0099_m_000003_156/' directory.
[2023-04-12T14:01:34.693+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:34.694+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.694+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:01:34.695+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:01:34.695+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:01:34.696+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:01:34.696+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:01:34.697+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:01:34.698+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:01:34.698+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:01:34.699+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:01:34.700+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:01:34.700+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:01:34.700+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:01:34.701+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:01:34.701+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:01:34.702+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:01:34.702+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:01:34.703+0000] {subprocess.py:93} INFO - 23/04/12 14:01:33 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/' directory.
[2023-04-12T14:01:34.703+0000] {subprocess.py:93} INFO - 23/04/12 14:01:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/_temporary/0/_temporary/attempt_202304121401337983705579995462923_0099_m_000004_157/' directory.
[2023-04-12T14:01:34.704+0000] {subprocess.py:93} INFO - 23/04/12 14:01:34 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/' directory.
[2023-04-12T14:01:34.705+0000] {subprocess.py:93} INFO - 23/04/12 14:01:34 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=radar_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/part-00002-933b5f8f-793d-4cf4-9f00-e3d6c86ebe89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/part-00001-933b5f8f-793d-4cf4-9f00-e3d6c86ebe89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/part-00003-933b5f8f-793d-4cf4-9f00-e3d6c86ebe89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/part-00000-933b5f8f-793d-4cf4-9f00-e3d6c86ebe89-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-9a2388ca-1b8c-4e33-8eed-9c64b663eb66/part-00004-933b5f8f-793d-4cf4-9f00-e3d6c86ebe89-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=5fb79abe-4f83-460f-a0d5-e1bcafc05c52, location=europe-west6}
[2023-04-12T14:01:37.444+0000] {subprocess.py:93} INFO - 23/04/12 14:01:37 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.radar_stats_2020_2. jobId: JobId{project=nfl-de-project, job=5fb79abe-4f83-460f-a0d5-e1bcafc05c52, location=europe-west6}
[2023-04-12T14:01:42.122+0000] {subprocess.py:93} INFO - 23/04/12 14:01:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:01:42.123+0000] {subprocess.py:93} INFO - 23/04/12 14:01:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:01:42.124+0000] {subprocess.py:93} INFO - 23/04/12 14:01:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:01:42.124+0000] {subprocess.py:93} INFO - 23/04/12 14:01:38 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-12T14:01:42.125+0000] {subprocess.py:93} INFO - 23/04/12 14:01:39 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 617b0eff-7d5c-426f-a9ae-555d455eb9b3.
[2023-04-12T14:01:42.126+0000] {subprocess.py:93} INFO - org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[2023-04-12T14:01:42.126+0000] {subprocess.py:93} INFO - ShuffleQueryStage 9
[2023-04-12T14:01:42.127+0000] {subprocess.py:93} INFO - +- Exchange hashpartitioning(name#2183, 200), ENSURE_REQUIREMENTS, [id=#10259]
[2023-04-12T14:01:42.128+0000] {subprocess.py:93} INFO -    +- *(10) Project [teamId#309, name#2183, value#2184]
[2023-04-12T14:01:42.129+0000] {subprocess.py:93} INFO -       +- *(10) BroadcastHashJoin [team#1945], [displayName#9], Inner, BuildRight, false
[2023-04-12T14:01:42.130+0000] {subprocess.py:93} INFO -          :- Generate stack(9, gamesPlayed, gamesPlayed#2065, totalYDS, totalYDS#2077, totalYDSG, totalYDSG#2089, passingYDS, passingYDS#2101, passingYDSG, passingYDSG#2113, rushingYDS, rushingYDS#2125, rushingYDSG, rushingYDSG#2137, points, points#2149, pointsPerGame, pointsPerGame#2161), [team#1945], false, [name#2183, value#2184]
[2023-04-12T14:01:42.131+0000] {subprocess.py:93} INFO -          :  +- *(9) Project [Team#1868 AS team#1945, cast(('Unnamed: 0_level_0', 'GP')#1869 as double) AS gamesPlayed#2065, cast(('Total', 'YDS')#1870 as double) AS totalYDS#2077, cast(('Total', 'YDS/G')#1871 as double) AS totalYDSG#2089, cast(('Passing', 'YDS')#1872 as double) AS passingYDS#2101, cast(('Passing', 'YDS/G')#1873 as double) AS passingYDSG#2113, cast(('Rushing', 'YDS')#1874 as double) AS rushingYDS#2125, cast(('Rushing', 'YDS/G')#1875 as double) AS rushingYDSG#2137, cast(('Points', 'PTS')#1876 as double) AS points#2149, cast(('Points', 'PTS/G')#1877 as double) AS pointsPerGame#2161]
[2023-04-12T14:01:42.132+0000] {subprocess.py:93} INFO -          :     +- *(9) Filter isnotnull(Team#1868)
[2023-04-12T14:01:42.133+0000] {subprocess.py:93} INFO -          :        +- InMemoryTableScan [('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Unnamed: 0_level_0', 'GP')#1869, Team#1868], [isnotnull(Team#1868)]
[2023-04-12T14:01:42.135+0000] {subprocess.py:93} INFO -          :              +- InMemoryRelation [Team#1868, ('Unnamed: 0_level_0', 'GP')#1869, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, __index_level_0__#1878L], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:01:42.136+0000] {subprocess.py:93} INFO -          :                    +- *(1) ColumnarToRow
[2023-04-12T14:01:42.137+0000] {subprocess.py:93} INFO -          :                       +- FileScan parquet [Team#1868,('Unnamed: 0_level_0', 'GP')#1869,('Total', 'YDS')#1870,('Total', 'YDS/G')#1871,('Passing', 'YDS')#1872,('Passing', 'YDS/G')#1873,('Rushing', 'YDS')#1874,('Rushing', 'YDS/G')#1875,('Points', 'PTS')#1876,('Points', 'PTS/G')#1877,__index_level_0__#1878L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams_defense_stats/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Team:string,('Unnamed: 0_level_0', 'GP'):string,('Total', 'YDS'):string,('Total', 'YDS/G')...
[2023-04-12T14:01:42.139+0000] {subprocess.py:93} INFO -          +- BroadcastQueryStage 3
[2023-04-12T14:01:42.139+0000] {subprocess.py:93} INFO -             +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#9821]
[2023-04-12T14:01:42.140+0000] {subprocess.py:93} INFO -                +- *(4) Project [displayName#9, cast(id#1 as int) AS teamId#309]
[2023-04-12T14:01:42.141+0000] {subprocess.py:93} INFO -                   +- *(4) Filter isnotnull(displayName#9)
[2023-04-12T14:01:42.141+0000] {subprocess.py:93} INFO -                      +- InMemoryTableScan [displayName#9, id#1], [isnotnull(displayName#9)]
[2023-04-12T14:01:42.142+0000] {subprocess.py:93} INFO -                            +- InMemoryRelation [$ref#0, id#1, guid#2, uid#3, slug#4, location#5, name#6, nickname#7, abbreviation#8, displayName#9, shortDisplayName#10, color#11, alternateColor#12, isActive#13, isAllStar#14, logos#15, links#16, alternateIds.sdr#17, record.$ref#18, venue.$ref#19, venue.id#20, venue.fullName#21, venue.address.city#22, venue.address.state#23, ... 19 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:01:42.143+0000] {subprocess.py:93} INFO -                                  +- *(1) ColumnarToRow
[2023-04-12T14:01:42.144+0000] {subprocess.py:93} INFO -                                     +- FileScan parquet [$ref#0,id#1,guid#2,uid#3,slug#4,location#5,name#6,nickname#7,abbreviation#8,displayName#9,shortDisplayName#10,color#11,alternateColor#12,isActive#13,isAllStar#14,logos#15,links#16,alternateIds.sdr#17,record.$ref#18,venue.$ref#19,venue.id#20,venue.fullName#21,venue.address.city#22,venue.address.state#23,... 19 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<$ref:string,id:string,guid:string,uid:string,slug:string,location:string,name:string,nickn...
[2023-04-12T14:01:42.145+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:01:42.146+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[2023-04-12T14:01:42.146+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:162)
[2023-04-12T14:01:42.147+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:80)
[2023-04-12T14:01:42.148+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.149+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:80)
[2023-04-12T14:01:42.151+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:195)
[2023-04-12T14:01:42.152+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:01:42.152+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:01:42.153+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:01:42.153+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:01:42.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:179)
[2023-04-12T14:01:42.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
[2023-04-12T14:01:42.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T14:01:42.159+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T14:01:42.160+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T14:01:42.161+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T14:01:42.162+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T14:01:42.164+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.166+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.167+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.169+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.170+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.171+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:01:42.172+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:01:42.173+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.174+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:01:42.175+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:01:42.176+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:01:42.177+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:01:42.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:01:42.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:01:42.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:01:42.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T14:01:42.185+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T14:01:42.186+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T14:01:42.187+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T14:01:42.188+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T14:01:42.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T14:01:42.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T14:01:42.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T14:01:42.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T14:01:42.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.198+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.199+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:01:42.200+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:01:42.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:01:42.202+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:01:42.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:01:42.205+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:01:42.206+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:01:42.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:01:42.210+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:01:42.211+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T14:01:42.212+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T14:01:42.213+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T14:01:42.214+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T14:01:42.216+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T14:01:42.217+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T14:01:42.217+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T14:01:42.219+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T14:01:42.219+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T14:01:42.220+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T14:01:42.221+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T14:01:42.221+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T14:01:42.222+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T14:01:42.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T14:01:42.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T14:01:42.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T14:01:42.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T14:01:42.226+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:01:42.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T14:01:42.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T14:01:42.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T14:01:42.230+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T14:01:42.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T14:01:42.231+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:01:42.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:01:42.234+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T14:01:42.234+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:01:42.235+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.237+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:01:42.237+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T14:01:42.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:01:42.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.240+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.242+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.243+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.243+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T14:01:42.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T14:01:42.245+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T14:01:42.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T14:01:42.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T14:01:42.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T14:01:42.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.252+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:01:42.255+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:01:42.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:01:42.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:01:42.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)
[2023-04-12T14:01:42.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:01:42.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:01:42.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T14:01:42.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.265+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.268+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:01:42.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:01:42.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:01:42.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:01:42.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
[2023-04-12T14:01:42.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:01:42.274+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:01:42.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.277+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.277+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.278+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
[2023-04-12T14:01:42.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
[2023-04-12T14:01:42.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:122)
[2023-04-12T14:01:42.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:121)
[2023-04-12T14:01:42.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:162)
[2023-04-12T14:01:42.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[2023-04-12T14:01:42.282+0000] {subprocess.py:93} INFO - 	... 76 more
[2023-04-12T14:01:42.283+0000] {subprocess.py:93} INFO - 23/04/12 14:01:39 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681308030725-f2500223-2184-4aac-8ae3-1062d14cdb79/' directory.
[2023-04-12T14:01:42.284+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-04-12T14:01:42.284+0000] {subprocess.py:93} INFO -   File "/tmp/deced7ab8fd44c70859e6f80ac439a5f/transform_pyspark.py", line 533, in <module>
[2023-04-12T14:01:42.285+0000] {subprocess.py:93} INFO -     classify.write.format('bigquery') \
[2023-04-12T14:01:42.286+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1107, in save
[2023-04-12T14:01:42.287+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2023-04-12T14:01:42.288+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2023-04-12T14:01:42.289+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-04-12T14:01:42.289+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o315.save.
[2023-04-12T14:01:42.290+0000] {subprocess.py:93} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-04-12T14:01:42.291+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:111)
[2023-04-12T14:01:42.292+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T14:01:42.292+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T14:01:42.293+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T14:01:42.294+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T14:01:42.295+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T14:01:42.296+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T14:01:42.297+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T14:01:42.298+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.299+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.300+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.301+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.302+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:01:42.303+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:01:42.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.304+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:01:42.305+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:01:42.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:01:42.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:01:42.308+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:01:42.308+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:01:42.310+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:01:42.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T14:01:42.311+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T14:01:42.312+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T14:01:42.313+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T14:01:42.314+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T14:01:42.315+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T14:01:42.315+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T14:01:42.316+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T14:01:42.317+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T14:01:42.318+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T14:01:42.320+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T14:01:42.321+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T14:01:42.322+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job aborted.
[2023-04-12T14:01:42.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
[2023-04-12T14:01:42.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T14:01:42.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T14:01:42.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T14:01:42.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T14:01:42.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.329+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.330+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.330+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.331+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:01:42.332+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:01:42.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:01:42.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:01:42.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:01:42.336+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:01:42.337+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:01:42.338+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:01:42.339+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:01:42.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:01:42.340+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T14:01:42.341+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T14:01:42.341+0000] {subprocess.py:93} INFO - 	... 35 more
[2023-04-12T14:01:42.342+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
[2023-04-12T14:01:42.342+0000] {subprocess.py:93} INFO - ShuffleQueryStage 9
[2023-04-12T14:01:42.343+0000] {subprocess.py:93} INFO - +- Exchange hashpartitioning(name#2183, 200), ENSURE_REQUIREMENTS, [id=#10259]
[2023-04-12T14:01:42.343+0000] {subprocess.py:93} INFO -    +- *(10) Project [teamId#309, name#2183, value#2184]
[2023-04-12T14:01:42.344+0000] {subprocess.py:93} INFO -       +- *(10) BroadcastHashJoin [team#1945], [displayName#9], Inner, BuildRight, false
[2023-04-12T14:01:42.344+0000] {subprocess.py:93} INFO -          :- Generate stack(9, gamesPlayed, gamesPlayed#2065, totalYDS, totalYDS#2077, totalYDSG, totalYDSG#2089, passingYDS, passingYDS#2101, passingYDSG, passingYDSG#2113, rushingYDS, rushingYDS#2125, rushingYDSG, rushingYDSG#2137, points, points#2149, pointsPerGame, pointsPerGame#2161), [team#1945], false, [name#2183, value#2184]
[2023-04-12T14:01:42.345+0000] {subprocess.py:93} INFO -          :  +- *(9) Project [Team#1868 AS team#1945, cast(('Unnamed: 0_level_0', 'GP')#1869 as double) AS gamesPlayed#2065, cast(('Total', 'YDS')#1870 as double) AS totalYDS#2077, cast(('Total', 'YDS/G')#1871 as double) AS totalYDSG#2089, cast(('Passing', 'YDS')#1872 as double) AS passingYDS#2101, cast(('Passing', 'YDS/G')#1873 as double) AS passingYDSG#2113, cast(('Rushing', 'YDS')#1874 as double) AS rushingYDS#2125, cast(('Rushing', 'YDS/G')#1875 as double) AS rushingYDSG#2137, cast(('Points', 'PTS')#1876 as double) AS points#2149, cast(('Points', 'PTS/G')#1877 as double) AS pointsPerGame#2161]
[2023-04-12T14:01:42.346+0000] {subprocess.py:93} INFO -          :     +- *(9) Filter isnotnull(Team#1868)
[2023-04-12T14:01:42.347+0000] {subprocess.py:93} INFO -          :        +- InMemoryTableScan [('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Unnamed: 0_level_0', 'GP')#1869, Team#1868], [isnotnull(Team#1868)]
[2023-04-12T14:01:42.347+0000] {subprocess.py:93} INFO -          :              +- InMemoryRelation [Team#1868, ('Unnamed: 0_level_0', 'GP')#1869, ('Total', 'YDS')#1870, ('Total', 'YDS/G')#1871, ('Passing', 'YDS')#1872, ('Passing', 'YDS/G')#1873, ('Rushing', 'YDS')#1874, ('Rushing', 'YDS/G')#1875, ('Points', 'PTS')#1876, ('Points', 'PTS/G')#1877, __index_level_0__#1878L], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:01:42.348+0000] {subprocess.py:93} INFO -          :                    +- *(1) ColumnarToRow
[2023-04-12T14:01:42.348+0000] {subprocess.py:93} INFO -          :                       +- FileScan parquet [Team#1868,('Unnamed: 0_level_0', 'GP')#1869,('Total', 'YDS')#1870,('Total', 'YDS/G')#1871,('Passing', 'YDS')#1872,('Passing', 'YDS/G')#1873,('Rushing', 'YDS')#1874,('Rushing', 'YDS/G')#1875,('Points', 'PTS')#1876,('Points', 'PTS/G')#1877,__index_level_0__#1878L] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams_defense_stats/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Team:string,('Unnamed: 0_level_0', 'GP'):string,('Total', 'YDS'):string,('Total', 'YDS/G')...
[2023-04-12T14:01:42.349+0000] {subprocess.py:93} INFO -          +- BroadcastQueryStage 3
[2023-04-12T14:01:42.350+0000] {subprocess.py:93} INFO -             +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [id=#9821]
[2023-04-12T14:01:42.350+0000] {subprocess.py:93} INFO -                +- *(4) Project [displayName#9, cast(id#1 as int) AS teamId#309]
[2023-04-12T14:01:42.351+0000] {subprocess.py:93} INFO -                   +- *(4) Filter isnotnull(displayName#9)
[2023-04-12T14:01:42.352+0000] {subprocess.py:93} INFO -                      +- InMemoryTableScan [displayName#9, id#1], [isnotnull(displayName#9)]
[2023-04-12T14:01:42.352+0000] {subprocess.py:93} INFO -                            +- InMemoryRelation [$ref#0, id#1, guid#2, uid#3, slug#4, location#5, name#6, nickname#7, abbreviation#8, displayName#9, shortDisplayName#10, color#11, alternateColor#12, isActive#13, isAllStar#14, logos#15, links#16, alternateIds.sdr#17, record.$ref#18, venue.$ref#19, venue.id#20, venue.fullName#21, venue.address.city#22, venue.address.state#23, ... 19 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)
[2023-04-12T14:01:42.353+0000] {subprocess.py:93} INFO -                                  +- *(1) ColumnarToRow
[2023-04-12T14:01:42.353+0000] {subprocess.py:93} INFO -                                     +- FileScan parquet [$ref#0,id#1,guid#2,uid#3,slug#4,location#5,name#6,nickname#7,abbreviation#8,displayName#9,shortDisplayName#10,color#11,alternateColor#12,isActive#13,isAllStar#14,logos#15,links#16,alternateIds.sdr#17,record.$ref#18,venue.$ref#19,venue.id#20,venue.fullName#21,venue.address.city#22,venue.address.state#23,... 19 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[gs://nfl-data-lake_nfl-de-project/nfl_parquets/teams/2020/2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<$ref:string,id:string,guid:string,uid:string,slug:string,location:string,name:string,nickn...
[2023-04-12T14:01:42.354+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:01:42.354+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
[2023-04-12T14:01:42.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:162)
[2023-04-12T14:01:42.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:80)
[2023-04-12T14:01:42.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:80)
[2023-04-12T14:01:42.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:195)
[2023-04-12T14:01:42.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:01:42.359+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:01:42.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:193)
[2023-04-12T14:01:42.360+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:01:42.361+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:179)
[2023-04-12T14:01:42.361+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:295)
[2023-04-12T14:01:42.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.362+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.363+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.364+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.365+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T14:01:42.366+0000] {subprocess.py:93} INFO - 	... 57 more
[2023-04-12T14:01:42.367+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T14:01:42.368+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T14:01:42.369+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T14:01:42.369+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T14:01:42.370+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T14:01:42.370+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:01:42.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T14:01:42.371+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T14:01:42.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T14:01:42.372+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T14:01:42.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T14:01:42.373+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:01:42.374+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.375+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:01:42.376+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T14:01:42.377+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:01:42.378+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.379+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.380+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:01:42.381+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T14:01:42.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:01:42.383+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.384+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.385+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.386+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.387+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T14:01:42.388+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T14:01:42.389+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T14:01:42.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T14:01:42.390+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T14:01:42.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T14:01:42.391+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.392+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.393+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:01:42.394+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:01:42.394+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:01:42.395+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:01:42.396+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)
[2023-04-12T14:01:42.396+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:01:42.396+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:01:42.397+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.398+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.398+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.399+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T14:01:42.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.400+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.401+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.402+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.403+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:01:42.404+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:01:42.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:01:42.405+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:01:42.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.inputRDDs(BroadcastHashJoinExec.scala:178)
[2023-04-12T14:01:42.406+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:01:42.407+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:01:42.408+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:01:42.408+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:01:42.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:01:42.409+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:01:42.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:01:42.410+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)
[2023-04-12T14:01:42.411+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)
[2023-04-12T14:01:42.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:122)
[2023-04-12T14:01:42.412+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:121)
[2023-04-12T14:01:42.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:162)
[2023-04-12T14:01:42.413+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
[2023-04-12T14:01:42.414+0000] {subprocess.py:93} INFO - 	... 76 more
[2023-04-12T14:01:42.415+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:01:42.415+0000] {subprocess.py:93} INFO - 23/04/12 14:01:39 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@6140d3c5{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-12T14:01:42.544+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [deced7ab8fd44c70859e6f80ac439a5f] failed with error:
[2023-04-12T14:01:42.545+0000] {subprocess.py:93} INFO - Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
[2023-04-12T14:01:42.546+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/deced7ab8fd44c70859e6f80ac439a5f?project=nfl-de-project&region=europe-west6
[2023-04-12T14:01:42.547+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait 'deced7ab8fd44c70859e6f80ac439a5f' --region 'europe-west6' --project 'nfl-de-project'
[2023-04-12T14:01:42.548+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/deced7ab8fd44c70859e6f80ac439a5f/
[2023-04-12T14:01:42.549+0000] {subprocess.py:93} INFO - gs://nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/deced7ab8fd44c70859e6f80ac439a5f/driveroutput
[2023-04-12T14:01:42.723+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2023-04-12T14:01:42.747+0000] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py", line 217, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion.py", line 224, in task_pyspark
    operator.execute(context={})
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-04-12T14:01:42.763+0000] {taskinstance.py:1327} INFO - Marking task as FAILED. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230412T140014, end_date=20230412T140142
[2023-04-12T14:01:42.807+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 958 for task task_pyspark (Bash command failed. The command returned a non-zero exit code 1.; 10916)
[2023-04-12T14:01:42.840+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-04-12T14:01:42.898+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
