[2023-04-23T15:52:26.697+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [up_for_retry]>, dependency 'Not In Retry Period' FAILED: Task is not ready for retry yet but will be retried automatically. Current date is 2023-04-23T15:52:26.697158+00:00 and task will be retried at 2023-04-23T15:57:24.912798+00:00.
[2023-04-23T15:52:26.709+0000] {local_task_job.py:151} INFO - Task is not able to be run
[2023-04-23T15:53:17.245+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [None]>
[2023-04-23T15:53:17.260+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [None]>
[2023-04-23T15:53:17.260+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T15:53:17.261+0000] {taskinstance.py:1289} INFO - Starting attempt 38 of 40
[2023-04-23T15:53:17.262+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-23T15:53:17.281+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-23T15:53:17.294+0000] {standard_task_runner.py:55} INFO - Started process 10828 to run task
[2023-04-23T15:53:17.297+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '1213', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmp2chzai48']
[2023-04-23T15:53:17.300+0000] {standard_task_runner.py:83} INFO - Job 1213: Subtask task_pyspark
[2023-04-23T15:53:17.368+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host 235fa867d994
[2023-04-23T15:53:17.436+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=38
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-23T15:53:17.439+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-23T15:53:17.443+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-23T15:53:17.460+0000] {subprocess.py:86} INFO - Output:
[2023-04-23T15:53:20.967+0000] {subprocess.py:93} INFO - Job [4e42e20c58c842878be4c8635098abc6] submitted.
[2023-04-23T15:53:20.968+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-23T15:53:35.461+0000] {subprocess.py:93} INFO - 23/04/23 15:53:30 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-23T15:53:35.462+0000] {subprocess.py:93} INFO - 23/04/23 15:53:30 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-23T15:53:35.463+0000] {subprocess.py:93} INFO - 23/04/23 15:53:30 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-23T15:53:35.464+0000] {subprocess.py:93} INFO - 23/04/23 15:53:30 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-23T15:53:35.465+0000] {subprocess.py:93} INFO - 23/04/23 15:53:31 INFO org.sparkproject.jetty.util.log: Logging initialized @4426ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-23T15:53:35.466+0000] {subprocess.py:93} INFO - 23/04/23 15:53:31 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-23T15:53:35.467+0000] {subprocess.py:93} INFO - 23/04/23 15:53:31 INFO org.sparkproject.jetty.server.Server: Started @4560ms
[2023-04-23T15:53:35.468+0000] {subprocess.py:93} INFO - 23/04/23 15:53:31 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@23a7bbee{HTTP/1.1, (http/1.1)}{0.0.0.0:41391}
[2023-04-23T15:53:35.469+0000] {subprocess.py:93} INFO - 23/04/23 15:53:32 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:53:42.618+0000] {subprocess.py:93} INFO - 23/04/23 15:53:39 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-23T15:53:42.619+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-23T15:53:42.619+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-23T15:53:47.932+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-23T15:53:47.933+0000] {subprocess.py:93} INFO - 23/04/23 15:53:46 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-23T15:53:54.159+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.160+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.160+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.161+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.162+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.163+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.163+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.164+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.164+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:53:54.165+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:53:54.165+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:53:54.166+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:53:54.167+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:53:54.167+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:53:54.168+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:53:54.169+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:53:54.170+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:53:54.170+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:53:54.171+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:53:54.171+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:53:54.172+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:53:54.172+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:53:54.173+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:53:54.174+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:53:54.175+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:53:54.176+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:53:54.177+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:53:54.177+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:53:54.178+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:53:54.179+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:53:54.180+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:53:54.181+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:53:54.182+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:53:54.182+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:53:54.183+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:53:54.183+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:53:54.184+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:53:54.184+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:53:54.185+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:53:54.186+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:53:54.187+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:53:54.188+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:53:54.188+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:53:54.189+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:53:54.190+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:53:54.190+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:53:54.191+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:53:54.191+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:53:54.192+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:53:54.192+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:53:54.193+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:53:54.193+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:53:54.193+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:53:54.194+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:53:54.194+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:53:54.195+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:53:54.195+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:53:54.196+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:53:54.196+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:53:54.197+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:53:54.197+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:53:54.198+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:53:54.199+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:53:54.199+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:53:54.199+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T15:53:54.200+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T15:53:54.200+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T15:53:54.201+0000] {subprocess.py:93} INFO - 23/04/23 15:53:47 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-23T15:53:54.202+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/attempt_202304231553474210100568231680106_0010_m_000001_10/' directory.
[2023-04-23T15:53:54.202+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/attempt_202304231553474306223890690833774_0010_m_000002_11/' directory.
[2023-04-23T15:53:54.203+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/attempt_202304231553478775097275084285336_0010_m_000003_12/' directory.
[2023-04-23T15:53:54.203+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/attempt_20230423155347644713310372661614_0010_m_000000_9/' directory.
[2023-04-23T15:53:54.204+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/' directory.
[2023-04-23T15:53:54.204+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:53:54.205+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/' directory.
[2023-04-23T15:53:54.205+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:53:54.206+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/' directory.
[2023-04-23T15:53:54.206+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:53:54.207+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/' directory.
[2023-04-23T15:53:54.207+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.208+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.208+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:53:54.209+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:53:54.209+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:53:54.210+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:53:54.210+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:53:54.210+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:53:54.211+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:53:54.212+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:53:54.212+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:53:54.212+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:53:54.213+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:53:54.213+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:53:54.214+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:53:54.214+0000] {subprocess.py:93} INFO - 23/04/23 15:53:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:53:54.215+0000] {subprocess.py:93} INFO - 23/04/23 15:53:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/_temporary/0/_temporary/attempt_202304231553475501374717559744144_0010_m_000004_13/' directory.
[2023-04-23T15:53:54.215+0000] {subprocess.py:93} INFO - 23/04/23 15:53:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/' directory.
[2023-04-23T15:53:54.216+0000] {subprocess.py:93} INFO - 23/04/23 15:53:49 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=teams_2022_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/part-00002-07fc8547-8e3b-4082-8788-a2f7e7a2c408-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/part-00003-07fc8547-8e3b-4082-8788-a2f7e7a2c408-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/part-00000-07fc8547-8e3b-4082-8788-a2f7e7a2c408-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/part-00001-07fc8547-8e3b-4082-8788-a2f7e7a2c408-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-0f07e9bf-da57-4271-b2a5-4ed028dd40d6/part-00004-07fc8547-8e3b-4082-8788-a2f7e7a2c408-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=59f2045c-026d-4022-a244-1b4f028b4aea, location=europe-west6}
[2023-04-23T15:53:54.216+0000] {subprocess.py:93} INFO - 23/04/23 15:53:52 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.teams_2022_2. jobId: JobId{project=nfl-project-de, job=59f2045c-026d-4022-a244-1b4f028b4aea, location=europe-west6}
[2023-04-23T15:53:54.217+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.218+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:53:54.219+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:53:54.220+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:53:54.220+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:53:54.221+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:53:54.221+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:53:54.222+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:53:54.222+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:53:54.223+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:53:54.223+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:53:54.224+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:53:54.224+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:53:54.224+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:53:54.225+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:53:54.226+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:53:56.984+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-eb2cf3c6-ad3c-4655-b781-e47c77b37cc1/_temporary/0/_temporary/attempt_202304231553527685647720007411658_0011_m_000000_14/' directory.
[2023-04-23T15:53:56.985+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-eb2cf3c6-ad3c-4655-b781-e47c77b37cc1/_temporary/0/_temporary/' directory.
[2023-04-23T15:53:56.986+0000] {subprocess.py:93} INFO - 23/04/23 15:53:53 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-eb2cf3c6-ad3c-4655-b781-e47c77b37cc1/' directory.
[2023-04-23T15:53:56.987+0000] {subprocess.py:93} INFO - 23/04/23 15:53:54 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=leaders_2022_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-eb2cf3c6-ad3c-4655-b781-e47c77b37cc1/part-00000-0f8c7564-6c4e-49b8-a32f-2fe214f237bd-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=93bee34c-aeb2-43c4-9561-a9cb372ef5d8, location=europe-west6}
[2023-04-23T15:54:01.682+0000] {subprocess.py:93} INFO - 23/04/23 15:53:58 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.leaders_2022_2. jobId: JobId{project=nfl-project-de, job=93bee34c-aeb2-43c4-9561-a9cb372ef5d8, location=europe-west6}
[2023-04-23T15:54:01.683+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:01.683+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:01.684+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:01.684+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:01.685+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:01.686+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:01.686+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:01.687+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:01.687+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:01.687+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:01.688+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:01.688+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:01.689+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:01.690+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:01.690+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:01.691+0000] {subprocess.py:93} INFO - 23/04/23 15:53:59 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:01.691+0000] {subprocess.py:93} INFO - 23/04/23 15:54:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-721c6b80-494c-41d3-8a9f-bcc8f86d8c4c/_temporary/0/_temporary/attempt_202304231553594904306454689980057_0012_m_000000_15/' directory.
[2023-04-23T15:54:01.692+0000] {subprocess.py:93} INFO - 23/04/23 15:54:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-721c6b80-494c-41d3-8a9f-bcc8f86d8c4c/_temporary/0/_temporary/' directory.
[2023-04-23T15:54:01.692+0000] {subprocess.py:93} INFO - 23/04/23 15:54:00 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-721c6b80-494c-41d3-8a9f-bcc8f86d8c4c/' directory.
[2023-04-23T15:54:01.693+0000] {subprocess.py:93} INFO - 23/04/23 15:54:00 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=teams_defense_stats_2022_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-721c6b80-494c-41d3-8a9f-bcc8f86d8c4c/part-00000-069805c4-7e01-4a42-b82f-48362cb21802-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=190acf73-5119-4add-bfb8-410bd700d13e, location=europe-west6}
[2023-04-23T15:54:07.242+0000] {subprocess.py:93} INFO - 23/04/23 15:54:04 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.teams_defense_stats_2022_2. jobId: JobId{project=nfl-project-de, job=190acf73-5119-4add-bfb8-410bd700d13e, location=europe-west6}
[2023-04-23T15:54:07.243+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.243+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.244+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:07.244+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:07.245+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:07.246+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:07.246+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:07.247+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:07.248+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:07.248+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:07.249+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:07.249+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:07.250+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:07.250+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:07.251+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:07.252+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:07.253+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.253+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.254+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.255+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.255+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:07.256+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:07.256+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:07.257+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:07.258+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:07.258+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:07.259+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:07.260+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:07.261+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:07.261+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:07.262+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:07.262+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:07.263+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:07.263+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:07.264+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:07.265+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:07.265+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:07.267+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:07.267+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:07.268+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:07.268+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:07.269+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:07.270+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:07.271+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:07.271+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:07.272+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:07.273+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:07.273+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:07.274+0000] {subprocess.py:93} INFO - 23/04/23 15:54:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.275+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:07.275+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:07.276+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:07.276+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:07.277+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:07.277+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:07.278+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:07.279+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:07.279+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:07.280+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:07.281+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:07.282+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:07.282+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:07.283+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:07.283+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:12.562+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/attempt_202304231554054211074692334980819_0016_m_000000_18/' directory.
[2023-04-23T15:54:12.563+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/attempt_202304231554057597051970504289427_0016_m_000001_19/' directory.
[2023-04-23T15:54:12.564+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/attempt_202304231554052444560834488983726_0016_m_000002_20/' directory.
[2023-04-23T15:54:12.564+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/attempt_202304231554054456183867717183739_0016_m_000003_21/' directory.
[2023-04-23T15:54:12.565+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/' directory.
[2023-04-23T15:54:12.566+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:12.568+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:12.569+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:12.570+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:12.571+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:12.571+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:12.571+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:12.572+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:12.573+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:12.574+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:12.575+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:12.575+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:12.576+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:12.577+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:12.577+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:12.578+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:12.578+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:54:12.579+0000] {subprocess.py:93} INFO - 23/04/23 15:54:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/' directory.
[2023-04-23T15:54:12.579+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:54:12.580+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/' directory.
[2023-04-23T15:54:12.582+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-23T15:54:12.583+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/' directory.
[2023-04-23T15:54:12.584+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/_temporary/0/_temporary/attempt_202304231554051834958277702972600_0016_m_000004_22/' directory.
[2023-04-23T15:54:12.584+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/' directory.
[2023-04-23T15:54:12.585+0000] {subprocess.py:93} INFO - 23/04/23 15:54:07 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=teams_stats_2022_2_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatperGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/part-00002-56b19a62-4591-4b22-be08-5fe322755fb9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/part-00001-56b19a62-4591-4b22-be08-5fe322755fb9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/part-00003-56b19a62-4591-4b22-be08-5fe322755fb9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/part-00000-56b19a62-4591-4b22-be08-5fe322755fb9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-605cf91a-23e1-4654-b26e-b5923f3c7ff6/part-00004-56b19a62-4591-4b22-be08-5fe322755fb9-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=c307f6b6-7e30-411d-962a-d51a79b28372, location=europe-west6}
[2023-04-23T15:54:12.585+0000] {subprocess.py:93} INFO - 23/04/23 15:54:10 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.teams_stats_2022_2_PARTITIONED. jobId: JobId{project=nfl-project-de, job=c307f6b6-7e30-411d-962a-d51a79b28372, location=europe-west6}
[2023-04-23T15:54:19.726+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:19.727+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-23T15:54:19.727+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-23T15:54:19.728+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-23T15:54:19.728+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-23T15:54:19.729+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-23T15:54:19.729+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-23T15:54:19.730+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-23T15:54:19.730+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-23T15:54:19.730+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-23T15:54:19.731+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-23T15:54:19.731+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-23T15:54:19.732+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-23T15:54:19.733+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-23T15:54:19.733+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-23T15:54:19.734+0000] {subprocess.py:93} INFO - 23/04/23 15:54:13 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-23T15:54:19.734+0000] {subprocess.py:93} INFO - 23/04/23 15:54:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-2774fb04-1298-492a-b880-73cf953bccac/_temporary/0/_temporary/attempt_202304231554115534512757618757926_0018_m_000000_24/' directory.
[2023-04-23T15:54:19.735+0000] {subprocess.py:93} INFO - 23/04/23 15:54:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-2774fb04-1298-492a-b880-73cf953bccac/_temporary/0/_temporary/' directory.
[2023-04-23T15:54:19.735+0000] {subprocess.py:93} INFO - 23/04/23 15:54:14 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-2774fb04-1298-492a-b880-73cf953bccac/' directory.
[2023-04-23T15:54:19.736+0000] {subprocess.py:93} INFO - 23/04/23 15:54:15 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=athletes_2022_2_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682265211432-2774fb04-1298-492a-b880-73cf953bccac/part-00000-d994ab59-195d-47af-8c9f-c84941cd47ea-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=28555927-549b-4ddd-b827-7acc107e56ba, location=europe-west6}
[2023-04-23T15:54:19.736+0000] {subprocess.py:93} INFO - 23/04/23 15:54:17 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.athletes_2022_2_PARTITIONED. jobId: JobId{project=nfl-project-de, job=28555927-549b-4ddd-b827-7acc107e56ba, location=europe-west6}
[2023-04-23T15:54:19.737+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-23T15:54:19.737+0000] {subprocess.py:93} INFO - 23/04/23 15:54:17 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@23a7bbee{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-23T15:54:26.489+0000] {subprocess.py:93} INFO - Job [4e42e20c58c842878be4c8635098abc6] finished successfully.
[2023-04-23T15:54:26.508+0000] {subprocess.py:93} INFO - done: true
[2023-04-23T15:54:26.509+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/4e42e20c58c842878be4c8635098abc6/
[2023-04-23T15:54:26.510+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/4e42e20c58c842878be4c8635098abc6/driveroutput
[2023-04-23T15:54:26.510+0000] {subprocess.py:93} INFO - jobUuid: 71a55b5f-77dc-38fb-8cd8-4341e41c024b
[2023-04-23T15:54:26.511+0000] {subprocess.py:93} INFO - placement:
[2023-04-23T15:54:26.512+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-23T15:54:26.513+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-23T15:54:26.514+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-23T15:54:26.515+0000] {subprocess.py:93} INFO -   args:
[2023-04-23T15:54:26.515+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-23T15:54:26.516+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-23T15:54:26.517+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-23T15:54:26.518+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-23T15:54:26.519+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-23T15:54:26.520+0000] {subprocess.py:93} INFO - reference:
[2023-04-23T15:54:26.521+0000] {subprocess.py:93} INFO -   jobId: 4e42e20c58c842878be4c8635098abc6
[2023-04-23T15:54:26.522+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-23T15:54:26.523+0000] {subprocess.py:93} INFO - status:
[2023-04-23T15:54:26.524+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-23T15:54:26.525+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T15:54:19.774239Z'
[2023-04-23T15:54:26.526+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-23T15:54:26.527+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-23T15:54:26.528+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T15:53:22.627553Z'
[2023-04-23T15:54:26.529+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-23T15:54:26.530+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T15:53:22.672331Z'
[2023-04-23T15:54:26.531+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-23T15:54:26.532+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-23T15:54:26.533+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-23T15:53:22.894521Z'
[2023-04-23T15:54:26.680+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-23T15:54:26.682+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-23T15:54:26.695+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230423T155317, end_date=20230423T155426
[2023-04-23T15:54:26.754+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-23T15:54:26.771+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
