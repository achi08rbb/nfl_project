[2023-04-12T14:25:14.053+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T14:25:14.086+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [queued]>
[2023-04-12T14:25:14.089+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T14:25:14.092+0000] {taskinstance.py:1289} INFO - Starting attempt 25 of 26
[2023-04-12T14:25:14.094+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-12T14:25:14.129+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-11 00:00:00+00:00
[2023-04-12T14:25:14.134+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance Not Running' FAILED: Task is in the running state
[2023-04-12T14:25:14.142+0000] {taskinstance.py:1080} INFO - Dependencies not met for <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]>, dependency 'Task Instance State' FAILED: Task is in the 'running' state.
[2023-04-12T14:25:14.173+0000] {local_task_job.py:151} INFO - Task is not able to be run
[2023-04-12T14:25:14.210+0000] {standard_task_runner.py:55} INFO - Started process 11947 to run task
[2023-04-12T14:25:14.228+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'scheduled__2023-04-11T00:00:00+00:00', '--job-id', '961', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmp993t4igf']
[2023-04-12T14:25:14.242+0000] {standard_task_runner.py:83} INFO - Job 961: Subtask task_pyspark
[2023-04-12T14:25:14.434+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark scheduled__2023-04-11T00:00:00+00:00 [running]> on host a2e619923d04
[2023-04-12T14:25:14.522+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-11T00:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=25
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2023-04-11T00:00:00+00:00
[2023-04-12T14:25:14.550+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-12T14:25:14.562+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-de-project/code/transform_pyspark.py                 --                     --year=2020                     --season_type=2 ']
[2023-04-12T14:25:14.581+0000] {subprocess.py:86} INFO - Output:
[2023-04-12T14:25:21.203+0000] {subprocess.py:93} INFO - Job [0c724e93c8414322b7e27c1af23a1248] submitted.
[2023-04-12T14:25:21.205+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-12T14:25:29.640+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-12T14:25:29.641+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-12T14:25:29.641+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-12T14:25:29.642+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-12T14:25:29.643+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.sparkproject.jetty.util.log: Logging initialized @4211ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-12T14:25:29.643+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-12T14:25:29.644+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.sparkproject.jetty.server.Server: Started @4342ms
[2023-04-12T14:25:29.644+0000] {subprocess.py:93} INFO - 23/04/12 14:25:29 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@74af881e{HTTP/1.1, (http/1.1)}{0.0.0.0:45761}
[2023-04-12T14:25:32.681+0000] {subprocess.py:93} INFO - 23/04/12 14:25:31 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:37.584+0000] {subprocess.py:93} INFO - 23/04/12 14:25:37 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-12T14:25:37.585+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-12T14:25:39.841+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-12T14:25:42.733+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-12T14:25:45.308+0000] {subprocess.py:93} INFO - 23/04/12 14:25:44 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-12T14:25:45.309+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.310+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.311+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.312+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.313+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.314+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.315+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.316+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:45.317+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:45.318+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:45.319+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:45.320+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:45.321+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:45.321+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:45.322+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:45.322+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:45.323+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:45.325+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:45.327+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:45.329+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:45.330+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:45.332+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:45.333+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:45.334+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:45.334+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:45.335+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:45.336+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:45.336+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:45.337+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:45.337+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:45.338+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:45.339+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:45.340+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:45.341+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:45.342+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:45.342+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:45.342+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:45.343+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:45.343+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:45.344+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:45.344+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:45.345+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:45.346+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:45.347+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:45.348+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:45.349+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:45.349+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:45.350+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:45.350+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:45.351+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:45.351+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:45.352+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:45.352+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:45.353+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:45.354+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:45.355+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:45.355+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:45.356+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:45.356+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:45.357+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:45.357+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:45.357+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:45.358+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:45.358+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:45.359+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:25:45.360+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:25:50.004+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:25:50.005+0000] {subprocess.py:93} INFO - 23/04/12 14:25:45 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-12T14:25:50.006+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/attempt_202304121425454933972428958009726_0010_m_000002_11/' directory.
[2023-04-12T14:25:50.006+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/attempt_202304121425454411062072189980436_0010_m_000000_9/' directory.
[2023-04-12T14:25:50.007+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/attempt_202304121425453426055608365944369_0010_m_000003_12/' directory.
[2023-04-12T14:25:50.007+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/attempt_202304121425458547157629615329528_0010_m_000001_10/' directory.
[2023-04-12T14:25:50.008+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:50.009+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:50.009+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:50.010+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:50.010+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:50.011+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:50.011+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:50.012+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:50.013+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:50.013+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:50.014+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:50.014+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:50.014+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:50.015+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:50.016+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:50.016+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:50.016+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:50.017+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:50.017+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:50.018+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:50.018+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:50.019+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:50.019+0000] {subprocess.py:93} INFO - 23/04/12 14:25:46 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:50.019+0000] {subprocess.py:93} INFO - 23/04/12 14:25:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/_temporary/0/_temporary/attempt_202304121425453969697128993606047_0010_m_000004_13/' directory.
[2023-04-12T14:25:50.020+0000] {subprocess.py:93} INFO - 23/04/12 14:25:47 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/' directory.
[2023-04-12T14:25:50.020+0000] {subprocess.py:93} INFO - 23/04/12 14:25:47 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=location, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isAllStar, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=logo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/part-00000-fb478f51-e97e-471b-ab87-cfe362a7ce9e-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/part-00001-fb478f51-e97e-471b-ab87-cfe362a7ce9e-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/part-00004-fb478f51-e97e-471b-ab87-cfe362a7ce9e-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/part-00002-fb478f51-e97e-471b-ab87-cfe362a7ce9e-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-ec6d45ea-5790-4d28-9c45-6219747229ce/part-00003-fb478f51-e97e-471b-ab87-cfe362a7ce9e-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=eab5d302-86bb-458a-8754-1804012d6efd, location=europe-west6}
[2023-04-12T14:25:50.020+0000] {subprocess.py:93} INFO - 23/04/12 14:25:49 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_2020_2. jobId: JobId{project=nfl-de-project, job=eab5d302-86bb-458a-8754-1804012d6efd, location=europe-west6}
[2023-04-12T14:25:52.848+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.849+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.849+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:52.850+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:52.850+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:52.851+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:52.851+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:52.851+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:52.852+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:52.852+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:52.853+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:52.853+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:52.854+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:52.854+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:52.854+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:52.855+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:52.855+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.856+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.856+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:52.856+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:52.857+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:52.857+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:52.857+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:52.858+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:52.858+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:52.859+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:52.859+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:52.859+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:52.860+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:52.860+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:52.860+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:52.861+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:52.861+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.862+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.862+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.863+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:52.863+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:52.864+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:52.864+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:52.865+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.865+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:52.866+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:52.866+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:52.867+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:52.867+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:52.868+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:52.868+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:52.868+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:52.869+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:52.870+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:52.870+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:52.871+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:52.871+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:52.872+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:52.872+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:52.873+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:52.873+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:52.874+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:52.874+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:52.874+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:52.875+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:52.875+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:52.875+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:52.876+0000] {subprocess.py:93} INFO - 23/04/12 14:25:50 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:52.876+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/attempt_202304121425506205330881566249272_0014_m_000000_16/' directory.
[2023-04-12T14:25:52.877+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/attempt_202304121425502602867887893356345_0014_m_000002_18/' directory.
[2023-04-12T14:25:52.877+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/attempt_202304121425504398518183097856406_0014_m_000001_17/' directory.
[2023-04-12T14:25:52.877+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/attempt_202304121425507132414424504161111_0014_m_000003_19/' directory.
[2023-04-12T14:25:52.878+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:52.879+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.880+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:25:52.880+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:25:52.881+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:25:52.881+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:25:52.882+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:25:52.882+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:25:52.884+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:25:52.885+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:25:52.886+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:25:52.887+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:25:52.887+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:25:52.888+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:25:52.888+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:25:52.889+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:25:52.889+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:25:52.889+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:52.890+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:52.891+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:52.891+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:52.891+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:25:52.892+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/' directory.
[2023-04-12T14:25:52.892+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/_temporary/0/_temporary/attempt_202304121425502243971095093762665_0014_m_000004_20/' directory.
[2023-04-12T14:25:52.893+0000] {subprocess.py:93} INFO - 23/04/12 14:25:51 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/' directory.
[2023-04-12T14:25:52.893+0000] {subprocess.py:93} INFO - 23/04/12 14:25:52 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=teams_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/part-00001-636a179d-9479-42d1-902e-81b13ef544af-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/part-00003-636a179d-9479-42d1-902e-81b13ef544af-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/part-00002-636a179d-9479-42d1-902e-81b13ef544af-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/part-00004-636a179d-9479-42d1-902e-81b13ef544af-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-3ef01d40-53e1-458a-ad2c-88611bedc3f6/part-00000-636a179d-9479-42d1-902e-81b13ef544af-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=485b0e55-a1da-4a6c-bf4a-90eb776a8b01, location=europe-west6}
[2023-04-12T14:25:55.683+0000] {subprocess.py:93} INFO - 23/04/12 14:25:54 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.teams_stats_2020_2. jobId: JobId{project=nfl-de-project, job=485b0e55-a1da-4a6c-bf4a-90eb776a8b01, location=europe-west6}
[2023-04-12T14:26:00.352+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:00.353+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:00.354+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:00.354+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:00.355+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:00.356+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:00.356+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:00.357+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:00.358+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:00.358+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:00.359+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:00.360+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:00.361+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:00.362+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:00.362+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:00.363+0000] {subprocess.py:93} INFO - 23/04/12 14:25:56 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:00.364+0000] {subprocess.py:93} INFO - 23/04/12 14:25:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-cd091ebb-e437-4bfa-bea1-32329b4d14ad/_temporary/0/_temporary/attempt_2023041214255548761987007696579_0016_m_000000_22/' directory.
[2023-04-12T14:26:00.364+0000] {subprocess.py:93} INFO - 23/04/12 14:25:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-cd091ebb-e437-4bfa-bea1-32329b4d14ad/_temporary/0/_temporary/' directory.
[2023-04-12T14:26:00.365+0000] {subprocess.py:93} INFO - 23/04/12 14:25:57 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-cd091ebb-e437-4bfa-bea1-32329b4d14ad/' directory.
[2023-04-12T14:26:00.365+0000] {subprocess.py:93} INFO - 23/04/12 14:25:58 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=athletes_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weight_lbs, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=height_inches, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayHeight, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-cd091ebb-e437-4bfa-bea1-32329b4d14ad/part-00000-30a65c41-72b0-45e6-b8e7-89b21d358bc6-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=0642907b-46cf-45b4-8c1a-7da15fc9576a, location=europe-west6}
[2023-04-12T14:26:02.621+0000] {subprocess.py:93} INFO - 23/04/12 14:26:00 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.athletes_2020_2. jobId: JobId{project=nfl-de-project, job=0642907b-46cf-45b4-8c1a-7da15fc9576a, location=europe-west6}
[2023-04-12T14:26:05.470+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.472+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.474+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:05.475+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:05.476+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:05.477+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:05.478+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:05.479+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:05.481+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:05.482+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:05.483+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:05.484+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:05.485+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:05.486+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:05.488+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:05.489+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:05.490+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.491+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.492+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:05.493+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:05.494+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:05.495+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:05.497+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:05.497+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:05.498+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:05.499+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:05.499+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:05.500+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:05.501+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:05.502+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:05.502+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:05.503+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:05.504+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.505+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.505+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.506+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:05.506+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:05.507+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:05.508+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:05.509+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:05.509+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:05.510+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:05.511+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:05.511+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:05.512+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:05.512+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:05.513+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:05.513+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:05.514+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:05.515+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:05.516+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:05.517+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:05.517+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:05.518+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:05.518+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:05.519+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:05.520+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:05.520+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:05.521+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:05.522+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:05.522+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:05.524+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:05.525+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:05.526+0000] {subprocess.py:93} INFO - 23/04/12 14:26:04 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:05.526+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/attempt_202304121426037493842358967423774_0020_m_000003_29/' directory.
[2023-04-12T14:26:05.527+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/attempt_202304121426038488435025705236877_0020_m_000000_26/' directory.
[2023-04-12T14:26:05.527+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/attempt_202304121426037216547380093207341_0020_m_000002_28/' directory.
[2023-04-12T14:26:08.297+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/attempt_202304121426037243386645285905750_0020_m_000001_27/' directory.
[2023-04-12T14:26:08.298+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/' directory.
[2023-04-12T14:26:08.299+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:26:08.300+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/' directory.
[2023-04-12T14:26:08.301+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:08.302+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:08.303+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:08.304+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:08.305+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:08.306+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:08.306+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:08.307+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:08.308+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:08.308+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:08.309+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:08.309+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:08.310+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:08.310+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:08.312+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:08.313+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:08.313+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-12T14:26:08.314+0000] {subprocess.py:93} INFO - 23/04/12 14:26:05 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/' directory.
[2023-04-12T14:26:08.315+0000] {subprocess.py:93} INFO - 23/04/12 14:26:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/_temporary/0/_temporary/attempt_202304121426031292816057530213633_0020_m_000004_30/' directory.
[2023-04-12T14:26:08.316+0000] {subprocess.py:93} INFO - 23/04/12 14:26:06 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/' directory.
[2023-04-12T14:26:08.316+0000] {subprocess.py:93} INFO - 23/04/12 14:26:06 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=athletes_stats_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=description, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=abbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=perGameValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=category, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=rank, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/part-00001-68fcb2f3-98af-4aa3-8686-77c155ce848a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/part-00003-68fcb2f3-98af-4aa3-8686-77c155ce848a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/part-00000-68fcb2f3-98af-4aa3-8686-77c155ce848a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/part-00002-68fcb2f3-98af-4aa3-8686-77c155ce848a-c000.snappy.parquet, gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-5bb9a50f-d1fb-4644-b76b-2d40403ea0ac/part-00004-68fcb2f3-98af-4aa3-8686-77c155ce848a-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=8e04f88d-16d6-43e3-a9bd-8b5f9138025e, location=europe-west6}
[2023-04-12T14:26:13.077+0000] {subprocess.py:93} INFO - 23/04/12 14:26:09 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.athletes_stats_2020_2. jobId: JobId{project=nfl-de-project, job=8e04f88d-16d6-43e3-a9bd-8b5f9138025e, location=europe-west6}
[2023-04-12T14:26:13.078+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:13.079+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-12T14:26:13.079+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-12T14:26:13.080+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-12T14:26:13.080+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-12T14:26:13.080+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-12T14:26:13.081+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-12T14:26:13.081+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-12T14:26:13.082+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-12T14:26:13.082+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-12T14:26:13.083+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-12T14:26:13.083+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-12T14:26:13.083+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-12T14:26:13.084+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-12T14:26:13.085+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-12T14:26:13.085+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-12T14:26:13.086+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-05c87c4c-e318-4ac5-8a9c-b22b69b4ef14/_temporary/0/_temporary/attempt_202304121426091834439347581463925_0021_m_000000_31/' directory.
[2023-04-12T14:26:13.086+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-05c87c4c-e318-4ac5-8a9c-b22b69b4ef14/_temporary/0/_temporary/' directory.
[2023-04-12T14:26:13.086+0000] {subprocess.py:93} INFO - 23/04/12 14:26:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-05c87c4c-e318-4ac5-8a9c-b22b69b4ef14/' directory.
[2023-04-12T14:26:13.087+0000] {subprocess.py:93} INFO - 23/04/12 14:26:11 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-de-project, tableId=leaders_2020_2}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=displayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=value, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=name, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-05c87c4c-e318-4ac5-8a9c-b22b69b4ef14/part-00000-0cdce006-eac8-44a6-bd2c-421e6eab1bbd-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-de-project, job=91ffdefc-b0d1-4508-9b64-2fdd5e781c92, location=europe-west6}
[2023-04-12T14:26:17.056+0000] {subprocess.py:93} INFO - 23/04/12 14:26:13 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-de-project.nfl_data_all.leaders_2020_2. jobId: JobId{project=nfl-de-project, job=91ffdefc-b0d1-4508-9b64-2fdd5e781c92, location=europe-west6}
[2023-04-12T14:26:17.057+0000] {subprocess.py:93} INFO - 23/04/12 14:26:13 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 58040cea-4631-41cb-b6ba-a0f5317895e9.
[2023-04-12T14:26:17.058+0000] {subprocess.py:93} INFO - org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T14:26:17.059+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T14:26:17.060+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T14:26:17.061+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T14:26:17.062+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T14:26:17.063+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:26:17.064+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T14:26:17.065+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T14:26:17.067+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T14:26:17.068+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T14:26:17.070+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T14:26:17.071+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:26:17.072+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.073+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.074+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.075+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:26:17.077+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T14:26:17.078+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:26:17.080+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.081+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.082+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.084+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:26:17.085+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T14:26:17.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:26:17.086+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.087+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.088+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.089+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.090+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T14:26:17.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T14:26:17.091+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T14:26:17.092+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T14:26:17.093+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T14:26:17.094+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T14:26:17.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.095+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.096+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.097+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.098+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:26:17.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:26:17.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:26:17.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:26:17.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:26:17.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:26:17.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.103+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.104+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.105+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.106+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.106+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T14:26:17.107+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.108+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.109+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.109+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.110+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.110+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:26:17.111+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:26:17.111+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:26:17.112+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:26:17.112+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:26:17.113+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:26:17.114+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.114+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.115+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.116+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T14:26:17.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T14:26:17.120+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T14:26:17.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T14:26:17.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T14:26:17.122+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.123+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.124+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.125+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.126+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:26:17.127+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:26:17.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.128+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:26:17.129+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:26:17.130+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:26:17.131+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:26:17.132+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:26:17.133+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.134+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:26:17.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:26:17.135+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T14:26:17.136+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T14:26:17.137+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T14:26:17.138+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T14:26:17.140+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T14:26:17.141+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T14:26:17.142+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T14:26:17.143+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T14:26:17.144+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T14:26:17.145+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.147+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.148+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.149+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.150+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:26:17.151+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:26:17.152+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.153+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:26:17.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:26:17.154+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:26:17.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:26:17.155+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:26:17.156+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.157+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:26:17.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:26:17.158+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T14:26:17.159+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T14:26:17.160+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T14:26:17.161+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T14:26:17.161+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T14:26:17.162+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T14:26:17.163+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T14:26:17.164+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T14:26:17.164+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T14:26:17.166+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T14:26:17.167+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T14:26:17.167+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T14:26:17.168+0000] {subprocess.py:93} INFO - 23/04/12 14:26:13 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-834475897757-zf5hpybi/.spark-bigquery-local-1681309529955-cf8820bb-e931-4b88-912e-8c9683d72f23/' directory.
[2023-04-12T14:26:17.169+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2023-04-12T14:26:17.170+0000] {subprocess.py:93} INFO -   File "/tmp/0c724e93c8414322b7e27c1af23a1248/transform_pyspark.py", line 246, in <module>
[2023-04-12T14:26:17.170+0000] {subprocess.py:93} INFO -     df.write.format('bigquery') \
[2023-04-12T14:26:17.171+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1107, in save
[2023-04-12T14:26:17.172+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
[2023-04-12T14:26:17.173+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco
[2023-04-12T14:26:17.173+0000] {subprocess.py:93} INFO -   File "/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 326, in get_return_value
[2023-04-12T14:26:17.174+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o304.save.
[2023-04-12T14:26:17.175+0000] {subprocess.py:93} INFO - : com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery
[2023-04-12T14:26:17.176+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:111)
[2023-04-12T14:26:17.177+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)
[2023-04-12T14:26:17.178+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)
[2023-04-12T14:26:17.179+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)
[2023-04-12T14:26:17.179+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
[2023-04-12T14:26:17.180+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
[2023-04-12T14:26:17.181+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
[2023-04-12T14:26:17.183+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
[2023-04-12T14:26:17.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.187+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.188+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:26:17.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:26:17.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:26:17.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:26:17.192+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:26:17.193+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:26:17.194+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:26:17.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.196+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:26:17.197+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:26:17.198+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
[2023-04-12T14:26:17.199+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-04-12T14:26:17.200+0000] {subprocess.py:93} INFO - 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-04-12T14:26:17.200+0000] {subprocess.py:93} INFO - 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-04-12T14:26:17.201+0000] {subprocess.py:93} INFO - 	at java.lang.reflect.Method.invoke(Method.java:498)
[2023-04-12T14:26:17.201+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-04-12T14:26:17.202+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[2023-04-12T14:26:17.202+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-04-12T14:26:17.203+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-04-12T14:26:17.203+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-04-12T14:26:17.204+0000] {subprocess.py:93} INFO - 	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[2023-04-12T14:26:17.205+0000] {subprocess.py:93} INFO - 	at java.lang.Thread.run(Thread.java:750)
[2023-04-12T14:26:17.206+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: Job aborted.
[2023-04-12T14:26:17.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)
[2023-04-12T14:26:17.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)
[2023-04-12T14:26:17.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[2023-04-12T14:26:17.209+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[2023-04-12T14:26:17.210+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)
[2023-04-12T14:26:17.211+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.212+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.212+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.213+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.214+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.215+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)
[2023-04-12T14:26:17.216+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)
[2023-04-12T14:26:17.217+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.218+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
[2023-04-12T14:26:17.219+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
[2023-04-12T14:26:17.220+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
[2023-04-12T14:26:17.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
[2023-04-12T14:26:17.222+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[2023-04-12T14:26:17.222+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
[2023-04-12T14:26:17.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
[2023-04-12T14:26:17.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
[2023-04-12T14:26:17.224+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
[2023-04-12T14:26:17.225+0000] {subprocess.py:93} INFO - 	at com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:106)
[2023-04-12T14:26:17.226+0000] {subprocess.py:93} INFO - 	... 35 more
[2023-04-12T14:26:17.227+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.sql.AnalysisException: Attribute name "('Unnamed: 0_level_0', 'GP')" contains invalid character(s) among " ,;{}()\n\t=". Please use alias to rename it.
[2023-04-12T14:26:17.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkConversionRequirement(ParquetSchemaConverter.scala:579)
[2023-04-12T14:26:17.228+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$.checkFieldName(ParquetSchemaConverter.scala:570)
[2023-04-12T14:26:17.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2(ParquetWriteSupport.scala:485)
[2023-04-12T14:26:17.230+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.$anonfun$setSchema$2$adapted(ParquetWriteSupport.scala:485)
[2023-04-12T14:26:17.231+0000] {subprocess.py:93} INFO - 	at scala.collection.immutable.List.foreach(List.scala:431)
[2023-04-12T14:26:17.232+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport$.setSchema(ParquetWriteSupport.scala:485)
[2023-04-12T14:26:17.233+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:220)
[2023-04-12T14:26:17.234+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)
[2023-04-12T14:26:17.235+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)
[2023-04-12T14:26:17.236+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:497)
[2023-04-12T14:26:17.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:26:17.238+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.239+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.240+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.241+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:26:17.242+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)
[2023-04-12T14:26:17.243+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:207)
[2023-04-12T14:26:17.244+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.245+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.245+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.246+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:203)
[2023-04-12T14:26:17.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:202)
[2023-04-12T14:26:17.247+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:26:17.248+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.249+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.250+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.251+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.252+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.253+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:252)
[2023-04-12T14:26:17.254+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:221)
[2023-04-12T14:26:17.255+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:144)
[2023-04-12T14:26:17.256+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:95)
[2023-04-12T14:26:17.257+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:81)
[2023-04-12T14:26:17.258+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:155)
[2023-04-12T14:26:17.259+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.260+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.261+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.262+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.263+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:26:17.264+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:26:17.266+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:26:17.267+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:26:17.269+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:26:17.270+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:26:17.271+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.272+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.273+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.275+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.276+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.277+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.GenerateExec.doExecute(GenerateExec.scala:80)
[2023-04-12T14:26:17.278+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.279+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.280+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.281+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.282+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)
[2023-04-12T14:26:17.283+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)
[2023-04-12T14:26:17.284+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)
[2023-04-12T14:26:17.285+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)
[2023-04-12T14:26:17.286+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)
[2023-04-12T14:26:17.287+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)
[2023-04-12T14:26:17.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)
[2023-04-12T14:26:17.288+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)
[2023-04-12T14:26:17.290+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[2023-04-12T14:26:17.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)
[2023-04-12T14:26:17.291+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)
[2023-04-12T14:26:17.292+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)
[2023-04-12T14:26:17.293+0000] {subprocess.py:93} INFO - 	... 57 more
[2023-04-12T14:26:17.294+0000] {subprocess.py:93} INFO - 
[2023-04-12T14:26:17.296+0000] {subprocess.py:93} INFO - 23/04/12 14:26:14 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@74af881e{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-12T14:26:20.300+0000] {subprocess.py:93} INFO - ERROR: (gcloud.dataproc.jobs.submit.pyspark) Job [0c724e93c8414322b7e27c1af23a1248] failed with error:
[2023-04-12T14:26:20.301+0000] {subprocess.py:93} INFO - Google Cloud Dataproc Agent reports job failure. If logs are available, they can be found at:
[2023-04-12T14:26:20.302+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/dataproc/jobs/0c724e93c8414322b7e27c1af23a1248?project=nfl-de-project&region=europe-west6
[2023-04-12T14:26:20.303+0000] {subprocess.py:93} INFO - gcloud dataproc jobs wait '0c724e93c8414322b7e27c1af23a1248' --region 'europe-west6' --project 'nfl-de-project'
[2023-04-12T14:26:20.303+0000] {subprocess.py:93} INFO - https://console.cloud.google.com/storage/browser/nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/0c724e93c8414322b7e27c1af23a1248/
[2023-04-12T14:26:20.304+0000] {subprocess.py:93} INFO - gs://nfl-spark-staging_nfl-de-project/google-cloud-dataproc-metainfo/cf386a87-5cdc-4f46-9dda-06a203a6bb84/jobs/0c724e93c8414322b7e27c1af23a1248/driveroutput
[2023-04-12T14:26:20.481+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2023-04-12T14:26:20.498+0000] {taskinstance.py:1776} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/decorators/base.py", line 217, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 175, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/python.py", line 192, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion.py", line 224, in task_pyspark
    operator.execute(context={})
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2023-04-12T14:26:20.513+0000] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230411T000000, start_date=20230412T142514, end_date=20230412T142620
[2023-04-12T14:26:20.545+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 961 for task task_pyspark (Bash command failed. The command returned a non-zero exit code 1.; 11947)
[2023-04-12T14:26:20.565+0000] {local_task_job.py:212} INFO - Task exited with return code 1
[2023-04-12T14:26:20.585+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
