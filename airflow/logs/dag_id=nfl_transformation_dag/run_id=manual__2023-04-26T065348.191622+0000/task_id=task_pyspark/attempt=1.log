[2023-04-26T06:54:05.319+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-26T06:53:48.191622+00:00 [queued]>
[2023-04-26T06:54:05.334+0000] {taskinstance.py:1090} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-26T06:53:48.191622+00:00 [queued]>
[2023-04-26T06:54:05.335+0000] {taskinstance.py:1288} INFO - 
--------------------------------------------------------------------------------
[2023-04-26T06:54:05.336+0000] {taskinstance.py:1289} INFO - Starting attempt 1 of 3
[2023-04-26T06:54:05.337+0000] {taskinstance.py:1290} INFO - 
--------------------------------------------------------------------------------
[2023-04-26T06:54:05.351+0000] {taskinstance.py:1309} INFO - Executing <Task(_PythonDecoratedOperator): task_pyspark> on 2023-04-26 06:53:48.191622+00:00
[2023-04-26T06:54:05.361+0000] {standard_task_runner.py:55} INFO - Started process 962 to run task
[2023-04-26T06:54:05.367+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'nfl_transformation_dag', 'task_pyspark', 'manual__2023-04-26T06:53:48.191622+00:00', '--job-id', '1305', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion.py', '--cfg-path', '/tmp/tmpz6j2pq16']
[2023-04-26T06:54:05.370+0000] {standard_task_runner.py:83} INFO - Job 1305: Subtask task_pyspark
[2023-04-26T06:54:05.446+0000] {task_command.py:389} INFO - Running <TaskInstance: nfl_transformation_dag.task_pyspark manual__2023-04-26T06:53:48.191622+00:00 [running]> on host 36bdc6733002
[2023-04-26T06:54:05.512+0000] {taskinstance.py:1516} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=nfl_transformation_dag
AIRFLOW_CTX_TASK_ID=task_pyspark
AIRFLOW_CTX_EXECUTION_DATE=2023-04-26T06:53:48.191622+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2023-04-26T06:53:48.191622+00:00
[2023-04-26T06:54:05.516+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2023-04-26T06:54:05.519+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'gcloud dataproc jobs submit pyspark                 --cluster=nfl-spark-cluster                 --region=europe-west6                 --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar                gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py                 --                     --year=2022                     --season_type=2 ']
[2023-04-26T06:54:05.535+0000] {subprocess.py:86} INFO - Output:
[2023-04-26T06:54:10.895+0000] {subprocess.py:93} INFO - Job [0eefb2497d3a494e81feba21da23bcc8] submitted.
[2023-04-26T06:54:10.895+0000] {subprocess.py:93} INFO - Waiting for job output...
[2023-04-26T06:54:23.069+0000] {subprocess.py:93} INFO - 23/04/26 06:54:19 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker
[2023-04-26T06:54:23.070+0000] {subprocess.py:93} INFO - 23/04/26 06:54:19 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster
[2023-04-26T06:54:23.071+0000] {subprocess.py:93} INFO - 23/04/26 06:54:19 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat
[2023-04-26T06:54:23.072+0000] {subprocess.py:93} INFO - 23/04/26 06:54:19 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator
[2023-04-26T06:54:23.072+0000] {subprocess.py:93} INFO - 23/04/26 06:54:19 INFO org.sparkproject.jetty.util.log: Logging initialized @5497ms to org.sparkproject.jetty.util.log.Slf4jLog
[2023-04-26T06:54:23.073+0000] {subprocess.py:93} INFO - 23/04/26 06:54:20 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_322-b06
[2023-04-26T06:54:23.073+0000] {subprocess.py:93} INFO - 23/04/26 06:54:20 INFO org.sparkproject.jetty.server.Server: Started @5654ms
[2023-04-26T06:54:23.074+0000] {subprocess.py:93} INFO - 23/04/26 06:54:20 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@3c677{HTTP/1.1, (http/1.1)}{0.0.0.0:33021}
[2023-04-26T06:54:27.799+0000] {subprocess.py:93} INFO - 23/04/26 06:54:22 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:32.562+0000] {subprocess.py:93} INFO - 23/04/26 06:54:29 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
[2023-04-26T06:54:32.563+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df started
[2023-04-26T06:54:32.564+0000] {subprocess.py:93} INFO - Cleaning the data for the teams_df is successful!
[2023-04-26T06:54:39.088+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet started
[2023-04-26T06:54:41.922+0000] {subprocess.py:93} INFO - 23/04/26 06:54:37 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new decompressor [.snappy]
[2023-04-26T06:54:41.923+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.924+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.925+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.925+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.926+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.926+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.927+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.928+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:41.929+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:41.930+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:41.930+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:41.931+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:41.932+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:41.932+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:41.933+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:41.933+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:41.934+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:41.936+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:41.937+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:41.938+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:41.938+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:41.939+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:41.940+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:41.942+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:41.943+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:41.944+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:41.945+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:41.945+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:41.946+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:41.946+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:41.947+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:41.947+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:41.948+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:41.948+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:41.950+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:41.950+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:41.951+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:41.952+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:41.952+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:41.953+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:41.954+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:41.954+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:41.955+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:41.956+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:41.957+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:41.958+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:41.959+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:41.960+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:41.960+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:41.960+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:41.961+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:41.962+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:41.963+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:41.964+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:41.965+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:41.966+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:41.966+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:41.967+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:41.967+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:41.968+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:41.968+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:41.969+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:41.970+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:41.971+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:41.973+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T06:54:41.973+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T06:54:41.974+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T06:54:41.974+0000] {subprocess.py:93} INFO - 23/04/26 06:54:39 INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new compressor [.snappy]
[2023-04-26T06:54:44.759+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/attempt_202304260654386440427875753724298_0010_m_000003_12/' directory.
[2023-04-26T06:54:44.761+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/attempt_202304260654383342115396308162694_0010_m_000001_10/' directory.
[2023-04-26T06:54:44.761+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/attempt_202304260654387123813570906892436_0010_m_000000_9/' directory.
[2023-04-26T06:54:44.762+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/attempt_202304260654385244265175607657384_0010_m_000002_11/' directory.
[2023-04-26T06:54:44.763+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:44.763+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:44.764+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:44.764+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:44.765+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:44.765+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:44.766+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:44.766+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:44.767+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:44.767+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:44.768+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:44.768+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:44.769+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:44.769+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:44.769+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:44.770+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:44.770+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:44.771+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:44.771+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:44.772+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:44.773+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:44.773+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:44.774+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:44.774+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/_temporary/0/_temporary/attempt_202304260654385852700468589372367_0010_m_000004_13/' directory.
[2023-04-26T06:54:44.775+0000] {subprocess.py:93} INFO - 23/04/26 06:54:40 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/' directory.
[2023-04-26T06:54:44.775+0000] {subprocess.py:93} INFO - 23/04/26 06:54:41 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLocation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=isActive, type=BOOLEAN, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/part-00002-0b688d10-c546-4b2a-8442-0096e7441327-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/part-00004-0b688d10-c546-4b2a-8442-0096e7441327-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/part-00000-0b688d10-c546-4b2a-8442-0096e7441327-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/part-00001-0b688d10-c546-4b2a-8442-0096e7441327-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-4e601ece-78ec-4959-90c6-8bb74b5dfe66/part-00003-0b688d10-c546-4b2a-8442-0096e7441327-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=52826bb3-98b3-4c99-9e80-dd7ad87282ec, location=europe-west6}
[2023-04-26T06:54:47.295+0000] {subprocess.py:93} INFO - 23/04/26 06:54:44 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams. jobId: JobId{project=nfl-project-de, job=52826bb3-98b3-4c99-9e80-dd7ad87282ec, location=europe-west6}
[2023-04-26T06:54:47.296+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:47.297+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:47.297+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:47.298+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:47.299+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:47.300+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:47.301+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:47.301+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:47.302+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:47.302+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:47.303+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:47.304+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:47.304+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:47.305+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:47.305+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:47.306+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:51.966+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a108a998-bd87-4274-8727-c186368d8bff/_temporary/0/_temporary/attempt_202304260654441909425505248885835_0011_m_000000_14/' directory.
[2023-04-26T06:54:51.966+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a108a998-bd87-4274-8727-c186368d8bff/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:51.967+0000] {subprocess.py:93} INFO - 23/04/26 06:54:45 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a108a998-bd87-4274-8727-c186368d8bff/' directory.
[2023-04-26T06:54:51.968+0000] {subprocess.py:93} INFO - 23/04/26 06:54:46 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_leaders}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderDisplayValue, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=leaderShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a108a998-bd87-4274-8727-c186368d8bff/part-00000-00d5c5fc-1798-4756-a32e-4cabc3cf493d-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=51bc1360-3b11-4cec-9d81-bacb362c41df, location=europe-west6}
[2023-04-26T06:54:51.968+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_leaders. jobId: JobId{project=nfl-project-de, job=51bc1360-3b11-4cec-9d81-bacb362c41df, location=europe-west6}
[2023-04-26T06:54:51.969+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:51.969+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:51.970+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:51.971+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:51.971+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:51.972+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:51.973+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:51.973+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:51.974+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:51.974+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:51.975+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:51.976+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:51.976+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:51.977+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:51.978+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:51.978+0000] {subprocess.py:93} INFO - 23/04/26 06:54:48 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:51.979+0000] {subprocess.py:93} INFO - 23/04/26 06:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-12369059-87b2-4a15-8a89-32485db31b1d/_temporary/0/_temporary/attempt_202304260654487298090262463973360_0012_m_000000_15/' directory.
[2023-04-26T06:54:51.979+0000] {subprocess.py:93} INFO - 23/04/26 06:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-12369059-87b2-4a15-8a89-32485db31b1d/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:51.980+0000] {subprocess.py:93} INFO - 23/04/26 06:54:49 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-12369059-87b2-4a15-8a89-32485db31b1d/' directory.
[2023-04-26T06:54:51.981+0000] {subprocess.py:93} INFO - 23/04/26 06:54:49 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_defense_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statCategory, type=STRING, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-12369059-87b2-4a15-8a89-32485db31b1d/part-00000-460dccc5-3ebe-4d41-b391-27bcb9df6dcb-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=54374fd6-ffb7-4c9b-b718-c62f65090b0e, location=europe-west6}
[2023-04-26T06:54:56.413+0000] {subprocess.py:93} INFO - 23/04/26 06:54:52 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_defense_stats. jobId: JobId{project=nfl-project-de, job=54374fd6-ffb7-4c9b-b718-c62f65090b0e, location=europe-west6}
[2023-04-26T06:54:56.414+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.416+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.417+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:56.418+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:56.419+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:56.421+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:56.421+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:56.423+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:56.424+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:56.425+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:56.426+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:56.427+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:56.428+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:56.430+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:56.431+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:56.433+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:56.434+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.435+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.436+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:56.437+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:56.438+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:56.439+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:56.441+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:56.441+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:56.443+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:56.444+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:56.445+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:56.447+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:56.448+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:56.449+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:56.451+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:56.452+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:56.453+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.454+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.455+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:56.455+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:56.456+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:56.458+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:56.459+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:56.461+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:56.462+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:56.463+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:56.464+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:56.465+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:56.466+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:56.467+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:56.468+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:56.469+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:56.470+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.471+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:56.473+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:56.473+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:56.474+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:56.475+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:56.476+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:56.478+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:56.478+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:56.479+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:56.480+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:56.481+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:56.482+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:56.483+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:56.484+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:56.485+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:59.255+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/attempt_202304260654548544187835489061831_0016_m_000003_21/' directory.
[2023-04-26T06:54:59.256+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/attempt_20230426065454127498722448709522_0016_m_000002_20/' directory.
[2023-04-26T06:54:59.257+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/attempt_202304260654548184989009682632870_0016_m_000000_18/' directory.
[2023-04-26T06:54:59.258+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/attempt_202304260654543534945193157245240_0016_m_000001_19/' directory.
[2023-04-26T06:54:59.259+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:59.259+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:59.260+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:59.261+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:59.262+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:54:59.262+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:54:59.263+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:54:59.263+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:54:59.264+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:54:59.265+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:54:59.265+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:54:59.266+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:54:59.266+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:54:59.267+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:54:59.268+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:54:59.268+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:54:59.268+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:54:59.269+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:54:59.269+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:54:59.270+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:59.271+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:59.271+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:54:59.272+0000] {subprocess.py:93} INFO - 23/04/26 06:54:54 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/' directory.
[2023-04-26T06:54:59.272+0000] {subprocess.py:93} INFO - 23/04/26 06:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/_temporary/0/_temporary/attempt_202304260654541951197382278080429_0016_m_000004_22/' directory.
[2023-04-26T06:54:59.273+0000] {subprocess.py:93} INFO - 23/04/26 06:54:55 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/' directory.
[2023-04-26T06:54:59.273+0000] {subprocess.py:93} INFO - 23/04/26 06:54:55 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_teams_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatPerGameValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/part-00003-46274c7b-fcf6-466a-8e8e-b53883508291-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/part-00000-46274c7b-fcf6-466a-8e8e-b53883508291-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/part-00002-46274c7b-fcf6-466a-8e8e-b53883508291-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/part-00004-46274c7b-fcf6-466a-8e8e-b53883508291-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-079814e3-98e5-421d-92a7-e4b6166132b2/part-00001-46274c7b-fcf6-466a-8e8e-b53883508291-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=b9022902-ddc4-4796-9010-885df0bd494b, location=europe-west6}
[2023-04-26T06:54:59.274+0000] {subprocess.py:93} INFO - 23/04/26 06:54:57 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_teams_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=b9022902-ddc4-4796-9010-885df0bd494b, location=europe-west6}
[2023-04-26T06:55:03.971+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:03.972+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:03.973+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:03.974+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:03.975+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:03.976+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:03.977+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:03.978+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:03.979+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:03.979+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:03.980+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:03.981+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:03.982+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:03.983+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:03.984+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:03.985+0000] {subprocess.py:93} INFO - 23/04/26 06:55:00 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:03.986+0000] {subprocess.py:93} INFO - 23/04/26 06:55:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a7d6fba2-d258-4f94-875e-483958307709/_temporary/0/_temporary/attempt_202304260654588916915446079607085_0018_m_000000_24/' directory.
[2023-04-26T06:55:03.986+0000] {subprocess.py:93} INFO - 23/04/26 06:55:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a7d6fba2-d258-4f94-875e-483958307709/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:03.987+0000] {subprocess.py:93} INFO - 23/04/26 06:55:01 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a7d6fba2-d258-4f94-875e-483958307709/' directory.
[2023-04-26T06:55:06.244+0000] {subprocess.py:93} INFO - 23/04/26 06:55:01 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=firstName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=lastName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=fullName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=shortName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=weightLbs, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=heightInches, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=age, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=dateOfBirth, type=DATE, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=debutYear, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCity, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceState, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=birthPlaceCountry, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=experienceYears, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=statusName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=headshot, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=positionParent, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-a7d6fba2-d258-4f94-875e-483958307709/part-00000-3596771c-fe0b-44cf-b06a-756a634821e0-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=b102e1fd-dccc-471a-a625-14dcf41bb783, location=europe-west6}
[2023-04-26T06:55:09.055+0000] {subprocess.py:93} INFO - 23/04/26 06:55:05 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_PARTITIONED. jobId: JobId{project=nfl-project-de, job=b102e1fd-dccc-471a-a625-14dcf41bb783, location=europe-west6}
[2023-04-26T06:55:11.906+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.907+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.908+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:11.909+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:11.910+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:11.911+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:11.912+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:11.913+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:11.914+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:11.915+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:11.916+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:11.916+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:11.917+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:11.917+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:11.918+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:11.919+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:11.920+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.921+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.922+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:11.922+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:11.923+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:11.924+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:11.924+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:11.926+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:11.927+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:11.928+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:11.929+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:11.931+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:11.932+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:11.933+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:11.934+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:11.935+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:11.936+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.937+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.937+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:11.938+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:11.939+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:11.940+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:11.942+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:11.943+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:11.943+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:11.944+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:11.944+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:11.945+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:11.945+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:11.946+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:11.948+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:11.949+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:11.950+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.950+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:11.951+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:11.951+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:11.952+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:11.953+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:11.954+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:11.956+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:11.956+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:11.957+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:11.958+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:11.958+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:11.959+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:11.959+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:11.960+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:11.962+0000] {subprocess.py:93} INFO - 23/04/26 06:55:09 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:16.576+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/attempt_20230426065509516175101829522912_0022_m_000000_28/' directory.
[2023-04-26T06:55:16.577+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/attempt_202304260655095814771932363213201_0022_m_000001_29/' directory.
[2023-04-26T06:55:16.578+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:16.579+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/attempt_20230426065509127310738580362981_0022_m_000002_30/' directory.
[2023-04-26T06:55:16.580+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:16.580+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:16.581+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:16.582+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:16.583+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:16.584+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:16.585+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:16.586+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:16.587+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:16.588+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:16.589+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:16.590+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:16.590+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:16.591+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:16.592+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:16.592+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:16.593+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/attempt_202304260655098969951564900242368_0022_m_000003_31/' directory.
[2023-04-26T06:55:16.593+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:55:16.594+0000] {subprocess.py:93} INFO - 23/04/26 06:55:10 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:17.994+0000] {subprocess.py:93} INFO - 23/04/26 06:55:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/_temporary/0/_temporary/attempt_202304260655091928871707871305931_0022_m_000004_32/' directory.
[2023-04-26T06:55:17.995+0000] {subprocess.py:93} INFO - 23/04/26 06:55:11 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/' directory.
[2023-04-26T06:55:17.995+0000] {subprocess.py:93} INFO - 23/04/26 06:55:11 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_athletes_stats_PARTITIONED}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=athleteStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatShortDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athletePerGameValue, type=FLOAT, mode=REQUIRED, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=athleteId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=year, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=seasonType, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/part-00002-06383ed2-7bca-4ff9-8be5-11bf7157ec2c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/part-00003-06383ed2-7bca-4ff9-8be5-11bf7157ec2c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/part-00000-06383ed2-7bca-4ff9-8be5-11bf7157ec2c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/part-00001-06383ed2-7bca-4ff9-8be5-11bf7157ec2c-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-225d31d7-06dc-493d-a8e2-586401bd105b/part-00004-06383ed2-7bca-4ff9-8be5-11bf7157ec2c-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=ae328284-b180-4cde-a893-b35f864c1c9e, location=europe-west6}
[2023-04-26T06:55:17.996+0000] {subprocess.py:93} INFO - 23/04/26 06:55:13 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_athletes_stats_PARTITIONED. jobId: JobId{project=nfl-project-de, job=ae328284-b180-4cde-a893-b35f864c1c9e, location=europe-west6}
[2023-04-26T06:55:17.996+0000] {subprocess.py:93} INFO - Loading for nfl data to parquet ended
[2023-04-26T06:55:20.868+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.869+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.870+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.871+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:20.871+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:20.872+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:20.872+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:20.873+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:20.873+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:20.874+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:20.874+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:20.875+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:20.875+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:20.876+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:20.876+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:20.877+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:20.877+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:20.878+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.878+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:20.879+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:20.880+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:20.880+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:20.881+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:20.881+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:20.882+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:20.883+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:20.884+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:20.885+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:20.888+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:20.889+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:20.890+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:20.891+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:20.891+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.892+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.893+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.894+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.894+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:20.896+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:20.896+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:20.897+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:20.898+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:20.898+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:20.899+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:20.900+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:20.902+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:20.903+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:20.903+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:20.904+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:20.905+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:20.906+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:20.907+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:20.910+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:20.911+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:20.912+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:20.913+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:20.914+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:20.915+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:20.916+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:20.918+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:20.919+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:20.921+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:20.922+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:20.924+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:20.927+0000] {subprocess.py:93} INFO - 23/04/26 06:55:15 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:20.930+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/attempt_202304260655158170657945302493_0035_m_000003_59/' directory.
[2023-04-26T06:55:20.932+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/attempt_202304260655155129180261047761447_0035_m_000002_58/' directory.
[2023-04-26T06:55:20.935+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/attempt_202304260655151583147509244508266_0035_m_000001_57/' directory.
[2023-04-26T06:55:20.937+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/attempt_202304260655155823452549472424564_0035_m_000000_56/' directory.
[2023-04-26T06:55:20.939+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:20.941+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:55:20.942+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:20.944+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:55:20.945+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:20.946+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.947+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:20.947+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:20.948+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:20.949+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:20.952+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:20.953+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:20.954+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:20.955+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:20.957+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:20.959+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:20.961+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:20.962+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:20.963+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:20.964+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:20.965+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:20.965+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.
[2023-04-26T06:55:20.966+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:20.967+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/_temporary/0/_temporary/attempt_202304260655153769713184172902779_0035_m_000004_60/' directory.
[2023-04-26T06:55:20.968+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/' directory.
[2023-04-26T06:55:20.970+0000] {subprocess.py:93} INFO - 23/04/26 06:55:17 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_radar_stats}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamStatName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDescription, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatAbbreviation, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatValue, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamStatCategory, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=percentileRank, type=FLOAT, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/part-00000-30d714f8-9261-4d54-b73e-c5966ff603c9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/part-00004-30d714f8-9261-4d54-b73e-c5966ff603c9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/part-00002-30d714f8-9261-4d54-b73e-c5966ff603c9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/part-00001-30d714f8-9261-4d54-b73e-c5966ff603c9-c000.snappy.parquet, gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-c5bc3a1d-3dce-48f7-b37a-675069b0b696/part-00003-30d714f8-9261-4d54-b73e-c5966ff603c9-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=d20154f5-3810-47c3-8aee-82bcd34720d5, location=europe-west6}
[2023-04-26T06:55:27.349+0000] {subprocess.py:93} INFO - 23/04/26 06:55:21 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_radar_stats. jobId: JobId{project=nfl-project-de, job=d20154f5-3810-47c3-8aee-82bcd34720d5, location=europe-west6}
[2023-04-26T06:55:27.350+0000] {subprocess.py:93} INFO - 23/04/26 06:55:22 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.350+0000] {subprocess.py:93} INFO - 23/04/26 06:55:22 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.351+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.352+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.352+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.353+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.354+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.355+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.355+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.356+0000] {subprocess.py:93} INFO - 23/04/26 06:55:23 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.357+0000] {subprocess.py:93} INFO - 23/04/26 06:55:24 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.357+0000] {subprocess.py:93} INFO - 23/04/26 06:55:24 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.358+0000] {subprocess.py:93} INFO - 23/04/26 06:55:24 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.360+0000] {subprocess.py:93} INFO - 23/04/26 06:55:24 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.361+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.362+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.362+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:27.363+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:30.197+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:30.199+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:30.200+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:30.201+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 WARN org.apache.spark.sql.execution.window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.
[2023-04-26T06:55:30.201+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:30.202+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
[2023-04-26T06:55:30.203+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
[2023-04-26T06:55:30.203+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
[2023-04-26T06:55:30.204+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
[2023-04-26T06:55:30.205+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
[2023-04-26T06:55:30.206+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
[2023-04-26T06:55:30.206+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
[2023-04-26T06:55:30.207+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 8388608 bytes
[2023-04-26T06:55:30.208+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page size checking is: estimated
[2023-04-26T06:55:30.209+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Min row count for page size check is: 100
[2023-04-26T06:55:30.210+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Max row count for page size check is: 10000
[2023-04-26T06:55:30.211+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for column indexes is: 64
[2023-04-26T06:55:30.212+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Truncate length for statistics min/max  is: 2147483647
[2023-04-26T06:55:30.213+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Page row count limit to 20000
[2023-04-26T06:55:30.213+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO org.apache.parquet.hadoop.ParquetOutputFormat: Writing page checksums is: on
[2023-04-26T06:55:30.214+0000] {subprocess.py:93} INFO - 23/04/26 06:55:25 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-80183943-7817-486f-89e3-946f6bc8666f/_temporary/0/_temporary/attempt_202304260655256115270045400408167_0080_m_000000_120/' directory.
[2023-04-26T06:55:30.215+0000] {subprocess.py:93} INFO - 23/04/26 06:55:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-80183943-7817-486f-89e3-946f6bc8666f/_temporary/0/_temporary/' directory.
[2023-04-26T06:55:30.216+0000] {subprocess.py:93} INFO - 23/04/26 06:55:26 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-80183943-7817-486f-89e3-946f6bc8666f/' directory.
[2023-04-26T06:55:30.216+0000] {subprocess.py:93} INFO - 23/04/26 06:55:26 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Submitted job LoadJobConfiguration{type=LOAD, destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=nfl_data_all, projectId=nfl-project-de, tableId=2022_2_best_worst_teams}}, decimalTargetTypes=null, destinationEncryptionConfiguration=null, createDisposition=CREATE_IF_NEEDED, writeDisposition=WRITE_TRUNCATE, formatOptions=ParquetOptions{enableListInference=null, enumAsString=null}, nullMarker=null, maxBadRecords=null, schema=Schema{fields=[Field{name=teamId, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamDisplayName, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=teamLogo, type=STRING, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=defRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}, Field{name=offRank, type=INTEGER, mode=NULLABLE, description=null, policyTags=null, maxLength=null, scale=null, precision=null, defaultValueExpression=null, collation=null}]}, ignoreUnknownValue=null, sourceUris=[gs://dataproc-temp-europe-west6-447072446412-ooqjzme9/.spark-bigquery-local-1682492060323-80183943-7817-486f-89e3-946f6bc8666f/part-00000-31e272d3-002c-4dc7-a1a2-2854f76d5c26-c000.snappy.parquet], schemaUpdateOptions=null, autodetect=null, timePartitioning=null, clustering=null, useAvroLogicalTypes=null, labels=null, jobTimeoutMs=null, rangePartitioning=null, hivePartitioningOptions=null, referenceFileSchemaUri=null, connectionProperties=null, createSession=null}. jobId: JobId{project=nfl-project-de, job=aa46f6c9-6cd5-4543-8c46-ba51df55c61c, location=europe-west6}
[2023-04-26T06:55:33.017+0000] {subprocess.py:93} INFO - 23/04/26 06:55:29 INFO com.google.cloud.bigquery.connector.common.BigQueryClient: Done loading to nfl-project-de.nfl_data_all.2022_2_best_worst_teams. jobId: JobId{project=nfl-project-de, job=aa46f6c9-6cd5-4543-8c46-ba51df55c61c, location=europe-west6}
[2023-04-26T06:55:33.018+0000] {subprocess.py:93} INFO - 23/04/26 06:55:29 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@3c677{HTTP/1.1, (http/1.1)}{0.0.0.0:0}
[2023-04-26T06:55:36.278+0000] {subprocess.py:93} INFO - Job [0eefb2497d3a494e81feba21da23bcc8] finished successfully.
[2023-04-26T06:55:36.313+0000] {subprocess.py:93} INFO - done: true
[2023-04-26T06:55:36.315+0000] {subprocess.py:93} INFO - driverControlFilesUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/0eefb2497d3a494e81feba21da23bcc8/
[2023-04-26T06:55:36.316+0000] {subprocess.py:93} INFO - driverOutputResourceUri: gs://nfl-spark-staging_nfl-project-de/google-cloud-dataproc-metainfo/3f31be1a-ebc3-4b4c-987d-83767542db1e/jobs/0eefb2497d3a494e81feba21da23bcc8/driveroutput
[2023-04-26T06:55:36.318+0000] {subprocess.py:93} INFO - jobUuid: a997d5ea-dc5b-354c-b7a1-49ca9788f68b
[2023-04-26T06:55:36.319+0000] {subprocess.py:93} INFO - placement:
[2023-04-26T06:55:36.320+0000] {subprocess.py:93} INFO -   clusterName: nfl-spark-cluster
[2023-04-26T06:55:36.321+0000] {subprocess.py:93} INFO -   clusterUuid: 3f31be1a-ebc3-4b4c-987d-83767542db1e
[2023-04-26T06:55:36.322+0000] {subprocess.py:93} INFO - pysparkJob:
[2023-04-26T06:55:36.323+0000] {subprocess.py:93} INFO -   args:
[2023-04-26T06:55:36.324+0000] {subprocess.py:93} INFO -   - --year=2022
[2023-04-26T06:55:36.325+0000] {subprocess.py:93} INFO -   - --season_type=2
[2023-04-26T06:55:36.326+0000] {subprocess.py:93} INFO -   jarFileUris:
[2023-04-26T06:55:36.327+0000] {subprocess.py:93} INFO -   - gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.30.0.jar
[2023-04-26T06:55:36.328+0000] {subprocess.py:93} INFO -   mainPythonFileUri: gs://nfl-data-lake_nfl-project-de/code/transform_pyspark.py
[2023-04-26T06:55:36.330+0000] {subprocess.py:93} INFO - reference:
[2023-04-26T06:55:36.331+0000] {subprocess.py:93} INFO -   jobId: 0eefb2497d3a494e81feba21da23bcc8
[2023-04-26T06:55:36.332+0000] {subprocess.py:93} INFO -   projectId: nfl-project-de
[2023-04-26T06:55:36.332+0000] {subprocess.py:93} INFO - status:
[2023-04-26T06:55:36.333+0000] {subprocess.py:93} INFO -   state: DONE
[2023-04-26T06:55:36.334+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T06:55:29.871473Z'
[2023-04-26T06:55:36.335+0000] {subprocess.py:93} INFO - statusHistory:
[2023-04-26T06:55:36.336+0000] {subprocess.py:93} INFO - - state: PENDING
[2023-04-26T06:55:36.337+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T06:54:10.498500Z'
[2023-04-26T06:55:36.339+0000] {subprocess.py:93} INFO - - state: SETUP_DONE
[2023-04-26T06:55:36.340+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T06:54:10.557490Z'
[2023-04-26T06:55:36.340+0000] {subprocess.py:93} INFO - - details: Agent reported job success
[2023-04-26T06:55:36.341+0000] {subprocess.py:93} INFO -   state: RUNNING
[2023-04-26T06:55:36.342+0000] {subprocess.py:93} INFO -   stateStartTime: '2023-04-26T06:54:10.882094Z'
[2023-04-26T06:55:36.588+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2023-04-26T06:55:36.592+0000] {python.py:177} INFO - Done. Returned value was: None
[2023-04-26T06:55:36.627+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=nfl_transformation_dag, task_id=task_pyspark, execution_date=20230426T065348, start_date=20230426T065405, end_date=20230426T065536
[2023-04-26T06:55:36.711+0000] {local_task_job.py:212} INFO - Task exited with return code 0
[2023-04-26T06:55:36.738+0000] {taskinstance.py:2596} INFO - 0 downstream tasks scheduled from follow-on schedule check
